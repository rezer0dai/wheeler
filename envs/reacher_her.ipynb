{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HER robotic problem ~ Reacher 20 Arms ( UnityML ) ~ experimental WIP\n",
    "- *Wheeler task definition ( task wrapper, State decoder settings, NeuralNetwork, ReplayBuffer, .. )*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import generics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, time\n",
    "\n",
    "os.chdir(\"..\")\n",
    "\n",
    "import numpy as np\n",
    "import toml, gym\n",
    "\n",
    "import torch\n",
    "from torch.multiprocessing import Queue, Process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare statistic helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "def print_stats(scores, title):\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111)\n",
    "    plt.plot(np.arange(1, len(scores)+1), scores)\n",
    "    plt.ylabel('Rewards')\n",
    "    plt.xlabel(title)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load task configs ~ this should be adopted offline for particular task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MountainCarContinuous-v0\n"
     ]
    }
   ],
   "source": [
    "CFG = toml.loads(open('cfg.toml').read())\n",
    "GYM_CFG = toml.loads(open('gym.toml').read())\n",
    "\n",
    "torch.set_default_tensor_type(CFG['tensor'])\n",
    "print(CFG['task'])\n",
    "CFG['task'] = \"data/Reacher.x86_64\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import wheeler environment and particular utils we want to use ~ general ones ( shared across tasks )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.task import Task\n",
    "from utils.taskinfo import *\n",
    "\n",
    "from utils.rbf import *\n",
    "from utils.normalizer import *\n",
    "\n",
    "from utils.taskmgr import *\n",
    "from utils.replay import *\n",
    "\n",
    "from utils.fastmem import Memory\n",
    "\n",
    "from utils.curiosity import *\n",
    "\n",
    "from utils.unity import unity_factory\n",
    "\n",
    "from agent.zer0bot import agent_launch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Task wrapper ~ when is goal met, how to step ( update rewards function, .. ), when / how to reset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "CLOSE_ENOUGH = 1.25\n",
    "\n",
    "def extract_goal(state):\n",
    "    return state[-4-3:-1-3] # this is nice, from perspective, that we are OK if our states are stacked ~ we work with latest one\n",
    "\n",
    "def goal_distance(goal_a, goal_b):\n",
    "    return np.linalg.norm(goal_a - goal_b.reshape(goal_a.shape), axis=-1)\n",
    "\n",
    "def fun_reward(s, n, goal, objective_id, cfg, her):\n",
    "    xid = objective_id % 4\n",
    "    if True:#xid >= 3: # full task 3D naigation\n",
    "        return -1 * (2 * CLOSE_ENOUGH < np.abs(goal_distance(extract_goal(s), goal)))\n",
    "# X- Y - Z subtask navigation ~ multi task\n",
    "    a = np.abs(extract_goal(n)[xid] - goal[xid])\n",
    "    b = np.abs(extract_goal(s)[xid] - goal[xid])\n",
    "    if b < CLOSE_ENOUGH:\n",
    "        return 0.\n",
    "    return -1 + .9 * int(a < b) # little shape it maybe good idea ? ~ experimental\n",
    "\n",
    "def sample_goal(cfg, trajectory_goal, trajectory_state):\n",
    "    hs = cfg['her_state_size']\n",
    "    pos = extract_goal(trajectory_state.copy())\n",
    "    for i in range(3):# be carefull extremly expensive\n",
    "        radius = np.abs(np.random.rand() * CLOSE_ENOUGH)\n",
    "        angle = np.random.rand() * np.pi * 2\n",
    "        a = np.cos(angle) * radius\n",
    "        b = np.sin(angle) * radius\n",
    "        ids = np.random.choice(hs, 2, p=[1/hs]*hs, replace=False)\n",
    "\n",
    "        target = pos if random.randint(0, 5) else trajectory_goal\n",
    "        goal = target.copy()\n",
    "        goal[ids[0]] += a\n",
    "        goal[ids[1]] += b\n",
    "\n",
    "        if np.abs(goal_distance(goal, target)) < CLOSE_ENOUGH:\n",
    "            return goal, target\n",
    "    return (\n",
    "        pos if random.randint(0, 3) else trajectory_goal, \n",
    "        pos if random.randint(0, 3) else trajectory_goal\n",
    "    )\n",
    "\n",
    "def goal_select(total_after, n_step):\n",
    "    if total_after <= n_step + 1: # only last n_state remainds\n",
    "        return 0\n",
    "    if random.randint(0, 2):\n",
    "        return random.randint(1, n_step)\n",
    "    if 0 == random.randint(0, 3):\n",
    "        return 0\n",
    "    return random.randint(1, total_after - 1 - n_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GymTask(Task):\n",
    "    def reset(self, seed = None, test = False):\n",
    "        cfg = {\"goal_size\":5., \"goal_speed\":0.}\n",
    "        state = super().reset(cfg, test)[0]\n",
    "\n",
    "        self.goal_ = state\n",
    "        \n",
    "        if test: return state # we will get array of states\n",
    "        \n",
    "        state = state.reshape(-1)\n",
    "        return [ state ]\n",
    "    \n",
    "    def step_ex(self, action, test = False):\n",
    "        state, done, reward = self.env.step(self.bot_id, self.objective_id, action)\n",
    "        self.goal_ = state.copy()\n",
    "\n",
    "        if not test: reward = fun_reward(state, None, self.goal(0), self.objective_id, self.cfg, False)\n",
    "        \n",
    "        return action, state, reward, done, True\n",
    "\n",
    "    def goal_met(self, states, rewards, n_steps):\n",
    "        return sum(abs(r) for r in rewards) > 30\n",
    "\n",
    "    def goal(self, ind = 0):\n",
    "#        print(extract_goal(self.goal_.reshape(-1)).reshape(1, -1) if self.goal_.shape[0] != 20 else extract_goal(self.goal_[ind].reshape(-1)).reshape(1, -1))\n",
    "        if self.goal_.shape[0] != 20: return extract_goal(self.goal_.reshape(-1)).reshape(1, -1)\n",
    "        return extract_goal(self.goal_[ind].reshape(-1)).reshape(1, -1)\n",
    "    \n",
    "    def update_goal(self, _, goals, states, n_goals, n_states, updates):\n",
    "        for i, (g, s, n_g, n, u) in enumerate(zip(goals, states, n_goals, n_states, updates)):\n",
    "            if u:\n",
    "                ind = i + goal_select(len(states) - i, self.cfg['n_step'])\n",
    "                g, n_g = sample_goal(self.cfg, goals[ind], states[ind])\n",
    "            \n",
    "            yield (\n",
    "                fun_reward(s, None, g, self.objective_id, self.cfg, True),\n",
    "                g, s,\n",
    "                n_g, n,\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generic proxy for creating our Task ( multiprocess environments purpose mainly ) \n",
    "- but can also add wrapping function approx values ( action value to tanh, sigmoid, .. ) - this not works well with PPO now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GymInfo(TaskInfo):\n",
    "    def __init__(self, replaybuf, factory, Mgr, args):\n",
    "        super().__init__(\n",
    "                33, 4, -1, +1,\n",
    "                CFG,\n",
    "                replaybuf,\n",
    "                factory, Mgr, args)\n",
    "\n",
    "    def new(self, cfg, bot_id, objective_id):\n",
    "        task = GymTask(cfg,\n",
    "                self.env,\n",
    "                objective_id, bot_id,\n",
    "                self.action_low, self.action_high)\n",
    "        if -1 == objective_id:\n",
    "            task.reset()\n",
    "        return task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implement callback for testing policy ~ per X training rounds, we want to test it ~ enable visuals if you want"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def context_callback(count, print_every):\n",
    "    context_callback.z = 0\n",
    "    context_callback.scores = []\n",
    "    def callback(task, agent, scores):\n",
    "        context_callback.z += 1\n",
    "        if 0 == context_callback.z % print_every:\n",
    "            print(\"\\nTEST review : \", np.mean(context_callback.scores[-print_every:]), \"LAST stats :\", sum(scores), sum(map(lambda r: r != 0, scores)), len(scores))\n",
    "        # we can save scores to main queue, and avarage them, or we can ..\n",
    "        # run testing w/ visuals :\n",
    "        for _ in range(count):\n",
    "            done, states, rewards = task.test_policy(agent)\n",
    "            rewards = sum(rewards)\n",
    "            context_callback.scores.append(rewards)\n",
    "            if not done:\n",
    "                return None\n",
    "\n",
    "        print(\"\\n\")\n",
    "        print(\"=\"*80)\n",
    "        print(\"training over\", context_callback.z * GYM_CFG['n_simulations'] * GYM_CFG['mcts_rounds'])\n",
    "        print(\"=\"*80)\n",
    "\n",
    "        return context_callback.scores\n",
    "    return callback"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare neural network which we will be using"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models import ddpg_model, noisy_model, state_action_model\n",
    "\n",
    "def CriticNN(state_size, action_size, wrap_value, cfg):\n",
    "    return state_action_model.Critic(state_size, action_size, wrap_value, cfg, fcs1_units=256, fc2_units=128)\n",
    "\n",
    "def ActorNN(state_size, action_size, wrap_action, cfg):\n",
    "    return noisy_model.Actor(state_size, action_size, wrap_action, cfg, hiddens=[400, 300])\n",
    "    return noisy_model.Actor(state_size, action_size, wrap_action, cfg, hiddens=[128, 64])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select encoders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.encoders import *\n",
    "from utils.rnn import *#GRUEncoder\n",
    "\n",
    "def encoderstack():\n",
    "    norm = GlobalNormalizer(GYM_CFG, 33)\n",
    "    return norm\n",
    "    GYM_CFG['max_n_episode'] = 70 # we want gae + rnn; need re-eval eps ~ 1000 per shot is overkill\n",
    "    GYM_CFG['batch_size'] = 16 # this is basically numero of sample eps, an from each ep we draw about <1, 40> steps\n",
    "    experience = GRUEncoder(GYM_CFG, norm.total_size())#GRU#LSTM\n",
    "    encoder_norm = StackedEncoder(GYM_CFG, 33, norm, experience)\n",
    "    encoder_norm.share_memory()\n",
    "    return encoder_norm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cook Task : replay buffer ( fast / prio-gae-rnn ) + task manager ( local / remote / unity )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def taskfactory():\n",
    "#    return GymInfo(Memory, unity_factory(CFG, CFG['total_simulations']), RemoteTaskManager, (LocalTaskManager, 1 + GYM_CFG['n_simulations']))\n",
    "    return GymInfo(ReplayBuffer, unity_factory(CFG, CFG['total_simulations']), RemoteTaskManager, (LocalTaskManager, 1 + GYM_CFG['n_simulations']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Glue it all together ~ select buffer, encoders, agents, ... and RUN!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'tensor': 'torch.DoubleTensor', 'task': 'data/Reacher.x86_64', 'total_simulations': 20, 'cross_exp_size': 5000, 'max_reward_val': 1000, 'min_reward_val': -1000}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "'Academy' started successfully!\n",
      "Unity Academy name: Academy\n",
      "        Number of Brains: 1\n",
      "        Number of External Brains : 1\n",
      "        Lesson number : 0\n",
      "        Reset Parameters :\n",
      "\t\tgoal_speed -> 1.0\n",
      "\t\tgoal_size -> 5.0\n",
      "Unity brain name: ReacherBrain\n",
      "        Number of Visual Observations (per agent): 0\n",
      "        Vector Observation space type: continuous\n",
      "        Vector Observation space size (per agent): 33\n",
      "        Number of stacked Vector Observation: 1\n",
      "        Vector Action space type: continuous\n",
      "        Vector Action space size (per agent): 4\n",
      "        Vector Action descriptions: , , , \n",
      "INFO:unityagents:\n",
      "Academy Reset with parameters : \tgoal_size -> 5.0, goal_speed -> 0.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UNITY SERVER PROCESS PING ~ running at 3153\n",
      "UNITY SERVER PROCESS PING ~ running at 3153\n",
      "UNITY SERVER PROCESS PING ~ running at 3153\n",
      "UNITY SERVER PROCESS PING ~ running at 3153\n",
      "UNITY SERVER PROCESS PING ~ running at 3153\n",
      "UNITY SERVER PROCESS PING ~ running at 3153\n",
      "UNITY SERVER PROCESS PING ~ running at 3153\n",
      "UNITY SERVER PROCESS PING ~ running at 3153\n",
      "UNITY SERVER PROCESS PING ~ running at 3153\n",
      "UNITY SERVER PROCESS PING ~ running at 3153\n",
      "UNITY SERVER PROCESS PING ~ running at 3153\n",
      "UNITY SERVER PROCESS PING ~ running at 3153\n",
      "UNITY SERVER PROCESS PING ~ running at 3153\n",
      "UNITY SERVER PROCESS PING ~ running at 3153\n",
      "UNITY SERVER PROCESS PING ~ running at 3153\n",
      "UNITY SERVER PROCESS PING ~ running at 3153\n",
      "UNITY SERVER PROCESS PING ~ running at 3153\n",
      "UNITY SERVER PROCESS PING ~ running at 3153\n",
      "UNITY SERVER PROCESS PING ~ running at 3153\n",
      "UNITY SERVER PROCESS PING ~ running at 3153\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "Academy Reset with parameters : \tgoal_size -> 5.0, goal_speed -> 0.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0>   1::     0] training =  0, steps =  36, max_step = 10000, reward=0.000000 ::[[ 1.          1.         -0.77776737 -1.        ]]: "
     ]
    }
   ],
   "source": [
    "class GoalEncoder(IEncoder):\n",
    "    def __init__(self, cfg, size):\n",
    "        super().__init__(cfg)\n",
    "        self.size = size\n",
    "        self.net = torch.nn.Linear(size, cfg['her_state_features'], bias=False)\n",
    "    def out_size(self):\n",
    "        assert False, \"...\"\n",
    "    def forward(self, goals, _):\n",
    "        return self.net(goals)\n",
    "\n",
    "def main():\n",
    "    print(CFG)\n",
    "    \n",
    "    encoder = encoderstack()\n",
    "    task_factory = taskfactory()\n",
    "    task = task_factory.new(GYM_CFG, 0, -1)\n",
    "    \n",
    "    callback = context_callback(10, 10)\n",
    "    def callback_task(agent, stop_q):\n",
    "        return callback(task, agent, stop_q)\n",
    "\n",
    "    stop_q = Queue()\n",
    "    agent_launch(0, GYM_CFG, task_factory, encoder, ActorNN, CriticNN, stop_q, callback_task, GoalEncoder(GYM_CFG, GYM_CFG['her_state_size']))\n",
    "    \n",
    "    scores = stop_q.get()\n",
    "    print(\"FINISHED!\")\n",
    "    print_stats(scores, \"learning algorithm\")\n",
    "\n",
    "if '__main__' == __name__:\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
