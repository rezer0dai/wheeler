{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MountainCarContinuous-v0 environment of OpenAi GYM \n",
    "- *Wheeler task definition ( task wrapper, State decoder settings, NeuralNetwork, ReplayBuffer, .. )*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import generics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os, time\n",
    "\n",
    "os.chdir(\"..\")\n",
    "\n",
    "import numpy as np\n",
    "import toml, gym\n",
    "\n",
    "import torch\n",
    "from torch.multiprocessing import Queue, Process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load task configs ~ this should be adopted offline for particular task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "CFG = toml.loads(open('cfg.toml').read())\n",
    "GYM_CFG = toml.loads(open('gym.toml').read())\n",
    "\n",
    "torch.set_default_tensor_type(CFG['tensor'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import wheeler environment and particular utils we want to use ~ general ones ( shared across tasks )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from utils.task import Task\n",
    "from utils.taskinfo import *\n",
    "\n",
    "from utils.rbf import *\n",
    "from utils.normalizer import *\n",
    "\n",
    "from utils.taskmgr import *\n",
    "from utils.replay import *\n",
    "\n",
    "from utils.fastmem import Memory\n",
    "\n",
    "from utils.curiosity import *\n",
    "\n",
    "from agent.zer0bot import agent_launch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Task wrapper ~ when is goal met, how to step ( update rewards function, .. ), when / how to reset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class MCarTask(Task):\n",
    "    def __init__(self, cfg, env, objective_id, bot_id, action_low, action_high, rewarder):\n",
    "        self.reward = 0\n",
    "        self.rewards = []\n",
    "\n",
    "        super().__init__(\n",
    "                cfg,\n",
    "                env,\n",
    "                objective_id,\n",
    "                bot_id,\n",
    "                action_low, action_high)\n",
    "\n",
    "        self.rewarder = rewarder\n",
    "\n",
    "    def reset(self, seed = None, test = False):\n",
    "        state = super().reset(seed, test)\n",
    "        self.rewards.append(self.reward)\n",
    "        self.reward = 0\n",
    "        self.prev_state = state[0]\n",
    "        return state\n",
    "\n",
    "    def step_ex(self, action, test = False):\n",
    "        state, reward, done, _ = self.env.step(self.bot_id, self.objective_id, action)\n",
    "\n",
    "        if test: return action, state.reshape(1, -1), reward, done, True\n",
    "\n",
    "        self.reward += (done and reward > 0)\n",
    "\n",
    "        curiosity_test = '''\n",
    "        if not test and reward < 0:\n",
    "            ss, ns = np.vstack([s] * self.cfg['history_count']), np.vstack([n] * self.cfg['history_count'])\n",
    "            ss = ss.reshape(1, -1)\n",
    "            ns = ns.reshape(1, -1)\n",
    "            reward = self.rewarder.weight(ss, ns, action)[0]\n",
    "            self.rewarder.update(ss, ns, action)\n",
    "        self.prev_state = state\n",
    "\n",
    "        reward_update  = '''\n",
    "        if not test and not done:# and sum(self.rewards) < 3\n",
    "            true_state = np.abs(np.cos(np.pi/3.) + state[0])\n",
    "            reward += -(1. - true_state)\n",
    "#        '''\n",
    "\n",
    "        return action, state, reward, done, True\n",
    "\n",
    "    def goal_met(self, states, rewards, n_steps):\n",
    "        print(\"TEST : \", sum(rewards))\n",
    "        return sum(rewards) > 90."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generic proxy for creating our Task ( multiprocess environments purpose mainly ) \n",
    "- but can also add wrapping function approx values ( action value to tanh, sigmoid, .. ) - this not works well with PPO now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class MCarInfo(TaskInfo):\n",
    "    def __init__(self, env, replaybuf, factory, Mgr, args):\n",
    "        super().__init__(\n",
    "                len(env.reset()), 1, -1, +1,\n",
    "                CFG,\n",
    "                replaybuf,\n",
    "                factory, Mgr, args)\n",
    "\n",
    "        self.rewarder = CuriosityPrio(\n",
    "                self.state_size, self.action_size,\n",
    "                self.action_range, self.wrap_action, \"cpu\", GYM_CFG)\n",
    "\n",
    "    def new(self, cfg, bot_id, objective_id):\n",
    "        return MCarTask(cfg,\n",
    "                self.env,\n",
    "                objective_id, bot_id,\n",
    "                self.action_low, self.action_high,\n",
    "                self.rewarder)\n",
    "\n",
    "    @staticmethod\n",
    "    def factory(ind): # bare metal task creation\n",
    "        print(\"created %i-th task\"%ind)\n",
    "        CFG = toml.loads(open('cfg.toml').read())\n",
    "        return gym.make(CFG['task'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implement callback for testing policy ~ per X training rounds, we want to test it ~ enable visuals if you want"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def callback(task, agent, scores):\n",
    "    try: callback.z += 1\n",
    "    except: callback.z = 0\n",
    "    \n",
    "    # we can save scores to main queue, and avarage them, or we can ..\n",
    "    # run testing w/ visuals :\n",
    "    done = all(task.test_policy(agent)[0] for _ in range(10))\n",
    "    if not done:\n",
    "        return False\n",
    "\n",
    "    print(\"\\n\")\n",
    "    print(\"=\"*80)\n",
    "    print(\"training over\", callback.z * GYM_CFG['n_simulations'] * GYM_CFG['mcts_rounds'])\n",
    "    print(\"=\"*80)\n",
    "\n",
    "    for i in range(100): print(\"total steps : training : %i :: %i >\"%(\n",
    "        callback.z * GYM_CFG['mcts_rounds'] * GYM_CFG['n_simulations'],\n",
    "        len(task.test_policy(agent)[2])))\n",
    "\n",
    "    return True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare neural network which we will be using"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from models import ddpg_model, noisy_model\n",
    "\n",
    "def CriticNN(state_size, action_size, wrap_value, cfg):\n",
    "    return ddpg_model.Critic(state_size, action_size, wrap_value, cfg, fcs1_units=400, fc2_units=300)\n",
    "\n",
    "def ActorNN(state_size, action_size, wrap_action, cfg):\n",
    "    return noisy_model.Actor(state_size, action_size, wrap_action, cfg, hiddens=[400, 300])\n",
    "    return ddpg_model.Actor(state_size, action_size, wrap_action, cfg, fc1_units=400, fc2_units=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select encoders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from utils.encoders import *\n",
    "from utils.rnn import GRUEncoder\n",
    "\n",
    "def encoderstack(env):\n",
    "    encoder = RBFEncoder(env, [5., 2., 1., .5], [5] * 4)\n",
    "    \n",
    "    norm = GlobalNormalizer(encoder.out_size())\n",
    "#    return StackedEncoder(len(env.reset()), encoder, norm)\n",
    "\n",
    "    experience = GRUEncoder(encoder.out_size(), 0, 4, GYM_CFG['history_features'])#\n",
    "    encoder_norm = StackedEncoder(len(env.reset()), encoder, experience)\n",
    "    \n",
    "    return encoder_norm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cook Task : replay buffer ( fast / prio-gae-rnn ) + task manager ( local / remote / unity )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def taskfactory(env):\n",
    "    return MCarInfo(env, Memory, MCarInfo.factory, LocalTaskManager, ())\n",
    "    return MCarInfo(env, ReplayBuffer, MCarInfo.factory, LocalTaskManager, ())\n",
    "    return MCarInfo(env, ReplayBuffer, MCarInfo.factory, RemoteTaskManager, (LocalTaskManager, 1 + GYM_CFG['n_simulations']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Glue it all together ~ select buffer, encoders, agents, ... and RUN!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'task': 'MountainCarContinuous-v0', 'min_reward_val': -1000, 'tensor': 'torch.DoubleTensor', 'total_simulations': 2, 'max_reward_val': 1000, 'cross_exp_size': 5000}\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "created 0-th task\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "created 1-th task\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "created 2-th task\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "created 3-th task\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "[0>   1::     5] training =  0, steps =   5, max_step =   1, reward=-4.790110 ::[[0.22727493]]: "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py:547: UserWarning: Multiprocessing-backed parallel loops cannot be nested below threads, setting n_jobs=1\n",
      "  **self._backend_args)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0>   1::   618] training =  2, steps =  18, max_step = 300, reward=-18.282964 ::[[-1.]]: [ TARGET:-4.373119 replay::600 ]<----"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    print(CFG)\n",
    "\n",
    "    env = gym.make(CFG['task'])\n",
    "    \n",
    "    encoder = encoderstack(env)\n",
    "    task_factory = taskfactory(env)\n",
    "    task = task_factory.new(GYM_CFG, 0, -1)\n",
    "    \n",
    "    def callback_task(agent, stop_q):\n",
    "        return callback(task, agent, stop_q)\n",
    "    \n",
    "    stop_q = Queue()\n",
    "    agent_launch(0, GYM_CFG, task_factory, encoder, ActorNN, CriticNN, stop_q, callback_task)\n",
    "\n",
    "if '__main__' == __name__:\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
