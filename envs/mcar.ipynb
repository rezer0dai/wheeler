{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MountainCarContinuous-v0 environment of OpenAi GYM \n",
    "- *Wheeler task definition ( task wrapper, State decoder settings, NeuralNetwork, ReplayBuffer, .. )*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import generics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, time\n",
    "\n",
    "os.chdir(\"..\")\n",
    "\n",
    "import numpy as np\n",
    "import toml, gym\n",
    "\n",
    "import torch\n",
    "from torch.multiprocessing import Queue, Process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load task configs ~ this should be adopted offline for particular task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "CFG = toml.loads(open('cfg.toml').read())\n",
    "GYM_CFG = toml.loads(open('gym.toml').read())\n",
    "\n",
    "torch.set_default_tensor_type(CFG['tensor'])\n",
    "\n",
    "CFG['task'] = \"MountainCarContinuous-v0\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import wheeler environment and particular utils we want to use ~ general ones ( shared across tasks )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.task import Task\n",
    "from utils.taskinfo import *\n",
    "\n",
    "from utils.rbf import *\n",
    "from utils.normalizer import *\n",
    "\n",
    "from utils.taskmgr import *\n",
    "from utils.replay import *\n",
    "\n",
    "from utils.fastmem import Memory\n",
    "\n",
    "from utils.curiosity import *\n",
    "\n",
    "from agent.zer0bot import agent_launch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Task wrapper ~ when is goal met, how to step ( update rewards function, .. ), when / how to reset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MCarTask(Task):\n",
    "    def __init__(self, cfg, env, objective_id, bot_id, action_low, action_high, rewarder):\n",
    "        self.reward = 0\n",
    "        self.rewards = []\n",
    "\n",
    "        super().__init__(\n",
    "                cfg,\n",
    "                env,\n",
    "                objective_id,\n",
    "                bot_id,\n",
    "                action_low, action_high)\n",
    "\n",
    "        self.rewarder = rewarder\n",
    "\n",
    "    def reset(self, seed = None, test = False):\n",
    "        state = super().reset(seed, test)\n",
    "        self.rewards.append(self.reward)\n",
    "        self.reward = 0\n",
    "        self.prev_state = state[0]\n",
    "        return state\n",
    "\n",
    "    def step_ex(self, action, test = False):\n",
    "        state, reward, done, _ = self.env.step(self.bot_id, self.objective_id, action)\n",
    "\n",
    "        if test: return action, state.reshape(1, -1), reward, done, True\n",
    "\n",
    "        self.reward += (done and reward > 0)\n",
    "\n",
    "        curiosity_test = '''\n",
    "        if not test and reward < 0:\n",
    "            ss, ns = np.vstack([s] * self.cfg['history_count']), np.vstack([n] * self.cfg['history_count'])\n",
    "            ss = ss.reshape(1, -1)\n",
    "            ns = ns.reshape(1, -1)\n",
    "            reward = self.rewarder.weight(ss, ns, action)[0]\n",
    "            self.rewarder.update(ss, ns, action)\n",
    "        self.prev_state = state\n",
    "\n",
    "        reward_update  = '''\n",
    "        if not test and not done:# and sum(self.rewards) < 3\n",
    "            true_state = np.abs(np.cos(np.pi/3.) + state[0])\n",
    "            reward += -(1. - true_state)\n",
    "#        '''\n",
    "\n",
    "        return action, state, reward, done, True\n",
    "\n",
    "    def goal_met(self, states, rewards, n_steps):\n",
    "        print(\"TEST : \", sum(rewards))\n",
    "        return sum(rewards) > 90."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generic proxy for creating our Task ( multiprocess environments purpose mainly ) \n",
    "- but can also add wrapping function approx values ( action value to tanh, sigmoid, .. ) - this not works well with PPO now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MCarInfo(TaskInfo):\n",
    "    def __init__(self, env, replaybuf, factory, Mgr, args):\n",
    "        super().__init__(\n",
    "                len(env.reset()), 1, -1, +1,\n",
    "                CFG,\n",
    "                replaybuf,\n",
    "                factory, Mgr, args)\n",
    "\n",
    "        self.rewarder = CuriosityPrio(\n",
    "                self.state_size, self.action_size,\n",
    "                self.action_range, self.wrap_action, \"cpu\", GYM_CFG)\n",
    "\n",
    "    def new(self, cfg, bot_id, objective_id):\n",
    "        return MCarTask(cfg,\n",
    "                self.env,\n",
    "                objective_id, bot_id,\n",
    "                self.action_low, self.action_high,\n",
    "                self.rewarder)\n",
    "\n",
    "    @staticmethod\n",
    "    def factory(ind): # bare metal task creation\n",
    "        print(\"created %i-th task\"%ind)\n",
    "        CFG = toml.loads(open('cfg.toml').read())\n",
    "        return gym.make(CFG['task'])\n",
    "    \n",
    "#    def wrap_action(self, x):\n",
    "#        return torch.tanh(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implement callback for testing policy ~ per X training rounds, we want to test it ~ enable visuals if you want"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def callback(task, agent, scores):\n",
    "    try: callback.z += 1\n",
    "    except: callback.z = 0\n",
    "    \n",
    "    # we can save scores to main queue, and avarage them, or we can ..\n",
    "    # run testing w/ visuals :\n",
    "    done = all(task.test_policy(agent)[0] for _ in range(10))\n",
    "    if not done:\n",
    "        return False\n",
    "\n",
    "    print(\"\\n\")\n",
    "    print(\"=\"*80)\n",
    "    print(\"training over\", callback.z * GYM_CFG['n_simulations'] * GYM_CFG['mcts_rounds'])\n",
    "    print(\"=\"*80)\n",
    "\n",
    "    for i in range(100): print(\"total steps : training : %i :: %i >\"%(\n",
    "        callback.z * GYM_CFG['mcts_rounds'] * GYM_CFG['n_simulations'],\n",
    "        len(task.test_policy(agent)[2])))\n",
    "\n",
    "    return True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare neural network which we will be using"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models import ddpg_model, noisy_model\n",
    "\n",
    "def CriticNN(state_size, action_size, wrap_value, cfg):\n",
    "    return ddpg_model.Critic(state_size, action_size, wrap_value, cfg, fcs1_units=400, fc2_units=300)\n",
    "\n",
    "def ActorNN(state_size, action_size, wrap_action, cfg):\n",
    "    return noisy_model.Actor(state_size, action_size, wrap_action, cfg, hiddens=[256, 256, 128])\n",
    "    return ddpg_model.Actor(state_size, action_size, wrap_action, cfg, fc1_units=400, fc2_units=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select encoders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.encoders import *\n",
    "from utils.rnn import *#GRUEncoder\n",
    "\n",
    "def encoderstack(env):\n",
    "    encoder = RBFEncoder(GYM_CFG, env, [5., 2., 1., .5], [20] * 4)\n",
    "    #return encoder\n",
    "    \n",
    "    norm = GlobalNormalizer(GYM_CFG, encoder.total_size())\n",
    "    #norm = BatchNormalizer3D(GYM_CFG, encoder.total_size())\n",
    "    #return StackedEncoder(GYM_CFG, len(env.reset()), encoder, norm)\n",
    "    encoder = StackedEncoder(GYM_CFG, len(env.reset()), encoder, norm)\n",
    "\n",
    "    experience = GRUEncoder(GYM_CFG, encoder.total_size())#GRU#LSTM\n",
    "    encoder_norm = StackedEncoder(GYM_CFG, len(env.reset()), encoder, experience)\n",
    "    \n",
    "    return encoder_norm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cook Task : replay buffer ( fast / prio-gae-rnn ) + task manager ( local / remote / unity )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def taskfactory(env):\n",
    "    #return MCarInfo(env, Memory, MCarInfo.factory, LocalTaskManager, ())\n",
    "    return MCarInfo(env, ReplayBuffer, MCarInfo.factory, LocalTaskManager, ())\n",
    "    return MCarInfo(env, ReplayBuffer, MCarInfo.factory, RemoteTaskManager, (LocalTaskManager, 1 + GYM_CFG['n_simulations']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Glue it all together ~ select buffer, encoders, agents, ... and RUN!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'tensor': 'torch.DoubleTensor', 'task': 'MountainCarContinuous-v0', 'total_simulations': 2, 'cross_exp_size': 5000, 'max_reward_val': 1000, 'min_reward_val': -1000}\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "created 0-th task\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "created 1-th task\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "[0>   1::     0] training =  6, steps = 300, max_step =  67, reward=-238.428510 ::[[-1.]]: [ TARGET:-1.061843 replay::1567 ]<----- ]<----TEST :  -99.8999999999986\n",
      "[0>   2::     0] training =  4, steps =  96, max_step =  67, reward=-70.952219 ::[[-1.]]: [ TARGET:-0.986823 replay::3067 ]<------"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    print(CFG)\n",
    "\n",
    "    env = gym.make(CFG['task'])\n",
    "    \n",
    "    encoder = encoderstack(env)\n",
    "    task_factory = taskfactory(env)\n",
    "    task = task_factory.new(GYM_CFG, 0, -1)\n",
    "    \n",
    "    def callback_task(agent, stop_q):\n",
    "        return callback(task, agent, stop_q)\n",
    "    \n",
    "    stop_q = Queue()\n",
    "    agent_launch(0, GYM_CFG, task_factory, encoder, ActorNN, CriticNN, stop_q, callback_task)\n",
    "\n",
    "if '__main__' == __name__:\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
