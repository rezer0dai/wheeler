{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MountainCarContinuous-v0 environment of OpenAi GYM \n",
    "- *Wheeler task definition ( task wrapper, State decoder settings, NeuralNetwork, ReplayBuffer, .. )*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import generics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os, time\n",
    "\n",
    "os.chdir(\"..\")\n",
    "\n",
    "import numpy as np\n",
    "import toml, gym\n",
    "\n",
    "import torch\n",
    "from torch.multiprocessing import Queue, Process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load task configs ~ this should be adopted offline for particular task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "CFG = toml.loads(open('cfg.toml').read())\n",
    "GYM_CFG = toml.loads(open('gym.toml').read())\n",
    "\n",
    "torch.set_default_tensor_type(CFG['tensor'])\n",
    "\n",
    "CFG['task'] = \"MountainCarContinuous-v0\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import wheeler environment and particular utils we want to use ~ general ones ( shared across tasks )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from utils.task import Task\n",
    "from utils.taskinfo import *\n",
    "\n",
    "from utils.rbf import *\n",
    "from utils.normalizer import *\n",
    "\n",
    "from utils.taskmgr import *\n",
    "from utils.replay import *\n",
    "\n",
    "from utils.fastmem import Memory\n",
    "\n",
    "from utils.curiosity import *\n",
    "\n",
    "from agent.zer0bot import agent_launch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Task wrapper ~ when is goal met, how to step ( update rewards function, .. ), when / how to reset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class MCarTask(Task):\n",
    "    def __init__(self, cfg, env, objective_id, bot_id, action_low, action_high, rewarder):\n",
    "        self.reward = 0\n",
    "        self.rewards = []\n",
    "\n",
    "        super().__init__(\n",
    "                cfg,\n",
    "                env,\n",
    "                objective_id,\n",
    "                bot_id,\n",
    "                action_low, action_high)\n",
    "\n",
    "        self.rewarder = rewarder\n",
    "\n",
    "    def reset(self, seed = None, test = False):\n",
    "        state = super().reset(seed, test)\n",
    "        self.rewards.append(self.reward)\n",
    "        self.reward = 0\n",
    "        self.prev_state = state[0]\n",
    "        return state\n",
    "\n",
    "    def step_ex(self, action, test = False):\n",
    "        state, reward, done, _ = self.env.step(self.bot_id, self.objective_id, action)\n",
    "\n",
    "        if test: return action, state.reshape(1, -1), reward, done, True\n",
    "\n",
    "        self.reward += (done and reward > 0)\n",
    "\n",
    "        curiosity_test = '''\n",
    "        if not test and reward < 0:\n",
    "            ss, ns = np.vstack([s] * self.cfg['history_count']), np.vstack([n] * self.cfg['history_count'])\n",
    "            ss = ss.reshape(1, -1)\n",
    "            ns = ns.reshape(1, -1)\n",
    "            reward = self.rewarder.weight(ss, ns, action)[0]\n",
    "            self.rewarder.update(ss, ns, action)\n",
    "        self.prev_state = state\n",
    "\n",
    "        reward_update  = '''\n",
    "        if not test and not done:# and sum(self.rewards) < 3\n",
    "            true_state = np.abs(np.cos(np.pi/3.) + state[0])\n",
    "            reward += -(1. - true_state)\n",
    "#        '''\n",
    "\n",
    "        return action, state, reward, done, True\n",
    "\n",
    "    def goal_met(self, states, rewards, n_steps):\n",
    "        print(\"TEST : \", sum(rewards))\n",
    "        return sum(rewards) > 90."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generic proxy for creating our Task ( multiprocess environments purpose mainly ) \n",
    "- but can also add wrapping function approx values ( action value to tanh, sigmoid, .. ) - this not works well with PPO now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class MCarInfo(TaskInfo):\n",
    "    def __init__(self, env, replaybuf, factory, Mgr, args):\n",
    "        super().__init__(\n",
    "                len(env.reset()), 1, -1, +1,\n",
    "                CFG,\n",
    "                replaybuf,\n",
    "                factory, Mgr, args)\n",
    "\n",
    "        self.rewarder = CuriosityPrio(\n",
    "                self.state_size, self.action_size,\n",
    "                self.action_range, self.wrap_action, \"cpu\", GYM_CFG)\n",
    "\n",
    "    def new(self, cfg, bot_id, objective_id):\n",
    "        return MCarTask(cfg,\n",
    "                self.env,\n",
    "                objective_id, bot_id,\n",
    "                self.action_low, self.action_high,\n",
    "                self.rewarder)\n",
    "\n",
    "    @staticmethod\n",
    "    def factory(ind): # bare metal task creation\n",
    "        print(\"created %i-th task\"%ind)\n",
    "        CFG = toml.loads(open('cfg.toml').read())\n",
    "        return gym.make(CFG['task'])\n",
    "    \n",
    "#    def wrap_action(self, x):\n",
    "#        return torch.tanh(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implement callback for testing policy ~ per X training rounds, we want to test it ~ enable visuals if you want"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def callback(task, agent, scores):\n",
    "    try: callback.z += 1\n",
    "    except: callback.z = 0\n",
    "    \n",
    "    # we can save scores to main queue, and avarage them, or we can ..\n",
    "    # run testing w/ visuals :\n",
    "    done = all(task.test_policy(agent)[0] for _ in range(10))\n",
    "    if not done:\n",
    "        return False\n",
    "\n",
    "    print(\"\\n\")\n",
    "    print(\"=\"*80)\n",
    "    print(\"training over\", callback.z * GYM_CFG['n_simulations'] * GYM_CFG['mcts_rounds'])\n",
    "    print(\"=\"*80)\n",
    "\n",
    "    for i in range(100): print(\"total steps : training : %i :: %i >\"%(\n",
    "        callback.z * GYM_CFG['mcts_rounds'] * GYM_CFG['n_simulations'],\n",
    "        len(task.test_policy(agent)[2])))\n",
    "\n",
    "    return True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare neural network which we will be using"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from models import ddpg_model, noisy_model\n",
    "\n",
    "def CriticNN(state_size, action_size, wrap_value, cfg):\n",
    "    return ddpg_model.Critic(state_size, action_size, wrap_value, cfg, fcs1_units=400, fc2_units=300)\n",
    "\n",
    "def ActorNN(state_size, action_size, wrap_action, cfg):\n",
    "    #return noisy_model.Actor(state_size, action_size, wrap_action, cfg, hiddens=[400, 300])\n",
    "    return ddpg_model.Actor(state_size, action_size, wrap_action, cfg, fc1_units=400, fc2_units=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select encoders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from utils.encoders import *\n",
    "from utils.rnn import *#GRUEncoder\n",
    "\n",
    "def encoderstack(env):\n",
    "    encoder = RBFEncoder(GYM_CFG, env, [5., 2., 1., .5], [20] * 4)\n",
    "    return encoder\n",
    "    \n",
    "    norm = GlobalNormalizer(GYM_CFG, encoder.total_size())\n",
    "    #norm = BatchNormalizer3D(GYM_CFG, encoder.total_size())\n",
    "    #return StackedEncoder(GYM_CFG, len(env.reset()), encoder, norm)\n",
    "    encoder = StackedEncoder(GYM_CFG, len(env.reset()), encoder, norm)\n",
    "\n",
    "    experience = GRUEncoder(GYM_CFG, encoder.total_size())#GRU#LSTM\n",
    "    encoder_norm = StackedEncoder(GYM_CFG, len(env.reset()), encoder, experience)\n",
    "    \n",
    "    return encoder_norm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cook Task : replay buffer ( fast / prio-gae-rnn ) + task manager ( local / remote / unity )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def taskfactory(env):\n",
    "    return MCarInfo(env, Memory, MCarInfo.factory, LocalTaskManager, ())\n",
    "    return MCarInfo(env, ReplayBuffer, MCarInfo.factory, LocalTaskManager, ())\n",
    "    return MCarInfo(env, ReplayBuffer, MCarInfo.factory, RemoteTaskManager, (LocalTaskManager, 1 + GYM_CFG['n_simulations']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Glue it all together ~ select buffer, encoders, agents, ... and RUN!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'total_simulations': 2, 'task': 'MountainCarContinuous-v0', 'tensor': 'torch.DoubleTensor', 'min_reward_val': -1000, 'cross_exp_size': 5000, 'max_reward_val': 1000}\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "created 0-th task\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "created 1-th task\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "[0>   1::     0] training =  0, steps =  32, max_step = 10000, reward=-30.995581 ::[[0.00151748]]: "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py:547: UserWarning: Multiprocessing-backed parallel loops cannot be nested below threads, setting n_jobs=1\n",
      "  **self._backend_args)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0>   1::     0] training =  2, steps = 300, max_step = 300, reward=-262.943080 ::[[0.89808108]]: TEST :  -51.37895182121855\n",
      "[0>   2::     0] training =  2, steps = 300, max_step = 300, reward=-266.271024 ::[[-1.]]: TEST :  -28.105261498906746\n",
      "[0>   3::     0] training =  2, steps = 300, max_step = 300, reward=-248.741098 ::[[-1.]]: TEST :  -95.53901849205347\n",
      "[0>   4::     0] training =  2, steps = 300, max_step = 300, reward=-246.139837 ::[[-1.]]: TEST :  -98.64968228312753\n",
      "[0>   5::     0] training =  2, steps = 300, max_step = 300, reward=-251.954318 ::[[-1.]]: TEST :  -95.56050903777071\n",
      "[0>   6::     0] training =  2, steps = 300, max_step = 300, reward=-247.936222 ::[[-1.]]: TEST :  -97.20394999103429\n",
      "[0>   7::     0] training =  2, steps = 300, max_step = 300, reward=-255.717551 ::[[1.]]: TEST :  -99.8999999999986\n",
      "[0>   8::     0] training =  2, steps = 300, max_step = 300, reward=-266.543367 ::[[1.]]: TEST :  -99.8999999999986\n",
      "[0>   9::     0] training =  2, steps = 300, max_step = 300, reward=-245.359421 ::[[1.]]: TEST :  -99.8999999999986\n",
      "[0>  10::     0] training =  2, steps = 300, max_step = 300, reward=-259.749673 ::[[1.]]: TEST :  -99.8999999999986\n",
      "[0>  11::     0] training =  2, steps = 300, max_step = 300, reward=-268.077570 ::[[1.]]: TEST :  -99.8999999999986\n",
      "[0>  12::     0] training =  2, steps = 300, max_step = 300, reward=-252.427255 ::[[1.]]: TEST :  -99.8999999999986\n",
      "[0>  13::     0] training =  2, steps = 300, max_step = 300, reward=-268.911999 ::[[1.]]: TEST :  -99.8999999999986\n",
      "[0>  14::     0] training =  2, steps = 300, max_step = 300, reward=-261.999620 ::[[1.]]: TEST :  -99.8999999999986\n",
      "[0>  15::     0] training =  2, steps = 300, max_step = 300, reward=-254.482512 ::[[1.]]: TEST :  -99.8999999999986\n",
      "[0>  16::     0] training =  2, steps = 300, max_step = 300, reward=-269.495421 ::[[1.]]: TEST :  -99.8999999999986\n",
      "[0>  17::     0] training =  2, steps = 300, max_step = 300, reward=-267.889375 ::[[1.]]: TEST :  -99.8999999999986\n",
      "[0>  18::     0] training =  2, steps = 300, max_step = 300, reward=-248.341703 ::[[1.]]: TEST :  -99.8999999999986\n",
      "[0>  19::     0] training =  2, steps = 300, max_step = 300, reward=-266.106137 ::[[1.]]: TEST :  -99.8999999999986\n",
      "[0>  20::     0] training =  2, steps = 300, max_step = 300, reward=-250.837380 ::[[1.]]: TEST :  -99.8999999999986\n",
      "[0>  21::     0] training =  2, steps = 300, max_step = 300, reward=-263.096121 ::[[1.]]: TEST :  -99.8999999999986\n",
      "[0>  22::     0] training =  2, steps = 300, max_step = 300, reward=-268.392243 ::[[1.]]: TEST :  -99.8999999999986\n",
      "[0>  23::     0] training =  2, steps = 300, max_step = 300, reward=-247.380556 ::[[1.]]: TEST :  -99.8999999999986\n",
      "[0>  24::     0] training =  2, steps = 300, max_step = 300, reward=-267.286043 ::[[1.]]: TEST :  -99.8999999999986\n",
      "[0>  25::     0] training =  2, steps = 300, max_step = 300, reward=-260.768923 ::[[1.]]: TEST :  -99.8999999999986\n",
      "[0>  26::     0] training =  2, steps = 300, max_step = 300, reward=-249.878784 ::[[1.]]: TEST :  -99.8999999999986\n",
      "[0>  27::     0] training =  2, steps = 300, max_step = 300, reward=-267.855380 ::[[1.]]: TEST :  -99.8999999999986\n",
      "[0>  28::     0] training =  2, steps = 300, max_step = 300, reward=-264.116856 ::[[1.]]: TEST :  -99.8999999999986\n",
      "[0>  29::     0] training =  2, steps = 300, max_step = 300, reward=-268.353588 ::[[1.]]: TEST :  -99.8999999999986\n",
      "[0>  30::     0] training =  2, steps = 300, max_step = 300, reward=-261.648099 ::[[1.]]: TEST :  -99.8999999999986\n",
      "[0>  31::     0] training =  2, steps = 300, max_step = 300, reward=-268.144255 ::[[1.]]: TEST :  -99.8999999999986\n",
      "[0>  32::     0] training =  2, steps = 300, max_step = 300, reward=-261.899523 ::[[1.]]: TEST :  -99.8999999999986\n",
      "[0>  33::     0] training =  2, steps = 300, max_step = 300, reward=-266.135964 ::[[1.]]: TEST :  -99.8999999999986\n",
      "[0>  34::     0] training =  2, steps = 300, max_step = 300, reward=-245.792577 ::[[1.]]: TEST :  -99.8999999999986\n",
      "[0>  35::     0] training =  2, steps = 300, max_step = 300, reward=-244.863765 ::[[1.]]: TEST :  -99.8999999999986\n",
      "[0>  36::     0] training =  2, steps = 300, max_step = 300, reward=-269.537312 ::[[1.]]: TEST :  -99.8999999999986\n",
      "[0>  37::     0] training =  2, steps = 300, max_step = 300, reward=-265.359478 ::[[1.]]: TEST :  -99.8999999999986\n",
      "[0>  38::     0] training =  2, steps = 300, max_step = 300, reward=-257.788003 ::[[1.]]: TEST :  -99.8999999999986\n",
      "[0>  39::     0] training =  2, steps = 300, max_step = 300, reward=-255.580945 ::[[1.]]: TEST :  -99.8999999999986\n",
      "[0>  40::     0] training =  2, steps = 300, max_step = 300, reward=-262.269994 ::[[1.]]: TEST :  -99.8999999999986\n",
      "[0>  41::     0] training =  2, steps = 300, max_step = 300, reward=-258.184761 ::[[1.]]: TEST :  -99.8999999999986\n",
      "[0>  42::     0] training =  2, steps = 300, max_step = 300, reward=-264.256370 ::[[1.]]: TEST :  -99.8999999999986\n",
      "[0>  43::     0] training =  2, steps = 300, max_step = 300, reward=-260.074249 ::[[1.]]: TEST :  -99.8999999999986\n",
      "[0>  44::     0] training =  2, steps = 300, max_step = 300, reward=-258.759610 ::[[1.]]: TEST :  -99.8999999999986\n",
      "[0>  45::     0] training =  2, steps = 300, max_step = 300, reward=-245.092462 ::[[1.]]: TEST :  -99.8999999999986\n",
      "[0>  46::     0] training =  2, steps = 300, max_step = 300, reward=-260.389880 ::[[1.]]: TEST :  -99.8999999999986\n",
      "[0>  47::     0] training =  2, steps = 300, max_step = 300, reward=-262.094833 ::[[1.]]: TEST :  -99.8999999999986\n",
      "[0>  48::     0] training =  2, steps = 300, max_step = 300, reward=-257.010895 ::[[1.]]: TEST :  -99.8999999999986\n",
      "[0>  49::     0] training =  2, steps = 300, max_step = 300, reward=-269.775813 ::[[1.]]: TEST :  -99.8999999999986\n",
      "[0>  50::     0] training =  2, steps = 300, max_step = 300, reward=-267.285737 ::[[1.]]: TEST :  -99.8999999999986\n",
      "[0>  51::     0] training =  2, steps = 300, max_step = 300, reward=-264.986796 ::[[1.]]: TEST :  -99.8999999999986\n",
      "[0>  52::     0] training =  2, steps = 300, max_step = 300, reward=-253.189612 ::[[1.]]: TEST :  -99.8999999999986\n",
      "[0>  53::     0] training =  2, steps = 300, max_step = 300, reward=-246.829994 ::[[1.]]: TEST :  -99.8999999999986\n",
      "[0>  54::     0] training =  2, steps = 300, max_step = 300, reward=-269.357010 ::[[1.]]: TEST :  -99.8999999999986\n",
      "[0>  55::     0] training =  2, steps = 300, max_step = 300, reward=-264.951730 ::[[1.]]: TEST :  -99.8999999999986\n",
      "[0>  56::     0] training =  2, steps = 300, max_step = 300, reward=-264.214867 ::[[1.]]: TEST :  -99.8999999999986\n",
      "[0>  57::     0] training =  2, steps = 300, max_step = 300, reward=-268.816727 ::[[1.]]: TEST :  -99.8999999999986\n",
      "[0>  58::     0] training =  2, steps = 300, max_step = 300, reward=-268.652943 ::[[1.]]: TEST :  -99.8999999999986\n",
      "[0>  59::     0] training =  2, steps = 300, max_step = 300, reward=-249.360757 ::[[1.]]: TEST :  -99.8999999999986\n",
      "[0>  60::     0] training =  2, steps = 300, max_step = 300, reward=-248.868070 ::[[1.]]: TEST :  -99.8999999999986\n",
      "[0>  61::     0] training =  2, steps = 300, max_step = 300, reward=-267.007294 ::[[1.]]: TEST :  -99.8999999999986\n",
      "[0>  62::     0] training =  2, steps = 300, max_step = 300, reward=-262.133846 ::[[1.]]: TEST :  -99.8999999999986\n",
      "[0>  63::     0] training =  2, steps = 300, max_step = 300, reward=-267.112736 ::[[1.]]: TEST :  -99.8999999999986\n",
      "[0>  64::     0] training =  2, steps = 300, max_step = 300, reward=-258.276675 ::[[1.]]: TEST :  -99.8999999999986\n",
      "[0>  65::     0] training =  2, steps = 300, max_step = 300, reward=-265.282242 ::[[1.]]: TEST :  -99.8999999999986\n",
      "[0>  66::     0] training =  2, steps = 300, max_step = 300, reward=-268.527209 ::[[1.]]: TEST :  -99.8999999999986\n",
      "[0>  67::     0] training =  2, steps = 300, max_step = 300, reward=-267.455453 ::[[1.]]: TEST :  -99.8999999999986\n",
      "[0>  68::     0] training =  2, steps = 300, max_step = 300, reward=-249.596259 ::[[1.]]: TEST :  -99.8999999999986\n",
      "[0>  69::     0] training =  2, steps = 300, max_step = 300, reward=-252.625288 ::[[1.]]: TEST :  -99.8999999999986\n",
      "[0>  70::     0] training =  2, steps = 300, max_step = 300, reward=-263.978666 ::[[1.]]: TEST :  -99.8999999999986\n",
      "[0>  71::     0] training =  2, steps = 300, max_step = 300, reward=-260.467615 ::[[1.]]: TEST :  -99.8999999999986\n",
      "[0>  72::     0] training =  2, steps = 300, max_step = 300, reward=-265.413267 ::[[1.]]: TEST :  -99.8999999999986\n",
      "[0>  73::     0] training =  2, steps = 300, max_step = 300, reward=-267.159335 ::[[1.]]: TEST :  -99.8999999999986\n",
      "[0>  74::     0] training =  2, steps = 300, max_step = 300, reward=-257.151203 ::[[1.]]: TEST :  -99.8999999999986\n",
      "[0>  75::     0] training =  2, steps = 300, max_step = 300, reward=-259.035703 ::[[1.]]: TEST :  -99.8999999999986\n",
      "[0>  76::     0] training =  2, steps = 300, max_step = 300, reward=-269.931075 ::[[1.]]: TEST :  -99.8999999999986\n",
      "[0>  77::     0] training =  2, steps = 300, max_step = 300, reward=-264.371973 ::[[1.]]: TEST :  -99.8999999999986\n",
      "[0>  78::     0] training =  2, steps = 300, max_step = 300, reward=-241.566316 ::[[1.]]: TEST :  -99.8999999999986\n",
      "[0>  79::     0] training =  2, steps = 300, max_step = 300, reward=-262.330406 ::[[1.]]: TEST :  -99.8999999999986\n",
      "[0>  80::     0] training =  2, steps = 300, max_step = 300, reward=-268.908937 ::[[1.]]: TEST :  -99.8999999999986\n",
      "[0>  81::     0] training =  2, steps = 300, max_step = 300, reward=-264.806298 ::[[1.]]: TEST :  -99.8999999999986\n",
      "[0>  82::     0] training =  2, steps = 300, max_step = 300, reward=-268.284721 ::[[1.]]: TEST :  -99.8999999999986\n",
      "[0>  83::     0] training =  2, steps = 300, max_step = 300, reward=-251.753309 ::[[1.]]: TEST :  -99.8999999999986\n",
      "[0>  84::     0] training =  2, steps = 300, max_step = 300, reward=-267.243567 ::[[1.]]: TEST :  -99.8999999999986\n",
      "[0>  85::     0] training =  2, steps = 300, max_step = 300, reward=-264.676238 ::[[1.]]: TEST :  -99.8999999999986\n",
      "[0>  86::     0] training =  2, steps = 300, max_step = 300, reward=-250.559207 ::[[1.]]: TEST :  -99.8999999999986\n",
      "[0>  87::     0] training =  2, steps = 300, max_step = 300, reward=-269.445992 ::[[1.]]: TEST :  -99.8999999999986\n",
      "[0>  88::     0] training =  2, steps = 300, max_step = 300, reward=-267.532646 ::[[1.]]: TEST :  -99.8999999999986\n",
      "[0>  89::     0] training =  2, steps = 300, max_step = 300, reward=-262.575284 ::[[1.]]: TEST :  -99.8999999999986\n",
      "[0>  90::     0] training =  2, steps = 300, max_step = 300, reward=-261.666391 ::[[1.]]: TEST :  -99.8999999999986\n",
      "[0>  91::     0] training =  2, steps = 300, max_step = 300, reward=-268.418288 ::[[1.]]: TEST :  -99.8999999999986\n",
      "[0>  92::     0] training =  2, steps = 300, max_step = 300, reward=-266.046304 ::[[1.]]: TEST :  -99.8999999999986\n",
      "[0>  93::     0] training =  2, steps = 300, max_step = 300, reward=-267.657996 ::[[1.]]: TEST :  -99.8999999999986\n",
      "[0>  94::     0] training =  2, steps = 300, max_step = 300, reward=-250.517269 ::[[1.]]: TEST :  -99.8999999999986\n",
      "[0>  95::     0] training =  2, steps = 300, max_step = 300, reward=-267.237024 ::[[1.]]: TEST :  -99.8999999999986\n",
      "[0>  96::     0] training =  2, steps = 300, max_step = 300, reward=-263.186328 ::[[1.]]: TEST :  -99.8999999999986\n",
      "[0>  97::     0] training =  2, steps = 300, max_step = 300, reward=-244.180744 ::[[1.]]: TEST :  -99.8999999999986\n",
      "[0>  98::     0] training =  2, steps = 300, max_step = 300, reward=-264.173987 ::[[1.]]: TEST :  -99.8999999999986\n",
      "[0>  99::     0] training =  2, steps = 300, max_step = 300, reward=-261.139202 ::[[1.]]: TEST :  -99.8999999999986\n",
      "[0> 100::     0] training =  2, steps = 300, max_step = 300, reward=-265.413145 ::[[1.]]: TEST :  -99.8999999999986\n",
      "[0> 101::     0] training =  2, steps = 300, max_step = 300, reward=-259.435899 ::[[1.]]: TEST :  -99.8999999999986\n",
      "[0> 102::     0] training =  2, steps = 300, max_step = 300, reward=-263.899848 ::[[1.]]: TEST :  -99.8999999999986\n",
      "[0> 103::     0] training =  2, steps = 300, max_step = 300, reward=-261.575695 ::[[1.]]: TEST :  -99.8999999999986\n",
      "[0> 104::     0] training =  2, steps = 300, max_step = 300, reward=-247.043662 ::[[1.]]: TEST :  -99.8999999999986\n",
      "[0> 105::     0] training =  2, steps = 300, max_step = 300, reward=-267.614050 ::[[1.]]: TEST :  -99.8999999999986\n",
      "[0> 106::     0] training =  2, steps = 300, max_step = 300, reward=-263.752055 ::[[1.]]: TEST :  -99.8999999999986\n",
      "[0> 107::     0] training =  2, steps = 300, max_step = 300, reward=-264.543071 ::[[1.]]: TEST :  -99.8999999999986\n",
      "[0> 108::     0] training =  2, steps = 300, max_step = 300, reward=-266.114426 ::[[1.]]: TEST :  -99.8999999999986\n",
      "[0> 109::     0] training =  2, steps = 300, max_step = 300, reward=-247.931429 ::[[1.]]: TEST :  -99.8999999999986\n",
      "[0> 110::     0] training =  2, steps = 300, max_step = 300, reward=-255.118829 ::[[1.]]: TEST :  -99.8999999999986\n",
      "[0> 111::     0] training =  2, steps = 300, max_step = 300, reward=-268.879045 ::[[1.]]: TEST :  -99.8999999999986\n",
      "[0> 112::     0] training =  2, steps = 300, max_step = 300, reward=-268.780860 ::[[1.]]: TEST :  -99.8999999999986\n",
      "[0> 113::     0] training =  2, steps = 300, max_step = 300, reward=-259.145478 ::[[1.]]: TEST :  -99.8999999999986\n",
      "[0> 114::     0] training =  2, steps = 300, max_step = 300, reward=-262.747033 ::[[1.]]: TEST :  -99.8999999999986\n",
      "[0> 115::     0] training =  2, steps = 300, max_step = 300, reward=-260.758885 ::[[1.]]: TEST :  -99.8999999999986\n",
      "[0> 116::     0] training =  2, steps = 300, max_step = 300, reward=-269.253177 ::[[1.]]: TEST :  -99.8999999999986\n",
      "[0> 117::     0] training =  2, steps = 300, max_step = 300, reward=-250.312803 ::[[1.]]: TEST :  -99.8999999999986\n",
      "[0> 118::     0] training =  2, steps = 300, max_step = 300, reward=-263.680360 ::[[1.]]: TEST :  -99.8999999999986\n",
      "[0> 119::     0] training =  2, steps = 300, max_step = 300, reward=-260.673125 ::[[1.]]: TEST :  -99.8999999999986\n",
      "[0> 120::     0] training =  2, steps = 300, max_step = 300, reward=-269.896315 ::[[1.]]: TEST :  -99.8999999999986\n",
      "[0> 121::     0] training =  2, steps = 300, max_step = 300, reward=-267.685445 ::[[1.]]: TEST :  -99.8999999999986\n",
      "[0> 122::     0] training =  2, steps = 300, max_step = 300, reward=-264.929091 ::[[1.]]: TEST :  -99.8999999999986\n",
      "[0> 123::     0] training =  2, steps = 300, max_step = 300, reward=-257.832338 ::[[1.]]: TEST :  -99.8999999999986\n",
      "[0> 124::     0] training =  2, steps = 300, max_step = 300, reward=-247.875142 ::[[1.]]: TEST :  -99.8999999999986\n",
      "[0> 125::     0] training =  2, steps = 300, max_step = 300, reward=-268.533728 ::[[1.]]: TEST :  -99.8999999999986\n",
      "[0> 126::     0] training =  2, steps = 300, max_step = 300, reward=-260.715095 ::[[1.]]: TEST :  -99.8999999999986\n",
      "[0> 127::     0] training =  2, steps = 300, max_step = 300, reward=-267.393002 ::[[1.]]: TEST :  -99.8999999999986\n",
      "[0> 128::     0] training =  2, steps = 300, max_step = 300, reward=-261.915215 ::[[1.]]: TEST :  -99.8999999999986\n",
      "[0> 129::     0] training =  2, steps = 300, max_step = 300, reward=-269.481775 ::[[1.]]: TEST :  -99.8999999999986\n",
      "[0> 130::     0] training =  2, steps = 300, max_step = 300, reward=-265.915235 ::[[1.]]: TEST :  -99.8999999999986\n",
      "[0> 131::     0] training =  2, steps = 300, max_step = 300, reward=-265.918545 ::[[1.]]: TEST :  -99.8999999999986\n",
      "[0> 132::     0] training =  2, steps = 300, max_step = 300, reward=-264.754581 ::[[1.]]: TEST :  -99.8999999999986\n",
      "[0> 133::     0] training =  2, steps = 300, max_step = 300, reward=-261.901069 ::[[1.]]: TEST :  -99.8999999999986\n",
      "[0> 134::     0] training =  2, steps = 300, max_step = 300, reward=-268.900774 ::[[1.]]: TEST :  -99.8999999999986\n",
      "[0> 135::     0] training =  2, steps = 300, max_step = 300, reward=-261.507277 ::[[1.]]: TEST :  -99.8999999999986\n",
      "[0> 136::     0] training =  2, steps = 300, max_step = 300, reward=-267.510207 ::[[1.]]: TEST :  -99.8999999999986\n",
      "[0> 137::     0] training =  2, steps = 300, max_step = 300, reward=-261.308967 ::[[1.]]: TEST :  -99.8999999999986\n",
      "[0> 138::     0] training =  2, steps = 300, max_step = 300, reward=-266.750718 ::[[1.]]: TEST :  -99.8999999999986\n",
      "[0> 139::     0] training =  2, steps = 300, max_step = 300, reward=-267.039391 ::[[1.]]: TEST :  -99.8999999999986\n",
      "[0> 140::     0] training =  2, steps = 300, max_step = 300, reward=-265.982370 ::[[1.]]: TEST :  -99.8999999999986\n",
      "[0> 141::     0] training =  2, steps = 300, max_step = 300, reward=-242.214673 ::[[1.]]: TEST :  -99.8999999999986\n",
      "[0> 142::     0] training =  2, steps = 300, max_step = 300, reward=-259.239896 ::[[1.]]: TEST :  -99.8999999999986\n",
      "[0> 143::     0] training =  2, steps = 300, max_step = 300, reward=-257.922465 ::[[1.]]: TEST :  -99.8999999999986\n",
      "[0> 144::     0] training =  2, steps = 300, max_step = 300, reward=-254.761771 ::[[1.]]: TEST :  -99.8999999999986\n",
      "[0> 145::     0] training =  2, steps = 300, max_step = 300, reward=-268.875453 ::[[1.]]: TEST :  -99.8999999999986\n",
      "[0> 146::     0] training =  2, steps = 300, max_step = 300, reward=-266.504223 ::[[1.]]: TEST :  -99.8999999999986\n",
      "[0> 147::     0] training =  2, steps = 300, max_step = 300, reward=-257.922317 ::[[1.]]: TEST :  -99.8999999999986\n",
      "[0> 148::     0] training =  2, steps = 300, max_step = 300, reward=-247.538614 ::[[1.]]: TEST :  -99.8999999999986\n",
      "[0> 149::     0] training =  2, steps = 300, max_step = 300, reward=-261.405438 ::[[1.]]: TEST :  -99.8999999999986\n",
      "[0> 150::     0] training =  2, steps = 300, max_step = 300, reward=-261.293755 ::[[1.]]: TEST :  -99.8999999999986\n",
      "[0> 151::     0] training =  2, steps = 300, max_step = 300, reward=-249.773574 ::[[1.]]: TEST :  -99.8999999999986\n",
      "[0> 152::     0] training =  2, steps = 300, max_step = 300, reward=-246.578209 ::[[1.]]: TEST :  -99.8999999999986\n",
      "[0> 153::     0] training =  2, steps = 300, max_step = 300, reward=-267.526026 ::[[1.]]: TEST :  -99.8999999999986\n",
      "[0> 154::     0] training =  2, steps = 300, max_step = 300, reward=-266.782018 ::[[1.]]: TEST :  -99.8999999999986\n",
      "[0> 155::     0] training =  2, steps = 300, max_step = 300, reward=-268.185641 ::[[1.]]: TEST :  -99.8999999999986\n",
      "[0> 156::     0] training =  2, steps = 300, max_step = 300, reward=-267.950382 ::[[1.]]: TEST :  -99.8999999999986\n",
      "[0> 157::     0] training =  2, steps = 300, max_step = 300, reward=-260.052104 ::[[1.]]: TEST :  -99.8999999999986\n",
      "[0> 158::     0] training =  2, steps = 300, max_step = 300, reward=-247.166661 ::[[1.]]: TEST :  -99.8999999999986\n",
      "[0> 159::     0] training =  2, steps = 300, max_step = 300, reward=-265.027607 ::[[1.]]: TEST :  -99.8999999999986\n",
      "[0> 160::     0] training =  2, steps = 300, max_step = 300, reward=-246.600527 ::[[1.]]: TEST :  -99.8999999999986\n",
      "[0> 161::     0] training =  2, steps = 300, max_step = 300, reward=-246.509208 ::[[1.]]: TEST :  -99.8999999999986\n",
      "[0> 162::     0] training =  2, steps = 228, max_step = 300, reward=-184.543199 ::[[1.]]: "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "$ Torch: not enough memory: you tried to allocate 0GB. Buy new RAM! at ..\\aten\\src\\TH\\THGeneral.cpp:204",
     "output_type": "error",
     "traceback": [
      "\n",
	]
    }
   ],
   "source": [
    "def main():\n",
    "    print(CFG)\n",
    "\n",
    "    env = gym.make(CFG['task'])\n",
    "    \n",
    "    encoder = encoderstack(env)\n",
    "    task_factory = taskfactory(env)\n",
    "    task = task_factory.new(GYM_CFG, 0, -1)\n",
    "    \n",
    "    def callback_task(agent, stop_q):\n",
    "        return callback(task, agent, stop_q)\n",
    "    \n",
    "    stop_q = Queue()\n",
    "    agent_launch(0, GYM_CFG, task_factory, encoder, ActorNN, CriticNN, stop_q, callback_task)\n",
    "\n",
    "if '__main__' == __name__:\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
