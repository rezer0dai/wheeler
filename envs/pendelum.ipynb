{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MountainCarContinuous-v0 environment of OpenAi GYM \n",
    "- *Wheeler task definition ( task wrapper, State decoder settings, NeuralNetwork, ReplayBuffer, .. )*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import generics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, time\n",
    "\n",
    "os.chdir(\"..\")\n",
    "\n",
    "import numpy as np\n",
    "import toml, gym\n",
    "\n",
    "import torch\n",
    "from torch.multiprocessing import Queue, Process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load task configs ~ this should be adopted offline for particular task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "CFG = toml.loads(open('cfg.toml').read())\n",
    "GYM_CFG = toml.loads(open('gym.toml').read())\n",
    "\n",
    "torch.set_default_tensor_type(CFG['tensor'])\n",
    "\n",
    "CFG['task'] = \"Pendulum-v0\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import wheeler environment and particular utils we want to use ~ general ones ( shared across tasks )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.task import Task\n",
    "from utils.taskinfo import *\n",
    "\n",
    "from utils.rbf import *\n",
    "from utils.normalizer import *\n",
    "\n",
    "from utils.taskmgr import *\n",
    "from utils.replay import *\n",
    "\n",
    "from utils.fastmem import Memory\n",
    "\n",
    "from utils.curiosity import *\n",
    "\n",
    "from agent.zer0bot import agent_launch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Task wrapper ~ when is goal met, how to step ( update rewards function, .. ), when / how to reset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GymTask(Task):\n",
    "    def step_ex(self, action, test = False):\n",
    "        state, reward, done, _ = self.env.step(self.bot_id, self.objective_id, action)\n",
    "\n",
    "        if test: return action, state.reshape(1, -1), reward, done, True\n",
    "        \n",
    "        return action, state, reward, done, True\n",
    "\n",
    "    def goal_met(self, states, rewards, n_steps):\n",
    "        print(\"TEST : \", sum(rewards))\n",
    "        return sum(rewards) > -150."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generic proxy for creating our Task ( multiprocess environments purpose mainly ) \n",
    "- but can also add wrapping function approx values ( action value to tanh, sigmoid, .. ) - this not works well with PPO now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GymInfo(TaskInfo):\n",
    "    def __init__(self, env, replaybuf, factory, Mgr, args):\n",
    "        super().__init__(\n",
    "                len(env.reset()), 1,\n",
    "                float(env.action_space.low[0]), float(env.action_space.high[0]),\n",
    "                CFG,\n",
    "                replaybuf,\n",
    "                factory, Mgr, args)\n",
    "\n",
    "    def new(self, cfg, bot_id, objective_id):\n",
    "        return GymTask(cfg,\n",
    "                self.env,\n",
    "                objective_id, bot_id,\n",
    "                self.action_low, self.action_high)\n",
    "\n",
    "    @staticmethod\n",
    "    def factory(ind): # bare metal task creation\n",
    "        global CFG\n",
    "        print(\"created %i-th task\"%ind)\n",
    "        return gym.make(CFG['task'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implement callback for testing policy ~ per X training rounds, we want to test it ~ enable visuals if you want"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def callback(task, agent, scores):\n",
    "    try: callback.z += 1\n",
    "    except: callback.z = 0\n",
    "    \n",
    "    # we can save scores to main queue, and avarage them, or we can ..\n",
    "    # run testing w/ visuals :\n",
    "    done = all(task.test_policy(agent)[0] for _ in range(10))\n",
    "    if not done:\n",
    "        return False\n",
    "\n",
    "    print(\"\\n\")\n",
    "    print(\"=\"*80)\n",
    "    print(\"training over\", callback.z * GYM_CFG['n_simulations'] * GYM_CFG['mcts_rounds'])\n",
    "    print(\"=\"*80)\n",
    "\n",
    "    for i in range(100): print(\"total steps : training : %i :: %i >\"%(\n",
    "        callback.z * GYM_CFG['mcts_rounds'] * GYM_CFG['n_simulations'],\n",
    "        len(task.test_policy(agent)[2])))\n",
    "\n",
    "    return True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare neural network which we will be using"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models import ddpg_model, noisy_model, state_action_model\n",
    "\n",
    "def CriticNN(state_size, action_size, wrap_value, cfg):\n",
    "    return state_action_model.Critic(state_size, action_size, wrap_value, cfg, fcs1_units=256, fc2_units=128)\n",
    "    return ddpg_model.Critic(state_size, action_size, wrap_value, cfg, fcs1_units=400, fc2_units=300)\n",
    "\n",
    "def ActorNN(state_size, action_size, wrap_action, cfg):\n",
    "    return noisy_model.Actor(state_size, action_size, wrap_action, cfg, hiddens=[128, 64])\n",
    "    return ddpg_model.Actor(state_size, action_size, wrap_action, cfg, fc1_units=400, fc2_units=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select encoders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.encoders import *\n",
    "from utils.rnn import *#GRUEncoder\n",
    "\n",
    "def encoderstack(env):\n",
    "    norm = GlobalNormalizer(GYM_CFG, len(env.reset()))\n",
    "    #return norm\n",
    "    experience = GRUEncoder(GYM_CFG, norm.total_size())#GRU#LSTM\n",
    "    encoder_norm = StackedEncoder(GYM_CFG, len(env.reset()), norm, experience)\n",
    "    encoder_norm.share_memory()\n",
    "    return encoder_norm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cook Task : replay buffer ( fast / prio-gae-rnn ) + task manager ( local / remote / unity )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def taskfactory(env):\n",
    "#    return GymInfo(env, Memory, GymInfo.factory, LocalTaskManager, ())\n",
    "    return GymInfo(env, ReplayBuffer, GymInfo.factory, LocalTaskManager, ())\n",
    "    return GymInfo(env, ReplayBuffer, GymInfo.factory, RemoteTaskManager, (LocalTaskManager, 1 + GYM_CFG['n_simulations']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Glue it all together ~ select buffer, encoders, agents, ... and RUN!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'tensor': 'torch.DoubleTensor', 'task': 'Pendulum-v0', 'total_simulations': 2, 'cross_exp_size': 5000, 'max_reward_val': 1000, 'min_reward_val': -1000}\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "created 0-th task\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "created 1-th task\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "[0>   1::     0] training =  0, steps = 174, max_step = 10000, reward=-997.664009 ::[[0.10530957]]: >>train tensor(-15.9997) 128:0 ]<----\n",
      "[0>   1::     0] training =  0, steps = 176, max_step = 10000, reward=-1020.593429 ::[[0.12948259]]: >>train tensor(-16.3213) 95:0 ]<----\n",
      "[0>   1::     0] training =  0, steps = 178, max_step = 10000, reward=-1036.592369 ::[[0.02025984]]: [ TARGET:-36.005557 replay::0 ]<---->>train tensor(-16.3040) 147\n",
      ">>train tensor(-16.4755) 128\n",
      ">>train tensor(-16.5372) 130\n",
      ">>train tensor(-16.5229) 114\n",
      ">>train tensor(-16.2254) 45\n",
      "[0>   1::     0] training =  0, steps = 183, max_step = 10000, reward=-1055.627391 ::[[0.01314665]]: >>train tensor(-16.0046) 160 ]<----\n",
      "[0>   1::     0] training =  0, steps = 200, max_step = 200, reward=-1165.427079 ::[[-0.08325185]]:  TEST :  -1035.585250976497\n",
      ">>train tensor(-14.1583) 25\n",
      ">>train tensor(-15.9593) 14\n",
      ">>train tensor(-10.2441) 8\n",
      ">>train tensor(-14.0338) 25\n",
      ">>train tensor(-13.7314) 29\n",
      ">>train tensor(-20.7706) 23\n",
      ">>train tensor(-14.0454) 38\n",
      ">>train tensor(-16.2933) 10\n",
      ">>train tensor(-34.1024) 1\n",
      ">>train tensor(-17.5499) 11\n",
      "[0>   2::     0] training =  0, steps =  56, max_step = 200, reward=-366.092037 ::[[0.00680847]]: >>train tensor(-16.0205) 44:200 ]<----\n",
      "[0>   2::     0] training =  0, steps =  60, max_step = 200, reward=-389.514342 ::[[-0.06665357]]: >>train tensor(-16.6807) 57:200 ]<----\n",
      "[0>   2::     0] training =  0, steps =  67, max_step = 200, reward=-454.722393 ::[[-0.01758961]]: >>train tensor(-15.3593) 36:200 ]<----\n",
      "[0>   2::     0] training =  0, steps =  73, max_step = 200, reward=-481.125004 ::[[0.08861754]]: >>train tensor(-15.7527) 27:200 ]<----\n",
      "[0>   2::     0] training =  0, steps =  83, max_step = 200, reward=-560.078157 ::[[0.01207795]]: [ TARGET:-7.260353 replay::200 ]<---->>train tensor(-15.4608) 71\n",
      "[0>   2::     0] training =  0, steps =  86, max_step = 200, reward=-579.781815 ::[[0.01739195]]: >>train tensor(-16.3000) 50200 ]<----\n",
      "[0>   2::     0] training =  0, steps =  88, max_step = 200, reward=-588.628111 ::[[0.03138404]]: [ TARGET:-32.739238 replay::200 ]<---->>train tensor(-17.8409) 62\n",
      ">>train tensor(-16.2440) 40\n",
      ">>train tensor(-16.8983) 44\n",
      ">>train tensor(-16.7807) 31\n",
      ">>train tensor(-15.8005) 33\n",
      ">>train tensor(-15.5963) 90\n",
      ">>train tensor(-16.9674) 49\n",
      "[0>   2::     0] training =  0, steps =  93, max_step = 200, reward=-608.698142 ::[[-0.05560385]]: >>train tensor(-13.9009) 44:200 ]<----\n",
      "[0>   2::     0] training =  0, steps = 103, max_step = 200, reward=-693.148510 ::[[-0.03425225]]: >>train tensor(-15.6563) 45\n",
      "[0>   2::     0] training =  0, steps = 117, max_step = 200, reward=-778.269799 ::[[-0.09499372]]: 80train tensor(-16.4079) \n",
      "[0>   2::     0] training =  0, steps = 135, max_step = 200, reward=-899.373369 ::[[0.01018215]]: >>train tensor(-15.5429) 40:200 ]<----\n",
      "[0>   2::     0] training =  0, steps = 140, max_step = 200, reward=-936.591308 ::[[-0.05368755]]: >>train tensor(-16.8573) 10700 ]<----\n",
      "[0>   2::     0] training =  0, steps = 150, max_step = 200, reward=-987.551721 ::[[-0.01364913]]: >>train tensor(-14.9530) 70200 ]<----\n",
      "[0>   2::     0] training =  0, steps = 164, max_step = 200, reward=-1080.858050 ::[[-0.01764478]]: >>train tensor(-16.8118) 101200 ]<----\n",
      "[0>   2::     0] training =  0, steps = 175, max_step = 200, reward=-1170.638307 ::[[-0.04149068]]: [ TARGET:-13.805617 replay::200 ]<---->>train tensor(-16.4582) 78\n",
      "[0>   2::     0] training =  0, steps = 190, max_step = 200, reward=-1270.379910 ::[[0.07623914]]: >>train tensor(-15.6336) 94:200 ]<----\n",
      "[0>   2::     0] training =  0, steps = 193, max_step = 200, reward=-1290.438014 ::[[0.03920904]]: >>train tensor(-17.8158) 83:200 ]<----\n",
      "[0>   2::     0] training =  0, steps = 198, max_step = 200, reward=-1310.723791 ::[[-0.04091768]]: >>train tensor(-15.9682) 12000 ]<----\n",
      "[0>   2::     0] training =  0, steps = 200, max_step = 200, reward=-1319.529732 ::[[-0.05491133]]: >>train tensor(-16.3950) 47\n",
      "TEST :  -1036.449395636449\n",
      ">>train tensor(-12.9295) 182\n",
      ">>train tensor(-12.9097) 155\n",
      ">>train tensor(-11.9642) 124\n",
      ">>train tensor(-13.3575) 173\n",
      ">>train tensor(-14.0300) 82\n",
      ">>train tensor(-14.3902) 107\n",
      ">>train tensor(-14.0473) 160\n",
      ">>train tensor(-11.8537) 33\n",
      ">>train tensor(-15.5165) 56\n",
      ">>train tensor(-11.5984) 50\n",
      ">>train tensor(-15.8899) 48\n",
      ">>train tensor(-15.5123) 40\n",
      ">>train tensor(-13.1289) 18\n",
      ">>train tensor(-17.4920) 71\n",
      ">>train tensor(-13.6956) 52\n",
      "[0>   3::     0] training =  0, steps =   5, max_step = 200, reward=-10.262189 ::[[-0.0986819]]: >>train tensor(-15.8423) 30400 ]<----\n",
      "[0>   3::     0] training =  0, steps =  22, max_step = 200, reward=-119.429495 ::[[-0.17697623]]: >>train tensor(-16.3437) 63\n",
      "[0>   3::     0] training =  0, steps =  51, max_step = 200, reward=-278.252010 ::[[-0.01718022]]:  tensor(-16.0873) 27eplay::400 ]<---->>train\n",
      "[0>   3::     0] training =  0, steps =  67, max_step = 200, reward=-363.694954 ::[[-0.01645284]]: >>train tensor(-13.5610) 66:400 ]<----\n",
      "[0>   3::     0] training =  0, steps =  73, max_step = 200, reward=-416.285552 ::[[-0.04067411]]: [ TARGET:-22.738528 replay::400 ]<---->>train tensor(-15.0645) 30\n",
      "[0>   3::     0] training =  0, steps =  92, max_step = 200, reward=-511.801239 ::[[-0.0460051]]: >>train tensor(-13.1035) 85::400 ]<----\n",
      "[0>   3::     0] training =  0, steps = 103, max_step = 200, reward=-586.924580 ::[[-0.08807114]]: [ TARGET:-10.374881 replay::400 ]<---->>train tensor(-12.3835) 35\n",
      "[0>   3::     0] training =  0, steps = 116, max_step = 200, reward=-672.670923 ::[[-0.04367718]]:  tensor(-14.5703) 69eplay::400 ]<---->>train\n",
      "[0>   3::     0] training =  0, steps = 123, max_step = 200, reward=-703.708368 ::[[-0.05864895]]: [ TARGET:-19.010263 replay::400 ]<---->>train tensor(-13.4587) 64\n",
      "[0>   3::     0] training =  0, steps = 127, max_step = 200, reward=-710.997011 ::[[-0.07366543]]: >>train tensor(-12.9804) 40400 ]<----\n",
      "[0>   3::     0] training =  0, steps = 136, max_step = 200, reward=-782.661722 ::[[-0.06988424]]: >>train tensor(-13.0167) 69:400 ]<----\n",
      "[0>   3::     0] training =  0, steps = 150, max_step = 200, reward=-837.941177 ::[[-0.00352189]]: >>train tensor(-14.8489) 85:400 ]<----\n",
      "[0>   3::     0] training =  0, steps = 162, max_step = 200, reward=-934.218077 ::[[-0.01379869]]: >>train tensor(-13.8486) 109400 ]<----\n",
      ">>train tensor(-13.1130) 48\n",
      ">>train tensor(-13.6653) 62\n",
      ">>train tensor(-12.5229) 107\n",
      "[0>   3::     0] training =  0, steps = 163, max_step = 200, reward=-936.634183 ::[[-0.00504329]]: [ TARGET:-32.646182 replay::400 ]<---->>train tensor(-12.3832) 87\n",
      ">>train tensor(-12.5663) 80\n",
      ">>train tensor(-12.9859) 50\n",
      "[0>   3::     0] training =  0, steps = 169, max_step = 200, reward=-949.184323 ::[[-0.0471304]]: >>train tensor(-13.6659) 127:400 ]<----\n",
      "[0>   3::     0] training =  0, steps = 174, max_step = 200, reward=-982.636368 ::[[-0.0809383]]: >>train tensor(-11.0402) 90::400 ]<----\n",
      "[0>   3::     0] training =  0, steps = 187, max_step = 200, reward=-1060.672270 ::[[-0.04468742]]: >>train tensor(-12.4680) 139400 ]<----\n",
      "[0>   3::     0] training =  0, steps = 200, max_step = 200, reward=-1157.976887 ::[[-0.06065993]]: >>train tensor(-12.3750) 13300 ]<----\n",
      ">>train tensor(-12.3544) 53\n",
      ">>train tensor(-12.4594) 57\n",
      "[0>   3::     0] training =  0, steps = 200, max_step = 200, reward=-1157.976887 ::[[-0.06841549]]: [ TARGET:-10.677420 replay::400 ]<---->>train tensor(-12.1467) 120\n",
      "TEST :  -1288.2358253430034\n",
      ">>train tensor(-9.3584) 145\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>train tensor(-9.4303) 116\n",
      ">>train tensor(-8.8663) 83\n",
      ">>train tensor(-10.8985) 123\n",
      ">>train tensor(-9.2911) 181\n",
      ">>train tensor(-7.8279) 74\n",
      ">>train tensor(-10.5022) 111\n",
      ">>train tensor(-9.7862) 160\n",
      ">>train tensor(-8.3146) 54\n",
      "[0>   4::     0] training =  0, steps =  12, max_step = 200, reward=-30.646167 ::[[-0.06167549]]: >>train tensor(-12.2773) 63600 ]<----\n",
      "[0>   4::     0] training =  0, steps =  31, max_step = 200, reward=-125.606928 ::[[-0.10604215]]: >>train tensor(-11.3493) 57\n",
      "[0>   4::     0] training =  0, steps =  45, max_step = 200, reward=-232.736858 ::[[-0.04812173]]: >>train tensor(-12.4035) 55\n",
      "[0>   4::     0] training =  0, steps =  50, max_step = 200, reward=-242.549156 ::[[-0.02034905]]: >>train tensor(-11.9962) 73\n",
      "[0>   4::     0] training =  0, steps =  51, max_step = 200, reward=-243.196361 ::[[-0.03371138]]: >>train tensor(-15.2209) 75\n",
      "[0>   4::     0] training =  0, steps =  54, max_step = 200, reward=-244.649478 ::[[-0.0492515]]: >>train tensor(-14.9127) 60\n",
      "[0>   4::     0] training =  0, steps =  63, max_step = 200, reward=-271.112402 ::[[-0.10504997]]: >>train tensor(-10.7334) 46600 ]<----\n",
      "[0>   4::     0] training =  0, steps =  69, max_step = 200, reward=-340.909524 ::[[-0.06163816]]: >>train tensor(-12.6218) 55\n",
      "[0>   4::     0] training =  0, steps =  71, max_step = 200, reward=-353.089448 ::[[-0.04714794]]: >>train tensor(-12.7514) 65\n",
      "[0>   4::     0] training =  0, steps =  82, max_step = 200, reward=-368.901837 ::[[-0.01276176]]: >>train tensor(-11.3242) 69:600 ]<----\n",
      "[0>   4::     0] training =  0, steps =  86, max_step = 200, reward=-380.361724 ::[[-0.03993997]]: >>train tensor(-11.0885) 53\n",
      "[0>   4::     0] training =  0, steps =  94, max_step = 200, reward=-463.944625 ::[[-0.07090848]]: >>train tensor(-11.1488) 46:600 ]<----\n",
      "[0>   4::     0] training =  0, steps = 107, max_step = 200, reward=-489.750156 ::[[-0.0644086]]: >>train tensor(-11.7242) 26::600 ]<----\n",
      "[0>   4::     0] training =  0, steps = 122, max_step = 200, reward=-597.897270 ::[[-0.04056673]]: [ TARGET:-17.009917 replay::600 ]<---->>train tensor(-9.6848) 70\n",
      ">>train tensor(-10.4728) 62\n",
      ">>train tensor(-10.5613) 27\n",
      "[0>   4::     0] training =  0, steps = 123, max_step = 200, reward=-601.287047 ::[[-0.05212166]]: [ TARGET:-24.921350 replay::600 ]<---->>train tensor(-8.2737) 40\n",
      "[0>   4::     0] training =  0, steps = 132, max_step = 200, reward=-611.277379 ::[[-0.0632929]]: >>train tensor(-10.0186) 48::600 ]<----\n",
      "[0>   4::     0] training =  0, steps = 150, max_step = 200, reward=-726.983886 ::[[-0.05455386]]: >>train tensor(-8.5536) 132:600 ]<----\n",
      "[0>   4::     0] training =  0, steps = 162, max_step = 200, reward=-742.501075 ::[[-0.0465453]]:  >>train tensor(-9.5964) 34\n",
      ">>train tensor(-8.2101) 90\n",
      ">>train tensor(-9.5001) 45\n",
      ">>train tensor(-10.0287) 116\n",
      "[0>   4::     0] training =  0, steps = 163, max_step = 200, reward=-747.714833 ::[[-0.06941599]]: [ TARGET:-11.786432 replay::600 ]<---->>train tensor(-8.8594) 135\n",
      ">>train tensor(-7.3183) 80\n",
      "[0>   4::     0] training =  0, steps = 186, max_step = 200, reward=-860.860311 ::[[-0.05633321]]: >>train tensor(-9.6799)lay::600 ]<---- 170\n",
      "[0>   4::     0] training =  0, steps = 199, max_step = 200, reward=-966.875071 ::[[-0.04371555]]: [ TARGET:-50.665289 replay::600 ]<---->>train tensor(-9.2841) 120\n",
      "[0>   4::     0] training =  0, steps = 200, max_step = 200, reward=-968.848687 ::[[-0.10094]]: >>train tensor(-10.7019) 76\n",
      ">>train tensor(-8.4825) 147\n",
      ">>train tensor(-9.3198) 185\n",
      ">>train tensor(-9.5990) 165\n",
      "[0>   4::     0] training =  0, steps = 200, max_step = 200, reward=-968.848687 ::[[-0.10833034]]: [ TARGET:-19.757341 replay::600 ]<---->>train tensor(-10.3372) 70\n",
      "TEST :  -968.2000967053725\n",
      ">>train tensor(-5.2086) 120\n",
      ">>train tensor(-5.1372) 84\n",
      ">>train tensor(-6.3241) 138\n",
      ">>train tensor(-6.2518) 230\n",
      "[0>   5::     0] training =  0, steps =   1, max_step = 200, reward=-0.565809 ::[[-0.0449071]]: [ TARGET:-37.525204 replay::800 ]<---->>train tensor(-7.1915) 162\n",
      ">>train tensor(-5.5531) 82\n",
      ">>train tensor(-5.4169) 145\n",
      ">>train tensor(-6.7757) 204\n",
      ">>train tensor(-5.6584) 160\n",
      ">>train tensor(-9.1314) 109\n",
      ">>train tensor(-10.3066) 101\n",
      ">>train tensor(-9.7329) 109\n",
      "[0>   5::     0] training =  0, steps =   2, max_step = 200, reward=-1.289954 ::[[-0.07522831]]: [ TARGET:-43.395717 replay::800 ]<---->>train tensor(-11.1067) 88\n",
      "[0>   5::     0] training =  0, steps =   7, max_step = 200, reward=-12.564702 ::[[-0.06625014]]: >>train tensor(-9.5767) 87\n",
      "[0>   5::     0] training =  0, steps =  12, max_step = 200, reward=-61.674185 ::[[-0.02752367]]: >>train tensor(-10.4234) 90\n",
      "[0>   5::     0] training =  0, steps =  23, max_step = 200, reward=-120.431831 ::[[-0.05659745]]: >>train tensor(-10.1017) 10400 ]<----\n",
      "[0>   5::     0] training =  0, steps =  45, max_step = 200, reward=-235.339830 ::[[-0.04640801]]: >>train tensor(-8.3170) 20\n",
      "[0>   5::     0] training =  0, steps =  51, max_step = 200, reward=-243.552197 ::[[-0.06665058]]: >>train tensor(-10.5575) 110\n",
      "[0>   5::     0] training =  0, steps =  55, max_step = 200, reward=-246.057267 ::[[-0.06752089]]: >>train tensor(-12.7131) 131\n",
      "[0>   5::     0] training =  0, steps =  68, max_step = 200, reward=-343.960649 ::[[-0.04357301]]: >>train tensor(-11.3002) 70:800 ]<----\n",
      "[0>   5::     0] training =  0, steps =  90, max_step = 200, reward=-407.777260 ::[[-0.0403362]]: >>train tensor(-10.4144) 79::800 ]<----\n",
      "[0>   5::     0] training =  0, steps = 106, max_step = 200, reward=-488.829790 ::[[-0.05379868]]: [ TARGET:-19.982354 replay::800 ]<---->>train tensor(-10.9426) 47\n",
      "[0>   5::     0] training =  0, steps = 122, max_step = 200, reward=-596.922783 ::[[-0.0431805]]: [ TARGET:-17.084568 replay::800 ]<---->>train tensor(-9.2344) 107\n",
      ">>train tensor(-5.8870) 35\n",
      ">>train tensor(-8.7339) 108\n",
      "[0>   5::     0] training =  0, steps = 123, max_step = 200, reward=-600.822907 ::[[-0.04220551]]: [ TARGET:-29.629414 replay::800 ]<---->>train tensor(-9.5292) 66\n",
      ">>train tensor(-8.3378) 40\n",
      "[0>   5::     0] training =  0, steps = 126, max_step = 200, reward=-607.040053 ::[[-0.04673509]]: >>train tensor(-6.4103) 122:800 ]<----\n",
      "[0>   5::     0] training =  0, steps = 148, max_step = 200, reward=-715.111709 ::[[-0.06737514]]: >>train tensor(-8.1928) 85::800 ]<----\n",
      "[0>   5::     0] training =  0, steps = 162, max_step = 200, reward=-738.958926 ::[[-0.06181231]]: [ TARGET:-17.868280 replay::800 ]<---->>train tensor(-7.1059) 133\n",
      ">>train tensor(-7.3262) 117\n",
      ">>train tensor(-8.7915) 73\n",
      ">>train tensor(-8.7836) 162\n",
      "[0>   5::     0] training =  0, steps = 163, max_step = 200, reward=-742.128947 ::[[-0.05429536]]: [ TARGET:-18.125498 replay::800 ]<---->>train tensor(-7.3074) 65\n",
      "[0>   5::     0] training =  0, steps = 168, max_step = 200, reward=-783.434607 ::[[-0.01667847]]: >>train tensor(-6.4231) 80\n",
      "[0>   5::     0] training =  0, steps = 186, max_step = 200, reward=-857.095208 ::[[-0.05211918]]: >>train tensor(-8.1336) 205:800 ]<----\n",
      "[0>   5::     0] training =  0, steps = 200, max_step = 200, reward=-963.898306 ::[[-0.03870645]]: >>train tensor(-8.1771) 155\n",
      ">>train tensor(-8.7200) 201\n",
      ">>train tensor(-8.1385) 100\n",
      ">>train tensor(-6.9270) 185\n",
      ">>train tensor(-9.0576) 64\n",
      "[0>   5::     0] training =  0, steps = 200, max_step = 200, reward=-963.898306 ::[[-0.03586415]]: [ TARGET:-28.031686 replay::800 ]<---->>train tensor(-7.8662) 123\n",
      "TEST :  -1172.4461514047798\n",
      ">>train tensor(-3.6816) 120\n",
      ">>train tensor(-5.9725) 122\n",
      ">>train tensor(-4.1199) 59\n",
      ">>train tensor(-4.6549) 193\n",
      ">>train tensor(-5.3459) 235\n",
      ">>train tensor(-5.7824) 129\n",
      ">>train tensor(-5.1377) 116\n",
      ">>train tensor(-5.2291) 208\n",
      "[0>   6::     0] training =  0, steps =   7, max_step = 200, reward=-10.972391 ::[[-0.06212377]]:  tensor(-4.7869) 160eplay::1000 ]<---->>train\n",
      "[0>   6::     0] training =  0, steps =  12, max_step = 200, reward=-53.738501 ::[[-0.00772817]]: >>train tensor(-5.8703) 81\n",
      "[0>   6::     0] training =  0, steps =  19, max_step = 200, reward=-113.333339 ::[[0.01910202]]: >>train tensor(-9.6563) 79\n",
      "[0>   6::     0] training =  0, steps =  24, max_step = 200, reward=-120.530550 ::[[0.01333482]]: [ TARGET:-31.787926 replay::1000 ]<---->>train tensor(-9.2722) 103\n",
      "[0>   6::     0] training =  0, steps =  29, max_step = 200, reward=-124.949114 ::[[-0.01125075]]: >>train tensor(-7.6425) 117\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0>   6::     0] training =  0, steps =  33, max_step = 200, reward=-138.468087 ::[[0.01057432]]: >>train tensor(-8.0901) 168\n",
      "[0>   6::     0] training =  0, steps =  55, max_step = 200, reward=-248.583268 ::[[-0.00773125]]: >>train tensor(-8.9464) 133:1000 ]<----\n",
      "[0>   6::     0] training =  0, steps =  76, max_step = 200, reward=-364.232739 ::[[-0.07279162]]: >>train tensor(-11.1425) 1151000 ]<----\n",
      "[0>   6::     0] training =  0, steps =  82, max_step = 200, reward=-375.249973 ::[[-0.0683756]]:  >>train tensor(-7.1414) 122\n",
      ">>train tensor(-8.8371) 127\n",
      "[0>   6::     0] training =  0, steps =  83, max_step = 200, reward=-380.213486 ::[[-0.06057989]]: [ TARGET:-24.020991 replay::1000 ]<---->>train tensor(-6.0132) 60\n",
      "[0>   6::     0] training =  0, steps =  99, max_step = 200, reward=-484.089881 ::[[-0.13246004]]: >>train tensor(-7.5053) 80::1000 ]<----\n",
      "[0>   6::     0] training =  0, steps = 122, max_step = 200, reward=-603.878156 ::[[-0.12660127]]: >>train tensor(-7.5870) 115\n",
      ">>train tensor(-8.6946) 128\n",
      ">>train tensor(-8.8723) 128\n",
      ">>train tensor(-8.0409) 124\n",
      ">>train tensor(-8.4178) 42\n",
      ">>train tensor(-6.8367) 55\n",
      "[0>   6::     0] training =  0, steps = 124, max_step = 200, reward=-605.115777 ::[[-0.01695928]]: >>train tensor(-7.6625) 40::1000 ]<----\n",
      "[0>   6::     0] training =  0, steps = 146, max_step = 200, reward=-723.882316 ::[[-0.03463438]]: >>train tensor(-6.7885) 941000 ]<----\n",
      "[0>   6::     0] training =  0, steps = 162, max_step = 200, reward=-792.111041 ::[[-0.03595012]]: >>train tensor(-7.4715) 71\n",
      ">>train tensor(-6.1407) 90\n",
      ">>train tensor(-5.1663) 129\n",
      ">>train tensor(-8.1118) 112\n",
      ">>train tensor(-4.0658) 79\n",
      "[0>   6::     0] training =  0, steps = 163, max_step = 200, reward=-804.972005 ::[[-0.03637258]]: [ TARGET:-28.247465 replay::1000 ]<---->>train tensor(-4.7361) 72\n",
      ">>train tensor(-4.5376) 80\n",
      "[0>   6::     0] training =  0, steps = 182, max_step = 200, reward=-862.005681 ::[[-0.01605355]]: [ TARGET:-29.228349 replay::1000 ]<---->>train tensor(-5.8807) 54\n",
      "[0>   6::     0] training =  0, steps = 193, max_step = 200, reward=-960.778020 ::[[-0.00522892]]: >>train tensor(-6.9792) 132:1000 ]<----\n",
      "[0>   6::     0] training =  0, steps = 200, max_step = 200, reward=-969.992109 ::[[-0.0149847]]:  ]: >>train tensor(-5.9669) 136\n",
      ">>train tensor(-7.7053) 127\n",
      ">>train tensor(-6.6803) 147\n",
      ">>train tensor(-5.1200) 133\n",
      "[0>   6::     0] training =  0, steps = 200, max_step = 200, reward=-969.992109 ::[[-0.01767064]]: [ TARGET:-28.798339 replay::1000 ]<---->>train tensor(-5.8085) 119\n",
      "TEST :  -1786.0510938876241\n",
      ">>train tensor(-1.5330) 120\n",
      ">>train tensor(-3.0317) 255\n",
      ">>train tensor(-4.1265) 117\n",
      ">>train tensor(-1.7934) 125\n",
      ">>train tensor(-5.1045) 86\n",
      ">>train tensor(-4.0917) 220\n",
      ">>train tensor(-2.9998) 139\n",
      ">>train tensor(-4.1932) 233\n",
      "[0>   7::     0] training =  0, steps =  13, max_step = 200, reward=-59.658462 ::[[-0.01328588]]: >>train tensor(-3.7232) 160:1200 ]<----\n",
      "[0>   7::     0] training =  0, steps =  17, max_step = 200, reward=-101.195953 ::[[-0.02633846]]: >>train tensor(-3.7439) 137\n",
      "[0>   7::     0] training =  0, steps =  21, max_step = 200, reward=-116.544996 ::[[-0.06249746]]: >>train tensor(-9.2326) 114\n",
      "[0>   7::     0] training =  0, steps =  23, max_step = 200, reward=-119.462761 ::[[-0.10178301]]: >>train tensor(-6.1658) 95\n",
      "[0>   7::     0] training =  0, steps =  26, max_step = 200, reward=-122.202035 ::[[-0.11338315]]: >>train tensor(-6.8454) 105:1200 ]<----\n",
      "[0>   7::     0] training =  0, steps =  31, max_step = 200, reward=-131.739839 ::[[-0.04286403]]: >>train tensor(-5.9804) 136\n",
      "[0>   7::     0] training =  0, steps =  56, max_step = 200, reward=-263.148346 ::[[-0.04082407]]: >>train tensor(-7.8096) 96::1200 ]<----\n",
      "[0>   7::     0] training =  0, steps =  82, max_step = 200, reward=-408.614333 ::[[-0.03102033]]: >>train tensor(-6.3458) 107\n",
      ">>train tensor(-7.9395) 125\n",
      ">>train tensor(-6.5036) 148\n",
      "[0>   7::     0] training =  0, steps =  86, max_step = 200, reward=-455.463024 ::[[-0.02978573]]: >>train tensor(-6.0337) 86:1200 ]<----\n",
      "[0>   7::     0] training =  0, steps = 111, max_step = 200, reward=-588.187764 ::[[-0.02190934]]: >>train tensor(-7.8502) 99::1200 ]<----\n",
      "[0>   7::     0] training =  0, steps = 122, max_step = 200, reward=-606.781064 ::[[-0.01247301]]: >>train tensor(-6.6347) 73\n",
      ">>train tensor(-6.0239) 104\n",
      ">>train tensor(-7.9089) 71\n",
      ">>train tensor(-6.1179) 69\n",
      ">>train tensor(-6.8076) 79\n",
      "[0>   7::     0] training =  0, steps = 123, max_step = 200, reward=-609.614075 ::[[-0.01248542]]: [ TARGET:-43.046698 replay::1200 ]<---->>train tensor(-4.9057) 142\n",
      ">>train tensor(-6.3107) 40\n",
      "[0>   7::     0] training =  0, steps = 140, max_step = 200, reward=-719.746060 ::[[-0.04384817]]: [ TARGET:-44.086019 replay::1200 ]<---->>train tensor(-2.5834) 56\n",
      "[0>   7::     0] training =  0, steps = 155, max_step = 200, reward=-813.761592 ::[[-0.04212047]]:  tensor(-5.1358) 77replay::1200 ]<---->>train\n",
      "[0>   7::     0] training =  0, steps = 162, max_step = 200, reward=-838.363764 ::[[-0.05423285]]: >>train tensor(-6.4946) 127\n",
      ">>train tensor(-4.7423) 135\n",
      ">>train tensor(-4.7274) 103\n",
      ">>train tensor(-5.1464) 178\n",
      ">>train tensor(-3.6932) 80\n",
      "[0>   7::     0] training =  0, steps = 166, max_step = 200, reward=-842.001958 ::[[-0.04109968]]: >>train tensor(-3.5682) 80::1200 ]<----\n",
      "[0>   7::     0] training =  0, steps = 189, max_step = 200, reward=-961.795698 ::[[-0.04549548]]: >>train tensor(-5.0466) 241:1200 ]<----\n",
      "[0>   7::     0] training =  0, steps = 200, max_step = 200, reward=-1037.398573 ::[[-0.0359462]]:  >>train tensor(-4.7547) 208\n",
      ">>train tensor(-5.8509) 143\n",
      ">>train tensor(-3.1350) 124\n",
      ">>train tensor(-3.5891) 238\n",
      ">>train tensor(-3.8335) 145\n",
      "[0>   7::     0] training =  0, steps = 200, max_step = 200, reward=-1037.398573 ::[[-0.03791524]]: [ TARGET:-53.342046 replay::1200 ]<---->>train tensor(-4.4346) 177\n",
      "TEST :  -1174.7210452873078\n",
      ">>train tensor(0.4513) 120\n",
      ">>train tensor(-1.0304) 128\n",
      ">>train tensor(-1.5993) 90\n",
      ">>train tensor(-2.3823) 228\n",
      ">>train tensor(-3.2010) 269\n",
      ">>train tensor(-3.5329) 74\n",
      ">>train tensor(-3.3065) 269\n",
      "[0>   8::     0] training =  0, steps =   1, max_step = 200, reward=-0.976321 ::[[-0.0182089]]: [ TARGET:-69.763708 replay::1400 ]<---->>train tensor(-4.4624) 180\n",
      "[0>   8::     0] training =  0, steps =  10, max_step = 200, reward=-46.713148 ::[[-0.05715358]]: [ TARGET:-70.837516 replay::1400 ]<---->>train tensor(-3.7343) 160\n",
      "[0>   8::     0] training =  0, steps =  13, max_step = 200, reward=-84.843321 ::[[-0.04326454]]: >>train tensor(-3.8075) 107\n",
      "[0>   8::     0] training =  0, steps =  17, max_step = 200, reward=-110.071795 ::[[-0.01383927]]: >>train tensor(-6.6988) 94\n",
      "[0>   8::     0] training =  0, steps =  41, max_step = 200, reward=-231.366631 ::[[-0.04974976]]: >>train tensor(-5.9091) 184:1400 ]<----\n",
      "[0>   8::     0] training =  0, steps =  56, max_step = 200, reward=-280.408337 ::[[-0.05727325]]: >>train tensor(-7.0698) 1101400 ]<----\n",
      "[0>   8::     0] training =  0, steps =  76, max_step = 200, reward=-371.553563 ::[[-0.06352144]]: >>train tensor(-5.4982) 152:1400 ]<----\n",
      "[0>   8::     0] training =  0, steps =  82, max_step = 200, reward=-430.678090 ::[[-0.02443835]]: >>train tensor(-5.3354) 112\n",
      ">>train tensor(-5.1811) 175\n",
      ">>train tensor(-4.7297) 204\n",
      ">>train tensor(-4.4640) 136\n",
      "[0>   8::     0] training =  0, steps =  83, max_step = 200, reward=-442.125453 ::[[-0.0141512]]: [ TARGET:-73.880899 replay::1400 ]<---->>train tensor(-4.1980) 166\n",
      "[0>   8::     0] training =  0, steps = 112, max_step = 200, reward=-593.705105 ::[[-0.01126612]]: >>train tensor(-4.5107) 84::1400 ]<----\n",
      "[0>   8::     0] training =  0, steps = 122, max_step = 200, reward=-608.303133 ::[[-0.05310273]]: >>train tensor(-5.8288) 118\n",
      ">>train tensor(-3.5279) 52\n",
      ">>train tensor(-2.9274) 63\n",
      ">>train tensor(-1.3663) 64\n",
      ">>train tensor(-4.0178) 69\n",
      "[0>   8::     0] training =  0, steps = 123, max_step = 200, reward=-612.453426 ::[[-0.05053031]]: [ TARGET:-51.062764 replay::1400 ]<---->>train tensor(-3.2459) 116\n",
      "[0>   8::     0] training =  0, steps = 127, max_step = 200, reward=-646.799753 ::[[-0.06642661]]: >>train tensor(-3.1309) 40\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0>   8::     0] training =  0, steps = 161, max_step = 200, reward=-837.229040 ::[[-0.05003108]]: >>train tensor(-1.2012) 209:1400 ]<----\n",
      "[0>   8::     0] training =  0, steps = 162, max_step = 200, reward=-838.155081 ::[[-0.0560782]]: >>train tensor(-2.1114) 72\n",
      ">>train tensor(-3.1915) 83\n",
      ">>train tensor(-1.2395) 240\n",
      ">>train tensor(-2.1853) 111\n",
      ">>train tensor(1.00000e-02 *\n",
      "       -9.4719) 126\n",
      "[0>   8::     0] training =  0, steps = 163, max_step = 200, reward=-839.014744 ::[[-0.05346183]]: [ TARGET:-63.417068 replay::1400 ]<---->>train tensor(-1.9982) 132\n",
      ">>train tensor(-2.2458) 80\n",
      "[0>   8::     0] training =  0, steps = 188, max_step = 200, reward=-960.936574 ::[[-0.08347767]]: >>train tensor(-2.9638) 244:1400 ]<----\n",
      "[0>   8::     0] training =  0, steps = 200, max_step = 200, reward=-1053.921558 ::[[-0.05482761]]: >>train tensor(-2.7194) 132\n",
      ">>train tensor(-2.8730) 233\n",
      ">>train tensor(-3.0980) 234\n",
      ">>train tensor(-1.5175) 160\n",
      ">>train tensor(-2.1428) 223\n",
      ">>train tensor(-2.1274) 114\n",
      "[0>   8::     0] training =  0, steps = 200, max_step = 200, reward=-1053.921558 ::[[-0.05822971]]: [ TARGET:-53.044690 replay::1400 ]<----TEST :  -1167.504938475027\n",
      ">>train tensor(2.9346) 120\n",
      ">>train tensor(0.4063) 164\n",
      ">>train tensor(0.7717) 226\n",
      ">>train tensor(1.0311) 153\n",
      ">>train tensor(-1.3537) 294\n",
      ">>train tensor(-1.9461) 228\n",
      ">>train tensor(-2.0731) 263\n",
      "[0>   9::     0] training =  0, steps =   1, max_step = 200, reward=-7.214130 ::[[-0.09983427]]: [ TARGET:-88.749519 replay::1600 ]<---->>train tensor(-4.2108) 90\n",
      "[0>   9::     0] training =  0, steps =  12, max_step = 200, reward=-102.955222 ::[[-0.07177637]]: >>train tensor(-3.5815) 160\n",
      "[0>   9::     0] training =  0, steps =  18, max_step = 200, reward=-146.155739 ::[[-0.06902097]]: >>train tensor(-3.8329) 226\n",
      "[0>   9::     0] training =  0, steps =  30, max_step = 200, reward=-249.482740 ::[[-0.06854344]]: [ TARGET:-96.788510 replay::1600 ]<---->>train tensor(-4.0910) 101\n",
      "[0>   9::     0] training =  0, steps =  57, max_step = 200, reward=-468.570251 ::[[-0.06689475]]: >>train tensor(-2.2641) 191:1600 ]<----\n",
      "[0>   9::     0] training =  0, steps =  82, max_step = 200, reward=-674.428700 ::[[-0.06112063]]: >>train tensor(-3.5449) 134\n",
      ">>train tensor(-2.3732) 114\n",
      ">>train tensor(-2.5322) 164\n",
      ">>train tensor(-1.6911) 162\n",
      ">>train tensor(-1.6044) 154\n",
      ">>train tensor(-1.8431) 185\n",
      "[0>   9::     0] training =  0, steps =  87, max_step = 200, reward=-712.150777 ::[[-0.06905132]]: >>train tensor(-2.0398) 145:1600 ]<----\n",
      "[0>   9::     0] training =  0, steps = 110, max_step = 200, reward=-909.588645 ::[[-0.10268597]]: >>train tensor(-3.7801) 49::1600 ]<----\n",
      "[0>   9::     0] training =  0, steps = 122, max_step = 200, reward=-1002.640345 ::[[-0.09129196]]: >>train tensor(-3.3148) 186\n",
      ">>train tensor(-2.9881) 101\n",
      ">>train tensor(-2.7835) 223\n",
      ">>train tensor(-2.9620) 150\n",
      ">>train tensor(-2.2267) 65\n",
      "[0>   9::     0] training =  0, steps = 123, max_step = 200, reward=-1012.664491 ::[[-0.0921231]]: [ TARGET:-72.625963 replay::1600 ]<---->>train tensor(-2.8808) 126\n",
      "[0>   9::     0] training =  0, steps = 125, max_step = 200, reward=-1031.724963 ::[[-0.07026216]]: >>train tensor(-8.9597) 40\n",
      "[0>   9::     0] training =  0, steps = 156, max_step = 200, reward=-1284.961494 ::[[-0.06596276]]: >>train tensor(-3.6038) 118:1600 ]<----\n",
      "[0>   9::     0] training =  0, steps = 162, max_step = 200, reward=-1336.076054 ::[[-0.07493838]]: >>train tensor(-3.1463) 218\n",
      ">>train tensor(-2.2638) 135\n",
      ">>train tensor(-4.2159) 92\n",
      ">>train tensor(-2.6874) 90\n",
      ">>train tensor(-2.7917) 155\n",
      "[0>   9::     0] training =  0, steps = 163, max_step = 200, reward=-1343.380320 ::[[-0.07687112]]: [ TARGET:-73.662182 replay::1600 ]<---->>train tensor(-1.4751) 174\n",
      "[0>   9::     0] training =  0, steps = 165, max_step = 200, reward=-1357.822786 ::[[-0.06995411]]: >>train tensor(-9.1002) 80\n",
      "[0>   9::     0] training =  0, steps = 195, max_step = 200, reward=-1608.586528 ::[[-0.06344375]]: >>train tensor(-2.4343) 165:1600 ]<----\n",
      "[0>   9::     0] training =  0, steps = 200, max_step = 200, reward=-1645.453491 ::[[-0.06517528]]: >>train tensor(-2.6644) 198\n",
      ">>train tensor(-1.3144) 189\n",
      ">>train tensor(-4.3095) 173\n",
      ">>train tensor(-1.6088) 119\n",
      ">>train tensor(-4.2885) 113\n",
      "[0>   9::     0] training =  0, steps = 200, max_step = 200, reward=-1645.453491 ::[[-0.06382439]]: [ TARGET:-76.886187 replay::1600 ]<---->>train tensor(-3.0479) 270\n",
      "TEST :  -968.9415017371684\n",
      ">>train tensor(-4.0449) 120\n",
      ">>train tensor(-1.8709) 97\n",
      ">>train tensor(-2.4603) 254\n",
      ">>train tensor(-1.8104) 245\n",
      ">>train tensor(-4.4502) 263\n",
      ">>train tensor(-4.3255) 210\n",
      ">>train tensor(-4.2017) 324\n",
      "[0>  10::     0] training =  0, steps =  31, max_step = 200, reward=-263.811106 ::[[-0.07683224]]: >>train tensor(-5.2336) 199:1600 ]<----\n",
      "[0>  10::     0] training =  0, steps =  37, max_step = 200, reward=-315.279531 ::[[-0.08335592]]: [ TARGET:-79.765988 replay::1600 ]<---->>train tensor(-7.8212) 160\n",
      "[0>  10::     0] training =  0, steps =  82, max_step = 200, reward=-694.231071 ::[[-0.08133182]]: >>train tensor(-0.2564) 169:1800 ]<----\n",
      ">>train tensor(-1.0693) 175\n",
      ">>train tensor(-0.9377) 232\n",
      ">>train tensor(-1.1598) 193\n",
      ">>train tensor(-1.2622) 147\n",
      ">>train tensor(-1.6293) 194\n",
      ">>train tensor(-3.7073) 188\n",
      ">>train tensor(-3.6836) 243\n",
      ">>train tensor(-5.2675) 151\n",
      "[0>  10::     0] training =  0, steps =  85, max_step = 200, reward=-719.470594 ::[[-0.04025389]]: >>train tensor(-4.7017) 205::1800 ]<----\n",
      "[0>  10::     0] training =  0, steps = 121, max_step = 200, reward=-1027.074634 ::[[-0.02446266]]: >>train tensor(-3.3433) 126:1800 ]<----\n",
      "[0>  10::     0] training =  0, steps = 122, max_step = 200, reward=-1036.478451 ::[[-0.01996194]]: >>train tensor(-6.2236) 184\n",
      ">>train tensor(-5.2789) 93\n",
      ">>train tensor(-6.7005) 73\n",
      ">>train tensor(-5.3334) 56\n",
      ">>train tensor(-3.3411) 141\n",
      "[0>  10::     0] training =  0, steps = 123, max_step = 200, reward=-1045.420381 ::[[-0.01727745]]: [ TARGET:-88.293269 replay::1800 ]<---->>train tensor(-3.4590) 171\n",
      ">>train tensor(-5.5260) 40\n",
      "[0>  10::     0] training =  0, steps = 162, max_step = 200, reward=-1370.687188 ::[[0.00102313]]: [ TARGET:-76.528998 replay::1800 ]<---->>train tensor(-3.4048) 205\n",
      ">>train tensor(-4.2423) 168\n",
      ">>train tensor(-3.3800) 68\n",
      ">>train tensor(-1.2205) 156\n",
      ">>train tensor(-1.5812) 165\n",
      ">>train tensor(-1.5886) 147\n",
      "[0>  10::     0] training =  0, steps = 163, max_step = 200, reward=-1378.242735 ::[[-0.04314971]]: [ TARGET:-91.303207 replay::1800 ]<---->>train tensor(1.0401) 184\n",
      "[0>  10::     0] training =  0, steps = 164, max_step = 200, reward=-1385.983080 ::[[0.02798996]]: >>train tensor(-3.5486) 80\n",
      "[0>  10::     0] training =  0, steps = 181, max_step = 200, reward=-1529.563321 ::[[0.0410076]]: [ TARGET:-118.720031 replay::1800 ]<---->>train tensor(-0.6469) 113\n",
      "[0>  10::     0] training =  0, steps = 200, max_step = 200, reward=-1691.064293 ::[[-0.0199524]]:  >>train tensor(0.4702) 229\n",
      ">>train tensor(-0.9238) 155\n",
      ">>train tensor(-1.1724) 217\n",
      ">>train tensor(0.3575) 237\n",
      ">>train tensor(-1.0828) 332\n",
      "[0>  10::     0] training =  0, steps = 200, max_step = 200, reward=-1691.064293 ::[[-0.02082292]]: [ TARGET:-110.451497 replay::1800 ]<---->>train tensor(1.00000e-02 *\n",
      "       1.9424) 243\n",
      "TEST :  -1310.602137246408\n",
      ">>train tensor(0.6421) 120\n",
      ">>train tensor(1.00000e-02 *\n",
      "       8.0424) 150\n",
      ">>train tensor(-0.5819) 235\n",
      ">>train tensor(-2.0966) 232\n",
      ">>train tensor(-6.2257) 300\n",
      ">>train tensor(-7.4109) 75\n",
      ">>train tensor(-6.4261) 298\n",
      ">>train tensor(-5.8830) 288\n",
      "[0>  11::     0] training =  0, steps =   8, max_step = 200, reward=-21.999506 ::[[-0.10726685]]: >>train tensor(-6.5768) 160800 ]<----\n",
      "[0>  11::     0] training =  0, steps =  42, max_step = 200, reward=-235.618024 ::[[-0.14582913]]: >>train tensor(-5.2217) 123:2000 ]<----\n",
      "[0>  11::     0] training =  0, steps =  80, max_step = 200, reward=-456.642632 ::[[-0.21040981]]: >>train tensor(-2.1485) 193::2000 ]<----\n",
      "[0>  11::     0] training =  0, steps =  82, max_step = 200, reward=-465.498564 ::[[-0.09664551]]: >>train tensor(3.0584) 137\n",
      ">>train tensor(6.5611) 212\n",
      ">>train tensor(5.2617) 235\n",
      ">>train tensor(5.5639) 204\n",
      ">>train tensor(6.3659) 170\n",
      ">>train tensor(0.6416) 170\n",
      ">>train tensor(-0.7121) 119\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0>  11::     0] training =  0, steps =  83, max_step = 200, reward=-468.343739 ::[[-0.09128656]]: [ TARGET:-71.922881 replay::2000 ]<---->>train tensor(-2.6213) 176\n",
      "[0>  11::     0] training =  0, steps = 122, max_step = 200, reward=-689.267920 ::[[-0.37220622]]: >>train tensor(-2.7704) 109\n",
      ">>train tensor(0.2566) 95\n",
      ">>train tensor(1.6749) 137\n",
      ">>train tensor(3.1861) 196\n",
      ">>train tensor(2.8237) 201\n",
      ">>train tensor(4.5157) 223\n",
      ">>train tensor(2.4948) 134\n",
      "[0>  11::     0] training =  0, steps = 128, max_step = 200, reward=-708.345438 ::[[-0.20357562]]: >>train tensor(-0.6819) 40::2000 ]<----\n",
      "[0>  11::     0] training =  0, steps = 161, max_step = 200, reward=-888.857587 ::[[-0.22103453]]: >>train tensor(0.2998) 168y::2000 ]<----\n",
      "[0>  11::     0] training =  0, steps = 162, max_step = 200, reward=-901.267140 ::[[-0.0822542]]: >>train tensor(-1.0014) 176\n",
      ">>train tensor(0.4808) 216\n",
      ">>train tensor(1.3191) 156\n",
      ">>train tensor(1.00000e-02 *\n",
      "       6.2484) 293\n",
      ">>train tensor(1.6625) 179\n",
      "[0>  11::     0] training =  0, steps = 163, max_step = 200, reward=-911.731808 ::[[-0.06166752]]: [ TARGET:-84.349778 replay::2000 ]<---->>train tensor(3.5621) 153\n",
      "[0>  11::     0] training =  0, steps = 165, max_step = 200, reward=-927.082073 ::[[-0.08501066]]: >>train tensor(1.8094) 80\n",
      "[0>  11::     0] training =  0, steps = 197, max_step = 200, reward=-1070.276789 ::[[-0.03050948]]: tensor(2.6377) 28142 replay::2000 ]<---->>train \n",
      "[0>  11::     0] training =  0, steps = 200, max_step = 200, reward=-1082.742989 ::[[-0.08177836]]: >>train tensor(1.4230) 209\n",
      ">>train tensor(0.7774) 164\n",
      ">>train tensor(-1.8961) 105\n",
      ">>train tensor(1.00000e-02 *\n",
      "       6.7842) 269\n",
      ">>train tensor(1.00000e-02 *\n",
      "       -5.9443) 286\n",
      "[0>  11::     0] training =  0, steps = 200, max_step = 200, reward=-1082.742989 ::[[-0.08374639]]: [ TARGET:-94.508776 replay::2000 ]<---->>train tensor(2.1359) 108\n",
      "TEST :  -1142.767431086501\n",
      ">>train tensor(0.6248) 120\n",
      ">>train tensor(1.6470) 157\n",
      ">>train tensor(-1.6257) 239\n",
      ">>train tensor(-0.7880) 161\n",
      ">>train tensor(-3.7129) 272\n",
      ">>train tensor(-2.9788) 112\n",
      "[0>  12::     0] training =  0, steps =   3, max_step = 200, reward=-3.970139 ::[[-0.15848528]]: tensor(-6.9530) 1606 replay::2000 ]<----\n",
      "[0>  12::     0] training =  0, steps =  16, max_step = 200, reward=-109.307525 ::[[-0.33688124]]: >>train tensor(-5.0791) 340\n",
      "[0>  12::     0] training =  0, steps =  17, max_step = 200, reward=-112.718060 ::[[-0.17871845]]: [ TARGET:-99.123394 replay::2000 ]<---->>train tensor(-1.1738) 160\n",
      "[0>  12::     0] training =  0, steps =  51, max_step = 200, reward=-241.544097 ::[[-0.1483371]]: >>train tensor(1.4346) 289ay::2200 ]<----\n",
      "[0>  12::     0] training =  0, steps =  82, max_step = 200, reward=-366.773283 ::[[0.03952519]]:  >>train tensor(4.8982) 237\n",
      ">>train tensor(5.0726) 235\n",
      ">>train tensor(4.9277) 204\n",
      ">>train tensor(2.3969) 263\n",
      ">>train tensor(1.2690) 224\n",
      ">>train tensor(-1.8360) 193\n",
      ">>train tensor(-2.3142) 181\n",
      ">>train tensor(-1.2728) 224\n",
      "[0>  12::     0] training =  0, steps =  85, max_step = 200, reward=-370.395182 ::[[-0.25079824]]: >>train tensor(0.7224) 167::2200 ]<----\n",
      "[0>  12::     0] training =  0, steps = 122, max_step = 200, reward=-599.280646 ::[[-0.20947892]]: >>train tensor(-0.2898) 146::2200 ]<----\n",
      ">>train tensor(2.0594) 159\n",
      ">>train tensor(3.4346) 209\n",
      ">>train tensor(3.8456) 67\n",
      ">>train tensor(2.7607) 69\n",
      ">>train tensor(1.9781) 207\n",
      ">>train tensor(-4.0214) 150\n",
      "[0>  12::     0] training =  0, steps = 123, max_step = 200, reward=-601.457555 ::[[-0.28343792]]: [ TARGET:-115.879585 replay::2200 ]<---->>train tensor(1.3385) 40\n",
      "[0>  12::     0] training =  0, steps = 162, max_step = 200, reward=-775.379167 ::[[-0.26364712]]: [ TARGET:-129.770160 replay::2200 ]<---->>train tensor(-4.5028) 205\n",
      ">>train tensor(2.6819) 134\n",
      ">>train tensor(-4.1620) 130\n",
      ">>train tensor(-2.4478) 219\n",
      ">>train tensor(2.1677) 128\n",
      ">>train tensor(2.9099) 209\n",
      "[0>  12::     0] training =  0, steps = 163, max_step = 200, reward=-789.999332 ::[[-0.25786807]]: [ TARGET:-117.105766 replay::2200 ]<---->>train tensor(6.1152) 280\n",
      ">>train tensor(8.1301) 80\n",
      "[0>  12::     0] training =  0, steps = 200, max_step = 200, reward=-967.346949 ::[[-0.29458189]]: >>train tensor(3.2187) 222\n",
      ">>train tensor(1.7958) 159\n",
      ">>train tensor(0.5125) 228\n",
      ">>train tensor(-1.8210) 151\n",
      ">>train tensor(-3.3838) 358\n",
      ">>train tensor(-4.2797) 168\n",
      ">>train tensor(-5.9010) 106\n",
      "[0>  12::     0] training =  0, steps = 200, max_step = 200, reward=-967.346949 ::[[-0.2944484]]: [ TARGET:-120.304126 replay::2200 ]<---->>train tensor(-0.3329) 120\n",
      "TEST :  -1315.7318690556456\n",
      ">>train tensor(2.7564) 185\n",
      ">>train tensor(2.1584) 378\n",
      ">>train tensor(1.8892) 337\n",
      ">>train tensor(0.3892) 128\n",
      "[0>  13::     0] training =  0, steps =   2, max_step = 200, reward=-1.769013 ::[[-0.21928005]]: >>train tensor(-0.3117) 311::2200 ]<----\n",
      "[0>  13::     0] training =  0, steps =  27, max_step = 200, reward=-124.652067 ::[[-0.20086356]]: >>train tensor(-1.7992) 201::2200 ]<----\n",
      "[0>  13::     0] training =  0, steps =  43, max_step = 200, reward=-234.418881 ::[[0.09575289]]: [ TARGET:-125.310443 replay::2200 ]<---->>train tensor(3.7016) 221\n",
      ">>train tensor(0.8218) 160\n",
      "[0>  13::     0] training =  0, steps =  74, max_step = 200, reward=-381.161248 ::[[-0.1787386]]: tensor(-2.8311) 25184 replay::2400 ]<----\n",
      "[0>  13::     0] training =  0, steps =  82, max_step = 200, reward=-459.472071 ::[[-0.24648441]]: >>train tensor(-2.7900) 247\n",
      ">>train tensor(-3.6543) 212\n",
      ">>train tensor(-0.6646) 160\n",
      ">>train tensor(-2.9453) 313\n",
      ">>train tensor(-1.0261) 302\n",
      ">>train tensor(-0.6196) 256\n",
      ">>train tensor(-1.1661) 222\n",
      ">>train tensor(-2.4136) 247\n",
      "[0>  13::     0] training =  0, steps =  83, max_step = 200, reward=-463.882777 ::[[-0.26181201]]: [ TARGET:-139.375809 replay::2400 ]<---->>train tensor(1.00000e-02 *\n",
      "       -8.6662) 256\n",
      "[0>  13::     0] training =  0, steps = 122, max_step = 200, reward=-673.368422 ::[[-0.15624873]]: >>train tensor(-3.6511) 110\n",
      ">>train tensor(-1.9164) 209\n",
      ">>train tensor(1.0384) 102\n",
      ">>train tensor(-1.3618) 295\n",
      ">>train tensor(-2.0682) 240\n",
      ">>train tensor(-1.2981) 186\n",
      "[0>  13::     0] training =  0, steps = 123, max_step = 200, reward=-682.650110 ::[[-0.15936926]]: [ TARGET:-125.973480 replay::2400 ]<---->>train tensor(-2.6870) 261\n",
      "[0>  13::     0] training =  0, steps = 126, max_step = 200, reward=-700.298249 ::[[-0.03063182]]: >>train tensor(-1.9904) 40\n",
      "[0>  13::     0] training =  0, steps = 162, max_step = 200, reward=-867.975499 ::[[-0.035895]]: : >>train tensor(-2.4847) 322\n",
      ">>train tensor(-3.7187) 175\n",
      ">>train tensor(-3.0354) 266\n",
      ">>train tensor(-2.4337) 249\n",
      ">>train tensor(-3.5996) 126\n",
      ">>train tensor(-3.0511) 221\n",
      "[0>  13::     0] training =  0, steps = 163, max_step = 200, reward=-880.053356 ::[[-0.03509404]]: [ TARGET:-132.525328 replay::2400 ]<---->>train tensor(-2.6086) 283\n",
      ">>train tensor(-2.4351) 80\n",
      "[0>  13::     0] training =  0, steps = 200, max_step = 200, reward=-1072.404270 ::[[-0.01260324]]: >>train tensor(-2.1963) 168\n",
      ">>train tensor(-0.9804) 252\n",
      ">>train tensor(-2.3284) 110\n",
      ">>train tensor(-3.0076) 207\n",
      ">>train tensor(0.4963) 215\n",
      ">>train tensor(-2.9131) 342\n",
      "[0>  13::     0] training =  0, steps = 200, max_step = 200, reward=-1072.404270 ::[[-0.00990925]]: [ TARGET:-136.938083 replay::2400 ]<---->>train tensor(-2.3385) 275\n",
      "TEST :  -905.5945218238732\n",
      ">>train tensor(1.8252) 120\n",
      ">>train tensor(-0.7782) 207\n",
      ">>train tensor(-1.4221) 235\n",
      ">>train tensor(-0.9405) 259\n",
      ">>train tensor(-4.0212) 91\n",
      ">>train tensor(-2.5858) 96\n",
      "[0>  14::     0] training =  0, steps =   4, max_step = 200, reward=-29.219346 ::[[-0.0782241]]: >>train tensor(-2.9409) 183::2400 ]<----\n",
      "[0>  14::     0] training =  0, steps =  26, max_step = 200, reward=-208.229985 ::[[0.00099356]]: >>train tensor(-2.5807) 209\n",
      ">>train tensor(-1.7460) 160\n",
      "[0>  14::     0] training =  0, steps =  73, max_step = 200, reward=-565.016508 ::[[-0.03928474]]: [ TARGET:-151.089085 replay::2600 ]<---->>train tensor(-4.7437) 261\n",
      "[0>  14::     0] training =  0, steps =  82, max_step = 200, reward=-634.367115 ::[[-0.08648513]]: >>train tensor(-4.2969) 249\n",
      ">>train tensor(-3.6936) 237\n",
      ">>train tensor(-4.4273) 281\n",
      ">>train tensor(-3.7401) 268\n",
      ">>train tensor(-3.0658) 282\n",
      ">>train tensor(-4.1317) 247\n",
      ">>train tensor(-2.7493) 323\n",
      ">>train tensor(-2.5455) 191\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0>  14::     0] training =  0, steps =  88, max_step = 200, reward=-675.867423 ::[[-0.04405738]]: >>train tensor(1.00000e-02 *:2600 ]<----\n",
      "       2.1413) 277\n",
      "[0>  14::     0] training =  0, steps = 122, max_step = 200, reward=-936.189510 ::[[-0.03412247]]: >>train tensor(-3.4257) 355\n",
      ">>train tensor(-1.7677) 257\n",
      ">>train tensor(-1.6153) 293\n",
      ">>train tensor(-2.1922) 258\n",
      ">>train tensor(-0.9728) 167\n",
      ">>train tensor(-1.7939) 100\n",
      "[0>  14::     0] training =  0, steps = 123, max_step = 200, reward=-945.711799 ::[[-0.0260147]]: [ TARGET:-144.309655 replay::2600 ]<---->>train tensor(-1.1716) 91\n",
      "[0>  14::     0] training =  0, steps = 126, max_step = 200, reward=-975.177207 ::[[-0.01799293]]: >>train tensor(-2.5106) 40\n",
      "[0>  14::     0] training =  0, steps = 160, max_step = 200, reward=-1234.450717 ::[[-0.02203341]]: >>train tensor(-0.3401) 177::2600 ]<----\n",
      "[0>  14::     0] training =  0, steps = 162, max_step = 200, reward=-1249.729030 ::[[-0.01761814]]: >>train tensor(-1.4826) 354\n",
      ">>train tensor(1.00000e-02 *\n",
      "       -8.6751) 202\n",
      ">>train tensor(1.00000e-02 *\n",
      "       4.0781) 174\n",
      ">>train tensor(-1.8935) 152\n",
      ">>train tensor(-1.5502) 373\n",
      "[0>  14::     0] training =  0, steps = 163, max_step = 200, reward=-1256.458220 ::[[-0.01783146]]: [ TARGET:-143.554612 replay::2600 ]<---->>train tensor(-0.8093) 89\n",
      "[0>  14::     0] training =  0, steps = 165, max_step = 200, reward=-1268.760885 ::[[-0.03811241]]:  tensor(-2.9022) 80\n",
      "[0>  14::     0] training =  0, steps = 200, max_step = 200, reward=-1531.175143 ::[[-0.04587773]]: >>train tensor(-1.2906) 242\n",
      ">>train tensor(-0.5893) 104\n",
      ">>train tensor(-1.7807) 189\n",
      ">>train tensor(-1.7236) 364\n",
      ">>train tensor(-2.1574) 127\n",
      ">>train tensor(-1.4462) 300\n",
      "[0>  14::     0] training =  0, steps = 200, max_step = 200, reward=-1531.175143 ::[[-0.05537716]]: [ TARGET:-167.322122 replay::2600 ]<----tensor(-1.7115) 151\n",
      "TEST :  -1498.2027456366704\n",
      ">>train tensor(2.7463) 120\n",
      ">>train tensor(0.3753) 269\n",
      ">>train tensor(0.5184) 126\n",
      ">>train tensor(0.8096) 396\n",
      "[0>  15::     0] training =  0, steps =   2, max_step = 200, reward=-6.135391 ::[[-0.17904353]]: >>train tensor(-2.1096) 309::2600 ]<----\n",
      "[0>  15::     0] training =  0, steps =  26, max_step = 200, reward=-146.241375 ::[[-0.14820151]]: >>train tensor(0.6682) 302y::2600 ]<----\n",
      "[0>  15::     0] training =  0, steps =  42, max_step = 200, reward=-248.565205 ::[[-0.19341175]]: >>train tensor(1.5854) 113\n",
      "[0>  15::     0] training =  0, steps =  43, max_step = 200, reward=-253.426187 ::[[-0.20332929]]: [ TARGET:-154.060062 replay::2600 ]<---->>train tensor(-0.5025) 304\n",
      ">>train tensor(-2.5204) 160\n",
      "[0>  15::     0] training =  0, steps =  82, max_step = 200, reward=-488.260260 ::[[-0.32716676]]: >>train tensor(-1.3046) 262\n",
      ">>train tensor(0.6904) 259\n",
      ">>train tensor(1.00000e-02 *\n",
      "       -1.2145) 226\n",
      ">>train tensor(0.3362) 325\n",
      ">>train tensor(1.0793) 243\n",
      ">>train tensor(-0.5242) 267\n",
      ">>train tensor(0.3662) 237\n",
      ">>train tensor(-0.2701) 362\n",
      ">>train tensor(-0.3186) 302\n",
      "[0>  15::     0] training =  0, steps =  86, max_step = 200, reward=-523.047981 ::[[-0.23576863]]: >>train tensor(-0.8492) 311::2800 ]<----\n",
      "[0>  15::     0] training =  0, steps = 122, max_step = 200, reward=-727.939776 ::[[-0.3067566]]:  >>train tensor(-1.1923) 233\n",
      ">>train tensor(-0.9809) 219\n",
      ">>train tensor(-1.0768) 258\n",
      ">>train tensor(-2.7434) 252\n",
      ">>train tensor(-0.5164) 221\n",
      ">>train tensor(1.00000e-02 *\n",
      "       8.3431) 280\n",
      "[0>  15::     0] training =  0, steps = 123, max_step = 200, reward=-735.649902 ::[[-0.30981909]]: [ TARGET:-143.645842 replay::2800 ]<---->>train tensor(0.2853) 260\n",
      ">>train tensor(-0.3299) 40\n",
      "[0>  15::     0] training =  0, steps = 161, max_step = 200, reward=-960.236030 ::[[-0.30290597]]: [ TARGET:-143.514760 replay::2800 ]<---->>train tensor(0.6730) 91\n",
      "[0>  15::     0] training =  0, steps = 162, max_step = 200, reward=-967.659938 ::[[-0.30776287]]: >>train tensor(-1.0984) 188\n",
      ">>train tensor(1.3986) 143\n",
      ">>train tensor(0.2363) 192\n",
      ">>train tensor(-0.9159) 356\n",
      ">>train tensor(-0.7020) 210\n",
      "[0>  15::     0] training =  0, steps = 163, max_step = 200, reward=-976.869190 ::[[-0.29038399]]: [ TARGET:-146.901672 replay::2800 ]<---->>train tensor(0.4067) 158\n",
      "[0>  15::     0] training =  0, steps = 164, max_step = 200, reward=-988.018396 ::[[-0.16059815]]: >>train tensor(-1.5552) 80\n",
      "[0>  15::     0] training =  0, steps = 200, max_step = 200, reward=-1193.883778 ::[[-0.20820498]]: >>train tensor(-0.3751) 216\n",
      ">>train tensor(0.3547) 110\n",
      ">>train tensor(-0.9229) 409\n",
      ">>train tensor(-0.4349) 105\n",
      ">>train tensor(-0.6452) 124\n",
      ">>train tensor(1.3933) 324\n",
      "[0>  15::     0] training =  0, steps = 200, max_step = 200, reward=-1193.883778 ::[[-0.22612697]]: [ TARGET:-149.647914 replay::2800 ]<---->>train tensor(1.4016) 193\n",
      "TEST :  -1291.2753228623305\n",
      ">>train tensor(1.8103) 120\n",
      ">>train tensor(0.2789) 362\n",
      ">>train tensor(1.00000e-02 *\n",
      "       -1.4187) 181\n",
      ">>train tensor(-1.0926) 161\n",
      "[0>  16::     0] training =  0, steps =   1, max_step = 200, reward=-4.399913 ::[[-0.14096113]]: [ TARGET:-169.736179 replay::2800 ]<---->>train tensor(-2.4254) 207\n",
      "[0>  16::     0] training =  0, steps =   4, max_step = 200, reward=-18.256144 ::[[-0.20697697]]: >>train tensor(-3.0031) 374\n",
      "[0>  16::     0] training =  0, steps =  41, max_step = 200, reward=-275.419683 ::[[-0.27777031]]: [ TARGET:-158.175545 replay::2800 ]<---->>train tensor(-3.5141) 159\n",
      "[0>  16::     0] training =  0, steps =  43, max_step = 200, reward=-291.903080 ::[[-0.35266535]]: [ TARGET:-154.074051 replay::2800 ]<---->>train tensor(0.6040) 134\n",
      "[0>  16::     0] training =  0, steps =  46, max_step = 200, reward=-323.153252 ::[[-0.36204571]]: >>train tensor(1.00000e-02 *\n",
      "       -4.9827) 160\n",
      "[0>  16::     0] training =  0, steps =  82, max_step = 200, reward=-578.802970 ::[[-0.32870495]]: >>train tensor(1.7695) 321\n",
      ">>train tensor(0.7510) 334\n",
      ">>train tensor(2.0627) 235\n",
      ">>train tensor(1.5911) 315\n",
      ">>train tensor(1.0541) 323\n",
      ">>train tensor(-0.4654) 317\n",
      ">>train tensor(-0.9972) 286\n",
      ">>train tensor(-3.0719) 344\n",
      ">>train tensor(-1.1187) 270\n",
      "[0>  16::     0] training =  0, steps =  85, max_step = 200, reward=-598.828805 ::[[-0.20609829]]: >>train tensor(-0.3044) 267:3000 ]<----\n",
      "[0>  16::     0] training =  0, steps = 122, max_step = 200, reward=-854.976857 ::[[-0.29208827]]: >>train tensor(-0.5702) 274\n",
      ">>train tensor(0.4721) 88\n",
      ">>train tensor(1.0153) 283\n",
      ">>train tensor(1.4461) 93\n",
      ">>train tensor(1.2269) 307\n",
      ">>train tensor(0.8852) 268\n",
      "[0>  16::     0] training =  0, steps = 123, max_step = 200, reward=-860.007882 ::[[-0.2902459]]: [ TARGET:-164.726063 replay::3000 ]<---->>train tensor(1.4043) 382\n",
      ">>train tensor(-0.1411) 40\n",
      "[0>  16::     0] training =  0, steps = 162, max_step = 200, reward=-1134.652198 ::[[-0.13631457]]: >>train tensor(0.5689) 131\n",
      ">>train tensor(1.00000e-02 *\n",
      "       1.8983) 149\n",
      ">>train tensor(-0.8915) 377\n",
      ">>train tensor(0.3249) 107\n",
      ">>train tensor(1.3531) 274\n",
      ">>train tensor(1.0804) 276\n",
      "[0>  16::     0] training =  0, steps = 163, max_step = 200, reward=-1143.627499 ::[[-0.13535531]]: [ TARGET:-176.258461 replay::3000 ]<---->>train tensor(2.2927) 282\n",
      "[0>  16::     0] training =  0, steps = 165, max_step = 200, reward=-1164.655801 ::[[-0.34278165]]: >>train tensor(-0.5419) 80\n",
      "[0>  16::     0] training =  0, steps = 200, max_step = 200, reward=-1413.477115 ::[[-0.34282692]]: >>train tensor(2.2932) 301\n",
      ">>train tensor(0.9831) 102\n",
      ">>train tensor(-0.1489) 187\n",
      ">>train tensor(-1.2515) 441\n",
      ">>train tensor(-1.4432) 369\n",
      ">>train tensor(-1.3015) 317\n",
      "[0>  16::     0] training =  0, steps = 200, max_step = 200, reward=-1413.477115 ::[[-0.34772363]]: [ TARGET:-175.329932 replay::3000 ]<---->>train tensor(1.9116) 187\n",
      "TEST :  -1004.3148046716074\n",
      ">>train tensor(2.3526) 120\n",
      ">>train tensor(2.0155) 254\n",
      ">>train tensor(1.9559) 442\n",
      ">>train tensor(0.6360) 199\n",
      ">>train tensor(-0.8748) 272\n",
      "[0>  17::     0] training =  0, steps =  42, max_step = 200, reward=-319.423665 ::[[-0.1236865]]:  TARGET:-176.077768 replay::3000 ]<---->>train tensor(-1.2300) 280\n",
      ">>train tensor(-1.1557) 207\n",
      "[0>  17::     0] training =  0, steps =  43, max_step = 200, reward=-329.927871 ::[[-0.12457529]]: [ TARGET:-180.231872 replay::3000 ]<---->>train tensor(-1.3967) 147\n",
      ">>train tensor(-3.8264) 160\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0>  17::     0] training =  0, steps =  82, max_step = 200, reward=-631.678875 ::[[-0.15300154]]: >>train tensor(-0.2667) 291\n",
      ">>train tensor(1.1526) 322\n",
      ">>train tensor(1.9268) 270\n",
      ">>train tensor(1.2423) 312\n",
      ">>train tensor(0.4356) 400\n",
      ">>train tensor(-0.1344) 365\n",
      ">>train tensor(0.5242) 381\n",
      ">>train tensor(-1.9967) 326\n",
      ">>train tensor(-2.0260) 240\n",
      "[0>  17::     0] training =  0, steps =  85, max_step = 200, reward=-650.450909 ::[[-0.09414863]]: >>train tensor(-0.1507) 310::3200 ]<----\n",
      "[0>  17::     0] training =  0, steps = 122, max_step = 200, reward=-931.637948 ::[[-0.09435746]]: >>train tensor(-2.0602) 237\n",
      ">>train tensor(-1.1954) 441\n",
      ">>train tensor(-0.8684) 160\n",
      ">>train tensor(-0.9652) 134\n",
      ">>train tensor(-0.3475) 338\n",
      ">>train tensor(-0.3809) 273\n",
      "[0>  17::     0] training =  0, steps = 123, max_step = 200, reward=-939.636605 ::[[-0.0939582]]: [ TARGET:-189.381906 replay::3200 ]<---->>train tensor(1.7010) 88\n",
      "[0>  17::     0] training =  0, steps = 125, max_step = 200, reward=-957.962316 ::[[-0.02838861]]: >>train tensor(-2.1277) 40\n",
      "[0>  17::     0] training =  0, steps = 162, max_step = 200, reward=-1249.461459 ::[[-0.04890396]]: >>train tensor(1.7604) 229\n",
      ">>train tensor(0.3383) 143\n",
      ">>train tensor(0.6408) 279\n",
      ">>train tensor(0.1494) 394\n",
      ">>train tensor(0.5361) 236\n",
      ">>train tensor(-0.3882) 197\n",
      ">>train tensor(-2.4567) 178\n",
      "[0>  17::     0] training =  0, steps = 164, max_step = 200, reward=-1264.556412 ::[[-0.111719]]: >>train tensor(-5.9164) 80lay::3200 ]<----\n",
      "[0>  17::     0] training =  0, steps = 200, max_step = 200, reward=-1537.997432 ::[[-0.11815158]]: >>train tensor(-1.1496) 333\n",
      ">>train tensor(-1.9930) 147\n",
      ">>train tensor(-0.2294) 344\n",
      ">>train tensor(-0.3415) 422\n",
      ">>train tensor(-0.6041) 282\n",
      ">>train tensor(-2.5360) 238\n",
      "[0>  17::     0] training =  0, steps = 200, max_step = 200, reward=-1537.997432 ::[[-0.11826422]]: [ TARGET:-194.135485 replay::3200 ]<---->>train tensor(0.3161) 214\n",
      "TEST :  -1166.2572253697888\n",
      ">>train tensor(-0.4811) 120\n",
      ">>train tensor(0.2526) 254\n",
      ">>train tensor(0.5433) 410\n",
      ">>train tensor(-0.1734) 407\n",
      ">>train tensor(-1.2015) 190\n",
      "[0>  18::     0] training =  0, steps =  42, max_step = 200, reward=-259.953614 ::[[-0.10211778]]: TARGET:-203.060536 replay::3200 ]<---->>train tensor(-0.9514) 275\n",
      ">>train tensor(-0.8365) 180\n",
      ">>train tensor(-2.4002) 473\n",
      "[0>  18::     0] training =  0, steps =  43, max_step = 200, reward=-267.321035 ::[[-0.10051668]]: [ TARGET:-201.116928 replay::3200 ]<---->>train tensor(-3.9234) 160\n",
      "[0>  18::     0] training =  0, steps =  82, max_step = 200, reward=-525.112654 ::[[-0.08529904]]: >>train tensor(-0.4291) 336\n",
      ">>train tensor(-2.0053) 377\n",
      ">>train tensor(-1.9839) 330\n",
      ">>train tensor(-0.2453) 355\n",
      ">>train tensor(-1.9375) 322\n",
      ">>train tensor(-1.3550) 223\n",
      ">>train tensor(-0.9775) 353\n",
      ">>train tensor(-2.7238) 355\n",
      ">>train tensor(-1.9790) 363\n",
      "[0>  18::     0] training =  0, steps =  85, max_step = 200, reward=-556.328567 ::[[-0.03451797]]: [ TARGET:-223.117333 replay::3400 ]<---->>train tensor(-2.1309) 328\n",
      "[0>  18::     0] training =  0, steps = 122, max_step = 200, reward=-791.459533 ::[[-0.01578625]]: >>train tensor(-2.9916) 213\n",
      ">>train tensor(-1.8992) 278\n",
      ">>train tensor(-2.1548) 102\n",
      ">>train tensor(-2.6122) 168\n",
      ">>train tensor(-1.3905) 117\n",
      ">>train tensor(-0.3874) 174\n",
      "[0>  18::     0] training =  0, steps = 123, max_step = 200, reward=-798.811591 ::[[-0.02350861]]: [ TARGET:-203.240176 replay::3400 ]<---->>train tensor(-1.0646) 297\n",
      ">>train tensor(0.7291) 40\n",
      "[0>  18::     0] training =  0, steps = 162, max_step = 200, reward=-1044.271585 ::[[-0.0538051]]:  >>train tensor(0.8597) 152\n",
      ">>train tensor(-0.8702) 235\n",
      ">>train tensor(-0.7839) 295\n",
      ">>train tensor(-0.3917) 144\n",
      ">>train tensor(-1.3874) 379\n",
      ">>train tensor(-0.2253) 174\n",
      ">>train tensor(1.00000e-02 *\n",
      "       -5.9669) 261\n",
      "[0>  18::     0] training =  0, steps = 164, max_step = 200, reward=-1051.851924 ::[[-0.03414493]]:  tensor(-0.5840) 803 replay::3400 ]<---->>train\n",
      "[0>  18::     0] training =  0, steps = 200, max_step = 200, reward=-1282.705971 ::[[-0.05068027]]: >>train tensor(-0.8356) 407\n",
      ">>train tensor(0.2776) 309\n",
      ">>train tensor(-2.2191) 297\n",
      ">>train tensor(-1.2649) 179\n",
      ">>train tensor(-1.0429) 452\n",
      ">>train tensor(-0.6015) 176\n",
      "[0>  18::     0] training =  0, steps = 200, max_step = 200, reward=-1282.705971 ::[[-0.04683904]]: [ TARGET:-215.162600 replay::3400 ]<----tensor(0.4618) 472\n",
      "TEST :  -1663.3265869280604\n",
      ">>train tensor(2.5713) 120\n",
      ">>train tensor(0.7978) 458\n",
      ">>train tensor(-0.8419) 241\n",
      "[0>  19::     0] training =  0, steps =   1, max_step = 200, reward=-0.358621 ::[[-0.20164148]]: [ TARGET:-226.599102 replay::3400 ]<---->>train tensor(0.1871) 171\n",
      "[0>  19::     0] training =  0, steps =   5, max_step = 200, reward=-2.379423 ::[[-0.1444395]]:  >>train tensor(-1.1608) 460\n",
      "[0>  19::     0] training =  0, steps =  42, max_step = 200, reward=-205.841097 ::[[-0.19987971]]: >>train tensor(1.00000e-02 *\n",
      "       9.7055) 397\n",
      ">>train tensor(0.2728) 411\n",
      ">>train tensor(-0.9618) 381\n",
      "[0>  19::     0] training =  0, steps =  43, max_step = 200, reward=-215.908080 ::[[-0.2009869]]: [ TARGET:-212.159886 replay::3400 ]<---->>train tensor(1.00000e-02 *\n",
      "       -2.8117) 160\n",
      "[0>  19::     0] training =  0, steps =  82, max_step = 200, reward=-293.084972 ::[[-0.06864871]]: >>train tensor(-0.4458) 381\n",
      ">>train tensor(-0.7078) 379\n",
      ">>train tensor(-0.1584) 361\n",
      ">>train tensor(0.3155) 334\n",
      ">>train tensor(-1.0207) 248\n",
      ">>train tensor(-0.5022) 450\n",
      ">>train tensor(-0.4844) 299\n",
      ">>train tensor(-0.9015) 368\n",
      ">>train tensor(-0.9710) 302\n",
      "[0>  19::     0] training =  0, steps =  83, max_step = 200, reward=-307.718089 ::[[-0.10619095]]: [ TARGET:-230.206286 replay::3600 ]<---->>train tensor(-1.1735) 375\n",
      "[0>  19::     0] training =  0, steps = 122, max_step = 200, reward=-494.249280 ::[[-0.25785329]]: >>train tensor(-0.8816) 288\n",
      ">>train tensor(-0.6885) 384\n",
      ">>train tensor(-0.7997) 407\n",
      ">>train tensor(-1.5389) 347\n",
      ">>train tensor(-0.1667) 196\n",
      ">>train tensor(1.2950) 202\n",
      "[0>  19::     0] training =  0, steps = 123, max_step = 200, reward=-495.076350 ::[[-0.25646415]]: [ TARGET:-223.313807 replay::3600 ]<---->>train tensor(-0.2082) 148\n",
      "[0>  19::     0] training =  0, steps = 125, max_step = 200, reward=-496.302185 ::[[-0.20459681]]: >>train tensor(3.5438) 40\n",
      "[0>  19::     0] training =  0, steps = 162, max_step = 200, reward=-682.680085 ::[[-0.22410456]]: >>train tensor(-1.5807) 307\n",
      ">>train tensor(-1.9876) 354\n",
      ">>train tensor(-1.8127) 179\n",
      ">>train tensor(-2.2148) 419\n",
      ">>train tensor(-1.7342) 396\n",
      ">>train tensor(0.1163) 135\n",
      "[0>  19::     0] training =  0, steps = 163, max_step = 200, reward=-698.503698 ::[[-0.22329332]]: [ TARGET:-227.717410 replay::3600 ]<---->>train tensor(-0.2701) 345\n",
      "[0>  19::     0] training =  0, steps = 166, max_step = 200, reward=-731.912992 ::[[-0.16301987]]: >>train tensor(3.2635) 80\n",
      "[0>  19::     0] training =  0, steps = 200, max_step = 200, reward=-885.654246 ::[[-0.11302969]]: >>train tensor(-1.9410) 127\n",
      ">>train tensor(-1.1079) 373\n",
      ">>train tensor(-1.1566) 443\n",
      ">>train tensor(1.1227) 370\n",
      ">>train tensor(0.7757) 348\n",
      ">>train tensor(-0.3639) 273\n",
      "[0>  19::     0] training =  0, steps = 200, max_step = 200, reward=-885.654246 ::[[-0.11202458]]: [ TARGET:-213.010899 replay::3600 ]<---->>train tensor(1.00000e-02 *\n",
      "       9.3212) 172\n",
      "TEST :  -867.7297468159697\n",
      ">>train tensor(3.5989) 120\n",
      ">>train tensor(-1.3083) 175\n",
      ">>train tensor(-2.5030) 307\n",
      "[0>  20::     0] training =  0, steps =   1, max_step = 200, reward=-4.089062 ::[[-0.12819282]]: [ TARGET:-244.368773 replay::3600 ]<---->>train tensor(-3.7184) 225\n",
      "[0>  20::     0] training =  0, steps =  42, max_step = 200, reward=-274.442871 ::[[-0.26824051]]: >>train tensor(-2.8610) 372\n",
      ">>train tensor(0.5415) 233\n",
      ">>train tensor(1.1301) 328\n",
      "[0>  20::     0] training =  0, steps =  43, max_step = 200, reward=-282.575692 ::[[-0.2553198]]: [ TARGET:-222.961047 replay::3600 ]<---->>train tensor(1.1569) 406\n",
      ">>train tensor(4.5986) 160\n",
      "[0>  20::     0] training =  0, steps =  82, max_step = 200, reward=-560.060054 ::[[-0.16387404]]: >>train tensor(1.00000e-02 *\n",
      "       -6.3617) 356\n",
      ">>train tensor(-1.1999) 397\n",
      ">>train tensor(-0.1569) 402\n",
      ">>train tensor(0.5117) 315\n",
      ">>train tensor(-0.3363) 411\n",
      ">>train tensor(1.00000e-02 *\n",
      "       9.9668) 406\n",
      ">>train tensor(1.1305) 361\n",
      ">>train tensor(1.00000e-02 *\n",
      "       -6.5275) 381\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>train tensor(-0.7114) 346\n",
      "[0>  20::     0] training =  0, steps =  85, max_step = 200, reward=-583.133557 ::[[-0.17245399]]: [ TARGET:-246.778482 replay::3800 ]<---->>train tensor(-0.9281) 339\n",
      "[0>  20::     0] training =  0, steps = 122, max_step = 200, reward=-830.744182 ::[[-0.18158035]]: >>train tensor(-2.2179) 266\n",
      ">>train tensor(-0.1910) 135\n",
      ">>train tensor(0.8455) 222\n",
      ">>train tensor(1.2595) 101\n",
      ">>train tensor(1.5898) 110\n",
      ">>train tensor(0.6412) 476\n",
      "[0>  20::     0] training =  0, steps = 123, max_step = 200, reward=-835.333467 ::[[-0.18496491]]: [ TARGET:-229.142941 replay::3800 ]<---->>train tensor(0.5377) 295\n",
      "[0>  20::     0] training =  0, steps = 127, max_step = 200, reward=-852.871126 ::[[-0.40255989]]: >>train tensor(-1.0367) 40\n",
      "[0>  20::     0] training =  0, steps = 162, max_step = 200, reward=-1086.324642 ::[[-0.3489864]]:  >>train tensor(1.00000e-02 *\n",
      "       -2.7775) 371\n",
      ">>train tensor(-0.8186) 368\n",
      ">>train tensor(-0.2814) 219\n",
      ">>train tensor(0.3073) 131\n",
      ">>train tensor(0.7951) 365\n",
      ">>train tensor(0.6311) 156\n",
      "[0>  20::     0] training =  0, steps = 163, max_step = 200, reward=-1091.055539 ::[[-0.35851138]]: [ TARGET:-238.913470 replay::3800 ]<---->>train tensor(1.5168) 221\n",
      "[0>  20::     0] training =  0, steps = 166, max_step = 200, reward=-1110.469729 ::[[-0.10581879]]: tensor(-1.0297) 80\n",
      "[0>  20::     0] training =  0, steps = 200, max_step = 200, reward=-1337.676763 ::[[-0.11877214]]: >>train tensor(-0.1073) 121\n",
      ">>train tensor(0.7525) 306\n",
      ">>train tensor(0.3311) 465\n",
      ">>train tensor(-2.1471) 473\n",
      ">>train tensor(-0.5768) 347\n",
      ">>train tensor(-1.9091) 250\n",
      "[0>  20::     0] training =  0, steps = 200, max_step = 200, reward=-1337.676763 ::[[-0.11911294]]: [ TARGET:-235.830326 replay::3800 ]<---->>train tensor(1.7327) 260\n",
      "TEST :  -1280.651438486226\n",
      ">>train tensor(0.6177) 120\n",
      ">>train tensor(0.6312) 442\n",
      ">>train tensor(1.00000e-02 *\n",
      "       2.2133) 469\n",
      ">>train tensor(-0.5486) 172\n",
      "[0>  21::     0] training =  0, steps =  42, max_step = 200, reward=-237.248513 ::[[-0.27064714]]: >>train tensor(-2.9957) 190::3800 ]<----\n",
      ">>train tensor(-3.0240) 478\n",
      ">>train tensor(-2.2512) 473\n",
      "[0>  21::     0] training =  0, steps =  43, max_step = 200, reward=-243.147827 ::[[-0.27304349]]: [ TARGET:-236.962075 replay::3800 ]<---->>train tensor(2.6548) 300\n",
      ">>train tensor(-0.3367) 160\n",
      "[0>  21::     0] training =  0, steps =  82, max_step = 200, reward=-400.981564 ::[[-0.20007857]]: >>train tensor(1.7242) 385\n",
      ">>train tensor(1.0396) 379\n",
      ">>train tensor(1.6753) 405\n",
      ">>train tensor(0.8504) 480\n",
      ">>train tensor(-0.7483) 482\n",
      ">>train tensor(-1.8149) 369\n",
      ">>train tensor(-2.5084) 327\n",
      ">>train tensor(-2.8532) 415\n",
      ">>train tensor(-2.1992) 366\n",
      "[0>  21::     0] training =  0, steps =  85, max_step = 200, reward=-424.197201 ::[[-0.22327223]]: [ TARGET:-265.231557 replay::4000 ]<---->>train tensor(-0.8325) 484\n",
      "[0>  21::     0] training =  0, steps = 122, max_step = 200, reward=-647.031455 ::[[-0.26492471]]: >>train tensor(-1.0293) 224\n",
      ">>train tensor(1.7851) 277\n",
      ">>train tensor(1.2804) 224\n",
      ">>train tensor(1.2120) 355\n",
      ">>train tensor(1.4555) 276\n",
      ">>train tensor(0.1357) 407\n",
      "[0>  21::     0] training =  0, steps = 123, max_step = 200, reward=-647.902390 ::[[-0.26664805]]: [ TARGET:-255.944323 replay::4000 ]<---->>train tensor(-2.4202) 357\n",
      ">>train tensor(-2.8071) 40\n",
      "[0>  21::     0] training =  0, steps = 162, max_step = 200, reward=-872.443841 ::[[-0.19149987]]: >>train tensor(-2.0485) 272\n",
      ">>train tensor(-2.4086) 452\n",
      ">>train tensor(-1.6528) 446\n",
      ">>train tensor(-0.2312) 131\n",
      ">>train tensor(0.4949) 416\n",
      ">>train tensor(0.5176) 279\n",
      "[0>  21::     0] training =  0, steps = 163, max_step = 200, reward=-881.876599 ::[[-0.1900306]]: [ TARGET:-249.199221 replay::4000 ]<---->>train tensor(1.00000e-02 *\n",
      "       5.7880) 201\n",
      ">>train tensor(1.9922) 80\n",
      "[0>  21::     0] training =  0, steps = 200, max_step = 200, reward=-1041.125640 ::[[-0.16700259]]: >>train tensor(-1.5439) 213\n",
      ">>train tensor(-2.1656) 251\n",
      ">>train tensor(-2.4803) 405\n",
      ">>train tensor(-1.8277) 163\n",
      ">>train tensor(-1.0663) 277\n",
      ">>train tensor(0.3212) 237\n",
      "[0>  21::     0] training =  0, steps = 200, max_step = 200, reward=-1041.125640 ::[[-0.16692525]]: [ TARGET:-251.313817 replay::4000 ]<---- tensor(1.5937) 128\n",
      "TEST :  -1440.417122681765\n",
      ">>train tensor(1.4529) 120\n",
      ">>train tensor(0.4228) 508\n",
      "[0>  22::     0] training =  0, steps =   1, max_step = 200, reward=-2.673446 ::[[-0.22648669]]: [ TARGET:-272.983052 replay::4000 ]<---->>train tensor(1.1539) 414\n",
      "[0>  22::     0] training =  0, steps =   3, max_step = 200, reward=-9.693473 ::[[-0.23117681]]: >>train tensor(-1.1329) 240\n",
      "[0>  22::     0] training =  0, steps =  42, max_step = 200, reward=-246.357419 ::[[-0.30164318]]: >>train tensor(-1.2337) 192\n",
      ">>train tensor(-1.4652) 491\n",
      ">>train tensor(-1.8316) 463\n",
      "[0>  22::     0] training =  0, steps =  43, max_step = 200, reward=-251.892321 ::[[-0.28776538]]: [ TARGET:-269.411803 replay::4000 ]<---->>train tensor(1.7206) 410\n",
      "[0>  22::     0] training =  0, steps =  46, max_step = 200, reward=-277.885681 ::[[-0.08404422]]: >>train tensor(0.9254) 160\n",
      "[0>  22::     0] training =  0, steps =  82, max_step = 200, reward=-487.112238 ::[[-0.08667812]]: >>train tensor(1.5735) 403\n",
      ">>train tensor(1.5356) 411\n",
      ">>train tensor(-0.5141) 494\n",
      ">>train tensor(-0.8621) 457\n",
      ">>train tensor(1.00000e-03 *\n",
      "       5.5644) 434\n",
      ">>train tensor(0.1572) 418\n",
      ">>train tensor(0.2085) 360\n",
      ">>train tensor(-1.4629) 333\n",
      ">>train tensor(1.00000e-02 *\n",
      "       -5.0373) 469\n",
      "[0>  22::     0] training =  0, steps =  85, max_step = 200, reward=-514.820303 ::[[-0.19876143]]: >>train tensor(1.8196) 416y::4200 ]<----\n",
      "[0>  22::     0] training =  0, steps = 122, max_step = 200, reward=-728.726762 ::[[-0.16515319]]: >>train tensor(1.0429) 187\n",
      ">>train tensor(-0.5963) 345\n",
      ">>train tensor(-0.9276) 380\n",
      ">>train tensor(-1.6844) 266\n",
      ">>train tensor(-1.4245) 192\n",
      ">>train tensor(-0.3929) 316\n",
      "[0>  22::     0] training =  0, steps = 123, max_step = 200, reward=-738.070920 ::[[-0.15487383]]: [ TARGET:-263.763289 replay::4200 ]<---->>train tensor(-0.5654) 203\n",
      "[0>  22::     0] training =  0, steps = 125, max_step = 200, reward=-762.344248 ::[[-0.08040145]]: >>train tensor(-0.1967) 40\n",
      "[0>  22::     0] training =  0, steps = 162, max_step = 200, reward=-968.356610 ::[[-0.09531762]]: >>train tensor(-0.3345) 124\n",
      ">>train tensor(-0.4341) 331\n",
      ">>train tensor(-0.1102) 427\n",
      ">>train tensor(-0.7305) 365\n",
      ">>train tensor(-1.1989) 460\n",
      ">>train tensor(-1.4036) 180\n",
      ">>train tensor(-1.2790) 232\n",
      "[0>  22::     0] training =  0, steps = 167, max_step = 200, reward=-1022.541782 ::[[-0.10908017]]: >>train tensor(-0.2544) 80::4200 ]<----\n",
      "[0>  22::     0] training =  0, steps = 200, max_step = 200, reward=-1187.221211 ::[[-0.13948414]]: >>train tensor(-0.1228) 451\n",
      ">>train tensor(1.0162) 203\n",
      ">>train tensor(-1.5351) 136\n",
      ">>train tensor(1.00000e-02 *\n",
      "       -2.0193) 460\n",
      ">>train tensor(1.00000e-02 *\n",
      "       3.4230) 283\n",
      ">>train tensor(-1.2281) 586\n",
      "[0>  22::     0] training =  0, steps = 200, max_step = 200, reward=-1187.221211 ::[[-0.13352062]]: [ TARGET:-271.472471 replay::4200 ]<---->>train tensor(-0.8336) 437\n",
      "TEST :  -1739.869935359372\n",
      ">>train tensor(1.6678) 120\n",
      ">>train tensor(0.4835) 422\n",
      ">>train tensor(-0.7086) 455\n",
      ">>train tensor(-1.0271) 197\n",
      "[0>  23::     0] training =  0, steps =  42, max_step = 200, reward=-245.930884 ::[[-0.30311572]]: TARGET:-274.372637 replay::4200 ]<---->>train tensor(-1.0075) 559\n",
      ">>train tensor(1.00000e-02 *\n",
      "       -3.9243) 338\n",
      ">>train tensor(-0.3266) 451\n",
      ">>train tensor(1.3694) 259\n",
      "[0>  23::     0] training =  0, steps =  44, max_step = 200, reward=-252.744761 ::[[-0.05615492]]: >>train tensor(0.8288) 160::4200 ]<----\n",
      "[0>  23::     0] training =  0, steps =  82, max_step = 200, reward=-405.572182 ::[[-0.10744654]]: >>train tensor(-0.2311) 376\n",
      ">>train tensor(-0.5418) 469\n",
      ">>train tensor(-1.3630) 484\n",
      ">>train tensor(0.2032) 361\n",
      ">>train tensor(-1.0561) 447\n",
      ">>train tensor(-0.4938) 413\n",
      ">>train tensor(-1.0156) 409\n",
      ">>train tensor(-1.0001) 374\n",
      ">>train tensor(-0.1172) 491\n",
      "[0>  23::     0] training =  0, steps =  85, max_step = 200, reward=-432.395271 ::[[-0.20019343]]: >>train tensor(-0.4155) 386::4400 ]<----\n",
      "[0>  23::     0] training =  0, steps = 122, max_step = 200, reward=-647.558986 ::[[-0.18655654]]: >>train tensor(-1.1348) 281\n",
      ">>train tensor(-1.0239) 331\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>train tensor(-0.3681) 149\n",
      ">>train tensor(-1.4383) 455\n",
      ">>train tensor(-0.4474) 265\n",
      ">>train tensor(-0.8645) 142\n",
      "[0>  23::     0] training =  0, steps = 123, max_step = 200, reward=-648.345969 ::[[-0.18769546]]: [ TARGET:-286.686298 replay::4400 ]<---->>train tensor(0.2689) 411\n",
      "[0>  23::     0] training =  0, steps = 124, max_step = 200, reward=-649.061784 ::[[-0.14925852]]: tensor(-0.9984) 40\n",
      "[0>  23::     0] training =  0, steps = 162, max_step = 200, reward=-853.194009 ::[[-0.15470082]]: >>train tensor(0.7570) 307\n",
      ">>train tensor(-1.1121) 305\n",
      ">>train tensor(-0.6236) 458\n",
      ">>train tensor(-0.2109) 143\n",
      ">>train tensor(-1.2750) 465\n",
      ">>train tensor(-1.6268) 226\n",
      ">>train tensor(-0.5635) 436\n",
      "[0>  23::     0] training =  0, steps = 167, max_step = 200, reward=-896.512672 ::[[-0.28251349]]: >>train tensor(0.9704) 80ay::4400 ]<----\n",
      "[0>  23::     0] training =  0, steps = 200, max_step = 200, reward=-1040.939048 ::[[-0.25504425]]: >>train tensor(-1.5970) 164\n",
      ">>train tensor(-0.4035) 367\n",
      ">>train tensor(-1.7062) 316\n",
      ">>train tensor(-0.7354) 456\n",
      ">>train tensor(0.2225) 217\n",
      ">>train tensor(1.00000e-03 *\n",
      "       9.8242) 307\n",
      "[0>  23::     0] training =  0, steps = 200, max_step = 200, reward=-1040.939048 ::[[-0.25646221]]: [ TARGET:-284.910133 replay::4400 ]<---->>train tensor(1.2332) 529\n",
      "TEST :  -1908.9645042846105\n",
      ">>train tensor(0.6662) 120\n",
      ">>train tensor(-1.6153) 267\n",
      ">>train tensor(-1.5850) 639\n",
      "[0>  24::     0] training =  0, steps =   1, max_step = 200, reward=-1.144000 ::[[-0.14304756]]: [ TARGET:-294.139944 replay::4400 ]<---->>train tensor(-1.7020) 554\n",
      "[0>  24::     0] training =  0, steps =  42, max_step = 200, reward=-233.943462 ::[[-0.24662115]]: >>train tensor(-2.7742) 443\n",
      ">>train tensor(-1.7716) 655\n",
      ">>train tensor(-0.6530) 524\n",
      ">>train tensor(0.6545) 332\n",
      "[0>  24::     0] training =  0, steps =  44, max_step = 200, reward=-237.162503 ::[[-0.16698238]]: >>train tensor(0.5631) 160::4400 ]<----\n",
      "[0>  24::     0] training =  0, steps =  82, max_step = 200, reward=-440.243845 ::[[-0.14712554]]: >>train tensor(0.1229) 394\n",
      ">>train tensor(-0.3788) 502\n",
      ">>train tensor(-0.5725) 389\n",
      ">>train tensor(-0.1318) 485\n",
      ">>train tensor(-1.3097) 409\n",
      ">>train tensor(-0.5593) 377\n",
      ">>train tensor(-0.7639) 455\n",
      ">>train tensor(-0.8495) 439\n",
      ">>train tensor(1.00000e-02 *\n",
      "       -8.9831) 603\n",
      "[0>  24::     0] training =  0, steps =  85, max_step = 200, reward=-463.064701 ::[[-0.08074431]]: >>train tensor(-0.6519) 519::4600 ]<----\n",
      "[0>  24::     0] training =  0, steps = 122, max_step = 200, reward=-613.678247 ::[[-0.16986199]]: >>train tensor(-0.1379) 268\n",
      ">>train tensor(0.4982) 351\n",
      ">>train tensor(0.1106) 474\n",
      ">>train tensor(1.00000e-02 *\n",
      "       -6.2316) 337\n",
      ">>train tensor(-0.7069) 468\n",
      ">>train tensor(-1.5318) 323\n",
      "[0>  24::     0] training =  0, steps = 123, max_step = 200, reward=-620.152720 ::[[-0.1714099]]: [ TARGET:-296.490003 replay::4600 ]<---->>train tensor(-1.0482) 338\n",
      ">>train tensor(-0.6653) 40\n",
      "[0>  24::     0] training =  0, steps = 162, max_step = 200, reward=-837.133373 ::[[-0.16319046]]: >>train tensor(-0.4027) 236\n",
      ">>train tensor(-0.2625) 306\n",
      ">>train tensor(-1.2257) 167\n",
      ">>train tensor(-0.6372) 375\n",
      ">>train tensor(-1.6778) 204\n",
      ">>train tensor(0.3637) 166\n",
      "[0>  24::     0] training =  0, steps = 163, max_step = 200, reward=-838.156409 ::[[-0.16236035]]: [ TARGET:-308.818847 replay::4600 ]<---->>train tensor(0.3923) 212\n",
      "[0>  24::     0] training =  0, steps = 166, max_step = 200, reward=-842.809537 ::[[-0.16760559]]: >>train tensor(0.7443) 80\n",
      "[0>  24::     0] training =  0, steps = 200, max_step = 200, reward=-1042.221700 ::[[-0.1185606]]:  >>train tensor(-1.2471) 148\n",
      ">>train tensor(-1.1301) 389\n",
      ">>train tensor(-1.7624) 424\n",
      ">>train tensor(-1.0593) 403\n",
      ">>train tensor(-0.3599) 432\n",
      ">>train tensor(-2.6091) 502\n",
      "[0>  24::     0] training =  0, steps = 200, max_step = 200, reward=-1042.221700 ::[[-0.11877533]]: [ TARGET:-301.172534 replay::4600 ]<---->>train tensor(0.4342) 410\n",
      "TEST :  -1452.2617642241137\n",
      ">>train tensor(1.9372) 120\n",
      ">>train tensor(1.00000e-03 *\n",
      "       -7.2888) 347\n",
      "[0>  25::     0] training =  0, steps =   1, max_step = 200, reward=-3.942112 ::[[-0.09630668]]: [ TARGET:-327.078593 replay::4600 ]<---->>train tensor(-2.1527) 157\n",
      "[0>  25::     0] training =  0, steps =  35, max_step = 200, reward=-233.927313 ::[[-0.11726582]]: >>train tensor(-2.0859) 368::4600 ]<----\n",
      "[0>  25::     0] training =  0, steps =  42, max_step = 200, reward=-274.300238 ::[[-0.09975616]]: >>train tensor(-1.6662) 483\n",
      ">>train tensor(-1.1781) 394\n",
      ">>train tensor(-0.6731) 485\n",
      "[0>  25::     0] training =  0, steps =  43, max_step = 200, reward=-284.696568 ::[[-0.09648382]]: [ TARGET:-302.468208 replay::4600 ]<---->>train tensor(0.2043) 419\n",
      ">>train tensor(1.6011) 160\n",
      "[0>  25::     0] training =  0, steps =  82, max_step = 200, reward=-553.282179 ::[[-0.13111697]]: >>train tensor(-0.6130) 479\n",
      ">>train tensor(-1.1782) 479\n",
      ">>train tensor(-1.3195) 558\n",
      ">>train tensor(-0.8527) 530\n",
      ">>train tensor(-1.5729) 407\n",
      ">>train tensor(-1.4650) 508\n",
      ">>train tensor(0.3552) 464\n",
      ">>train tensor(-0.9383) 524\n",
      ">>train tensor(-0.9751) 449\n",
      "[0>  25::     0] training =  0, steps =  85, max_step = 200, reward=-573.010726 ::[[-0.13426743]]: >>train tensor(-0.2844) 543:4800 ]<-----\n",
      "[0>  25::     0] training =  0, steps = 122, max_step = 200, reward=-813.887954 ::[[-0.10769664]]: >>train tensor(-0.6957) 174\n",
      ">>train tensor(-0.5742) 445\n",
      ">>train tensor(-1.0180) 508\n",
      ">>train tensor(-0.8645) 480\n",
      ">>train tensor(-0.8018) 252\n",
      ">>train tensor(0.6044) 130\n",
      "[0>  25::     0] training =  0, steps = 123, max_step = 200, reward=-817.959632 ::[[-0.11061883]]: [ TARGET:-315.780535 replay::4800 ]<---->>train tensor(1.00000e-02 *\n",
      "       -1.8405) 373\n",
      ">>train tensor(-0.4930) 40\n",
      "[0>  25::     0] training =  0, steps = 162, max_step = 200, reward=-1064.052698 ::[[-0.19344601]]: >>train tensor(0.3501) 167\n",
      ">>train tensor(-0.2224) 283\n",
      ">>train tensor(-0.1850) 379\n",
      ">>train tensor(-0.9941) 214\n",
      ">>train tensor(-0.3089) 317\n",
      ">>train tensor(1.3991) 455\n",
      "[0>  25::     0] training =  0, steps = 163, max_step = 200, reward=-1068.148166 ::[[-0.19772011]]: [ TARGET:-326.655331 replay::4800 ]<---->>train tensor(2.2223) 356\n",
      "[0>  25::     0] training =  0, steps = 165, max_step = 200, reward=-1078.483446 ::[[-0.21641985]]: >>train tensor(-0.9580) 80\n",
      "[0>  25::     0] training =  0, steps = 200, max_step = 200, reward=-1306.821093 ::[[-0.19372281]]: >>train tensor(-0.3418) 369\n",
      ">>train tensor(-0.6581) 509\n",
      ">>train tensor(-0.9245) 225\n",
      ">>train tensor(-0.3975) 355\n",
      ">>train tensor(-1.1316) 167\n",
      ">>train tensor(-1.2306) 548\n",
      "[0>  25::     0] training =  0, steps = 200, max_step = 200, reward=-1306.821093 ::[[-0.19505778]]: [ TARGET:-320.660892 replay::4800 ]<---->>train tensor(-0.1034) 243\n",
      "TEST :  -1339.2210804110914\n",
      ">>train tensor(1.2782) 120\n",
      ">>train tensor(1.6562) 295\n",
      "[0>  26::     0] training =  0, steps =   1, max_step = 200, reward=-1.656808 ::[[-0.23933072]]: [ TARGET:-327.069857 replay::4800 ]<---->>train tensor(0.3317) 338\n",
      "[0>  26::     0] training =  0, steps =   4, max_step = 200, reward=-10.144950 ::[[-0.21009137]]: >>train tensor(-0.4088) 598\n",
      "[0>  26::     0] training =  0, steps =  42, max_step = 200, reward=-234.173251 ::[[-0.17142775]]: >>train tensor(-2.1109) 435\n",
      ">>train tensor(-2.5403) 195\n",
      ">>train tensor(-0.2724) 301\n",
      ">>train tensor(-0.6310) 620\n",
      "[0>  26::     0] training =  0, steps =  48, max_step = 200, reward=-246.955369 ::[[-0.17655339]]: >>train tensor(-0.4804) 160::4800 ]<----\n",
      "[0>  26::     0] training =  0, steps =  82, max_step = 200, reward=-461.202416 ::[[-0.18994189]]: >>train tensor(0.9353) 527\n",
      ">>train tensor(-0.2704) 488\n",
      ">>train tensor(1.00000e-02 *\n",
      "       2.9603) 513\n",
      ">>train tensor(-1.4369) 529\n",
      ">>train tensor(-1.2092) 556\n",
      ">>train tensor(-0.3399) 499\n",
      ">>train tensor(1.00000e-02 *\n",
      "       4.8202) 495\n",
      ">>train tensor(-1.2017) 576\n",
      ">>train tensor(0.2222) 572\n",
      "[0>  26::     0] training =  0, steps =  84, max_step = 200, reward=-467.556948 ::[[-0.14867252]]: tensor(-0.2709) 448 replay::5000 ]<---->>train \n",
      "[0>  26::     0] training =  0, steps = 122, max_step = 200, reward=-661.849489 ::[[-0.16974758]]: >>train tensor(-0.6017) 328\n",
      ">>train tensor(-0.3951) 504\n",
      ">>train tensor(-0.2256) 269\n",
      ">>train tensor(-1.4269) 432\n",
      ">>train tensor(-1.3288) 464\n",
      ">>train tensor(-0.1109) 393\n",
      "[0>  26::     0] training =  0, steps = 123, max_step = 200, reward=-673.316748 ::[[-0.16678814]]: [ TARGET:-324.288064 replay::5000 ]<---->>train tensor(-0.4353) 399\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>train tensor(0.4609) 40\n",
      "[0>  26::     0] training =  0, steps = 162, max_step = 200, reward=-846.215894 ::[[-0.17746883]]: >>train tensor(-0.3907) 176\n",
      ">>train tensor(-1.3511) 283\n",
      ">>train tensor(-0.2284) 173\n",
      ">>train tensor(-1.3804) 271\n",
      ">>train tensor(0.1592) 162\n",
      ">>train tensor(-1.0827) 284\n",
      "[0>  26::     0] training =  0, steps = 163, max_step = 200, reward=-852.556385 ::[[-0.17476275]]: [ TARGET:-334.760496 replay::5000 ]<---->>train tensor(0.5145) 541\n",
      ">>train tensor(1.00000e-02 *\n",
      "       -4.6360) 80\n",
      "[0>  26::     0] training =  0, steps = 200, max_step = 200, reward=-1068.301135 ::[[-0.20418282]]: >>train tensor(-0.3613) 481\n",
      ">>train tensor(1.00000e-02 *\n",
      "       2.0484) 279\n",
      ">>train tensor(0.4247) 368\n",
      ">>train tensor(0.3951) 217\n",
      ">>train tensor(0.4176) 592\n",
      ">>train tensor(-0.6370) 551\n",
      ">>train tensor(1.00000e-03 *\n",
      "       1.9535) 146\n",
      "[0>  26::     0] training =  0, steps = 200, max_step = 200, reward=-1068.301135 ::[[-0.20192667]]: [ TARGET:-340.495147 replay::5000 ]<----TEST :  -1261.4033556239347\n",
      ">>train tensor(2.1920) 120\n",
      ">>train tensor(-1.0730) 466\n",
      "[0>  27::     0] training =  0, steps =   1, max_step = 200, reward=-1.415119 ::[[-0.20281774]]: [ TARGET:-327.658905 replay::5000 ]<---->>train tensor(-1.3689) 658\n",
      "[0>  27::     0] training =  0, steps =  26, max_step = 200, reward=-133.545340 ::[[-0.23994366]]: tensor(-0.7966) 5892 replay::5000 ]<----\n",
      "[0>  27::     0] training =  0, steps =  42, max_step = 200, reward=-235.061248 ::[[-0.25228952]]: >>train tensor(-0.9140) 455\n",
      ">>train tensor(1.0218) 296\n",
      ">>train tensor(1.00000e-02 *\n",
      "       6.6037) 608\n",
      "[0>  27::     0] training =  0, steps =  44, max_step = 200, reward=-237.859237 ::[[-0.10617928]]: [ TARGET:-332.891840 replay::5000 ]<---->>train tensor(0.3264) 466\n",
      ">>train tensor(1.5901) 160\n",
      "[0>  27::     0] training =  0, steps =  82, max_step = 200, reward=-467.551797 ::[[-0.15317212]]: >>train tensor(-1.7244) 411\n",
      ">>train tensor(-2.0284) 436\n",
      ">>train tensor(-1.8381) 423\n",
      ">>train tensor(-0.1884) 541\n",
      ">>train tensor(0.1756) 533\n",
      ">>train tensor(1.3413) 578\n",
      ">>train tensor(1.0133) 553\n",
      ">>train tensor(-0.9769) 563\n",
      ">>train tensor(-1.8925) 496\n",
      "[0>  27::     0] training =  0, steps =  85, max_step = 200, reward=-471.534305 ::[[-0.24716364]]: >>train tensor(-1.8362) 546::5200 ]<----\n",
      "[0>  27::     0] training =  0, steps = 122, max_step = 200, reward=-697.875555 ::[[-0.33157854]]: >>train tensor(0.1816) 274\n",
      ">>train tensor(0.1341) 306\n",
      ">>train tensor(0.4566) 369\n",
      ">>train tensor(-0.1206) 270\n",
      ">>train tensor(-0.1455) 346\n",
      ">>train tensor(-1.2131) 553\n",
      ">>train tensor(-1.7340) 208\n",
      "[0>  27::     0] training =  0, steps = 124, max_step = 200, reward=-702.673208 ::[[-0.21963357]]: >>train tensor(1.00000e-03 *:5200 ]<----\n",
      "       -9.9583) 40\n",
      "[0>  27::     0] training =  0, steps = 162, max_step = 200, reward=-919.536829 ::[[-0.17960804]]: >>train tensor(-0.6975) 349\n",
      ">>train tensor(1.00000e-02 *\n",
      "       -2.7644) 224\n",
      ">>train tensor(-1.3338) 397\n",
      ">>train tensor(-0.1597) 315\n",
      ">>train tensor(-1.9739) 320\n",
      ">>train tensor(-1.0927) 379\n",
      "[0>  27::     0] training =  0, steps = 163, max_step = 200, reward=-925.762716 ::[[-0.18115402]]: [ TARGET:-342.737477 replay::5200 ]<---->>train tensor(1.0472) 561\n",
      "[0>  27::     0] training =  0, steps = 167, max_step = 200, reward=-938.907216 ::[[-0.2102721]]: tensor(0.7533) 80\n",
      "[0>  27::     0] training =  0, steps = 200, max_step = 200, reward=-1102.955816 ::[[-0.20348192]]: >>train tensor(-0.4180) 262\n",
      ">>train tensor(-1.4711) 274\n",
      ">>train tensor(-0.5379) 240\n",
      ">>train tensor(-1.5359) 512\n",
      ">>train tensor(-1.3715) 605\n",
      ">>train tensor(-1.0572) 320\n",
      ">>train tensor(-2.3259) 220\n",
      "[0>  27::     0] training =  0, steps = 200, max_step = 200, reward=-1102.955816 ::[[-0.20264901]]: [ TARGET:-348.244117 replay::5200 ]<----TEST :  -1547.349249738507\n",
      ">>train tensor(2.8907) 120\n",
      ">>train tensor(-0.2935) 622\n",
      "[0>  28::     0] training =  0, steps =   1, max_step = 200, reward=-0.077356 ::[[-0.1863434]]: [ TARGET:-344.836183 replay::5200 ]<---->>train tensor(-0.3431) 706\n",
      "[0>  28::     0] training =  0, steps =  18, max_step = 200, reward=-84.226445 ::[[-0.19910645]]: tensor(-0.4825) 4061 replay::5200 ]<----\n",
      "[0>  28::     0] training =  0, steps =  42, max_step = 200, reward=-203.756116 ::[[-0.27594575]]: >>train tensor(-1.2664) 484\n",
      ">>train tensor(-0.3203) 510\n",
      ">>train tensor(-1.1167) 473\n",
      "[0>  28::     0] training =  0, steps =  43, max_step = 200, reward=-216.489852 ::[[-0.27325397]]: [ TARGET:-356.508301 replay::5200 ]<---->>train tensor(-1.1145) 602\n",
      "[0>  28::     0] training =  0, steps =  47, max_step = 200, reward=-245.772734 ::[[-0.20315299]]: >>train tensor(0.3905) 160\n",
      "[0>  28::     0] training =  0, steps =  82, max_step = 200, reward=-391.696937 ::[[-0.18952845]]: >>train tensor(-0.6211) 571\n",
      ">>train tensor(-2.0163) 566\n",
      ">>train tensor(-1.0381) 554\n",
      ">>train tensor(-1.4380) 469\n",
      ">>train tensor(-1.4069) 501\n",
      ">>train tensor(1.00000e-02 *\n",
      "       -8.4559) 542\n",
      ">>train tensor(-1.1776) 586\n",
      ">>train tensor(-0.9879) 517\n",
      ">>train tensor(-1.0792) 516\n",
      "[0>  28::     0] training =  0, steps =  85, max_step = 200, reward=-400.714791 ::[[-0.22386325]]: >>train tensor(-1.1709) 563::5400 ]<----\n",
      "[0>  28::     0] training =  0, steps = 122, max_step = 200, reward=-642.682874 ::[[-0.24501784]]: >>train tensor(-1.2609) 182\n",
      ">>train tensor(-1.1869) 374\n",
      ">>train tensor(0.1365) 415\n",
      ">>train tensor(-1.2485) 346\n",
      ">>train tensor(-0.9209) 541\n",
      ">>train tensor(-1.6179) 567\n",
      "[0>  28::     0] training =  0, steps = 123, max_step = 200, reward=-644.350630 ::[[-0.24497577]]: [ TARGET:-362.376157 replay::5400 ]<---->>train tensor(-2.1239) 449\n",
      ">>train tensor(1.3635) 40\n",
      "[0>  28::     0] training =  0, steps = 162, max_step = 200, reward=-834.592078 ::[[-0.22298522]]: >>train tensor(-2.8317) 551\n",
      ">>train tensor(-1.7393) 160\n",
      ">>train tensor(-1.1264) 422\n",
      ">>train tensor(-0.8154) 603\n",
      ">>train tensor(1.5518) 364\n",
      ">>train tensor(0.3944) 337\n",
      ">>train tensor(-0.3967) 418\n",
      "[0>  28::     0] training =  0, steps = 166, max_step = 200, reward=-883.817941 ::[[-0.17550059]]: >>train tensor(1.1774) 80ay::5400 ]<----\n",
      "[0>  28::     0] training =  0, steps = 200, max_step = 200, reward=-1038.573858 ::[[-0.2044411]]:  >>train tensor(-1.6957) 288\n",
      ">>train tensor(-3.5238) 207\n",
      ">>train tensor(-2.1855) 180\n",
      ">>train tensor(-3.1662) 240\n",
      ">>train tensor(1.00000e-02 *\n",
      "       -7.8792) 334\n",
      ">>train tensor(0.3844) 172\n",
      ">>train tensor(0.2616) 416\n",
      "[0>  28::     0] training =  0, steps = 200, max_step = 200, reward=-1038.573858 ::[[-0.20453437]]: [ TARGET:-358.071362 replay::5400 ]<----TEST :  -858.0432772214699\n",
      ">>train tensor(1.1564) 120\n",
      ">>train tensor(-0.8239) 477\n",
      "[0>  29::     0] training =  0, steps =   3, max_step = 200, reward=-2.100142 ::[[-0.23005641]]: [ TARGET:-384.178912 replay::5400 ]<---->>train tensor(-2.6675) 268\n",
      "[0>  29::     0] training =  0, steps =  34, max_step = 200, reward=-157.074030 ::[[-0.20952753]]: >>train tensor(-2.2191) 705::5400 ]<----\n",
      "[0>  29::     0] training =  0, steps =  42, max_step = 200, reward=-231.406176 ::[[-0.17593594]]: >>train tensor(-2.2958) 287\n",
      ">>train tensor(-1.1257) 497\n",
      ">>train tensor(-0.5864) 364\n",
      "[0>  29::     0] training =  0, steps =  43, max_step = 200, reward=-234.247275 ::[[-0.17522902]]: [ TARGET:-365.205483 replay::5400 ]<---->>train tensor(-0.4858) 390\n",
      ">>train tensor(-0.2490) 160\n",
      "[0>  29::     0] training =  0, steps =  82, max_step = 200, reward=-418.053795 ::[[-0.21678014]]: >>train tensor(-1.7610) 449\n",
      ">>train tensor(-1.5107) 528\n",
      ">>train tensor(-0.6095) 561\n",
      ">>train tensor(0.7102) 506\n",
      ">>train tensor(-0.3268) 571\n",
      ">>train tensor(0.2348) 519\n",
      ">>train tensor(-1.8224) 503\n",
      ">>train tensor(-2.3004) 498\n",
      ">>train tensor(-0.4368) 468\n",
      "[0>  29::     0] training =  0, steps =  87, max_step = 200, reward=-465.133371 ::[[-0.21964208]]: >>train tensor(-2.2464) 619::5600 ]<----\n",
      "[0>  29::     0] training =  0, steps = 122, max_step = 200, reward=-607.583142 ::[[-0.18997656]]: >>train tensor(-1.4583) 232\n",
      ">>train tensor(-1.1239) 190\n",
      ">>train tensor(-0.6387) 340\n",
      ">>train tensor(-1.3066) 545\n",
      ">>train tensor(-0.1780) 309\n",
      ">>train tensor(0.9794) 410\n",
      "[0>  29::     0] training =  0, steps = 123, max_step = 200, reward=-611.177486 ::[[-0.19011287]]: [ TARGET:-369.120761 replay::5600 ]<---->>train tensor(-0.2534) 327\n",
      ">>train tensor(0.6771) 40\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0>  29::     0] training =  0, steps = 162, max_step = 200, reward=-836.403427 ::[[-0.2077422]]:  >>train tensor(-1.0010) 514\n",
      ">>train tensor(-2.4357) 398\n",
      ">>train tensor(-3.6489) 156\n",
      ">>train tensor(-0.7796) 628\n",
      ">>train tensor(-0.2635) 534\n",
      ">>train tensor(0.6261) 215\n",
      "[0>  29::     0] training =  0, steps = 163, max_step = 200, reward=-837.722139 ::[[-0.20769997]]: [ TARGET:-374.619291 replay::5600 ]<---->>train tensor(-0.5544) 479\n",
      "[0>  29::     0] training =  0, steps = 167, max_step = 200, reward=-843.170331 ::[[-0.1455405]]: >>train tensor(1.9288) 80\n",
      "[0>  29::     0] training =  0, steps = 200, max_step = 200, reward=-1037.487139 ::[[-0.14129453]]: >>train tensor(-1.5762) 482\n",
      ">>train tensor(-1.8890) 567\n",
      ">>train tensor(-1.5403) 314\n",
      ">>train tensor(-0.4861) 621\n",
      ">>train tensor(0.7815) 220\n",
      ">>train tensor(0.1501) 347\n",
      "[0>  29::     0] training =  0, steps = 200, max_step = 200, reward=-1037.487139 ::[[-0.14126447]]: [ TARGET:-368.347342 replay::5600 ]<---->>train tensor(-1.3823) 395\n",
      "TEST :  -1026.3996020063285\n",
      ">>train tensor(1.6521) 120\n",
      ">>train tensor(-0.8671) 472\n",
      "[0>  30::     0] training =  0, steps =   1, max_step = 200, reward=-4.638577 ::[[-0.27928523]]: [ TARGET:-399.135630 replay::5600 ]<---->>train tensor(-2.7642) 392\n",
      "[0>  30::     0] training =  0, steps =  30, max_step = 200, reward=-222.086712 ::[[-0.249667]]: >>train tensor(-1.7006) 561ay::5600 ]<----\n",
      "[0>  30::     0] training =  0, steps =  42, max_step = 200, reward=-294.729460 ::[[-0.26322745]]: >>train tensor(-0.4048) 385\n",
      ">>train tensor(0.2483) 387\n",
      ">>train tensor(-0.4978) 604\n",
      "[0>  30::     0] training =  0, steps =  43, max_step = 200, reward=-305.558056 ::[[-0.26247738]]: [ TARGET:-372.724162 replay::5600 ]<---->>train tensor(0.3993) 285\n",
      "[0>  30::     0] training =  0, steps =  47, max_step = 200, reward=-340.556668 ::[[-0.18269803]]: >>train tensor(2.1487) 160\n",
      "[0>  30::     0] training =  0, steps =  82, max_step = 200, reward=-588.785781 ::[[-0.18692926]]: >>train tensor(-0.7162) 576\n",
      ">>train tensor(-0.6758) 584\n",
      ">>train tensor(-1.9239) 454\n",
      ">>train tensor(-2.0784) 510\n",
      ">>train tensor(-2.8734) 537\n",
      ">>train tensor(-0.6054) 546\n",
      ">>train tensor(-0.9020) 601\n",
      ">>train tensor(-0.7199) 503\n",
      ">>train tensor(0.7519) 501\n",
      "[0>  30::     0] training =  0, steps =  86, max_step = 200, reward=-610.363609 ::[[-0.2086784]]: >>train tensor(0.5812) 577ay::5800 ]<----\n",
      "[0>  30::     0] training =  0, steps = 122, max_step = 200, reward=-863.219420 ::[[-0.17107501]]: >>train tensor(-0.8508) 453\n",
      ">>train tensor(-2.0806) 457\n",
      ">>train tensor(-1.5280) 509\n",
      ">>train tensor(-3.6472) 385\n",
      ">>train tensor(-0.6624) 626\n",
      ">>train tensor(0.9508) 431\n",
      "[0>  30::     0] training =  0, steps = 123, max_step = 200, reward=-868.928254 ::[[-0.17115343]]: [ TARGET:-381.803802 replay::5800 ]<---->>train tensor(2.1323) 209\n",
      "[0>  30::     0] training =  0, steps = 126, max_step = 200, reward=-890.666685 ::[[-0.22273439]]: >>train tensor(-1.6533) 40\n",
      "[0>  30::     0] training =  0, steps = 162, max_step = 200, reward=-1149.756732 ::[[-0.22102061]]: >>train tensor(0.7656) 232\n",
      ">>train tensor(-0.6552) 193\n",
      ">>train tensor(0.1672) 536\n",
      ">>train tensor(-0.6623) 309\n",
      ">>train tensor(-1.0425) 237\n",
      ">>train tensor(-1.9658) 212\n",
      "[0>  30::     0] training =  0, steps = 163, max_step = 200, reward=-1160.797451 ::[[-0.22094238]]: [ TARGET:-386.101743 replay::5800 ]<----tensor(-1.4063) 636\n",
      "[0>  30::     0] training =  0, steps = 166, max_step = 200, reward=-1187.554452 ::[[-0.17091063]]: >>train tensor(-2.9056) 80\n",
      "[0>  30::     0] training =  0, steps = 200, max_step = 200, reward=-1428.452290 ::[[-0.19107071]]: >>train tensor(1.1741) 540\n",
      ">>train tensor(2.6366) 657\n",
      ">>train tensor(2.1560) 244\n",
      ">>train tensor(1.5795) 237\n",
      ">>train tensor(0.9234) 187\n",
      ">>train tensor(-3.1772) 603\n",
      "[0>  30::     0] training =  0, steps = 200, max_step = 200, reward=-1428.452290 ::[[-0.19095768]]: [ TARGET:-384.482380 replay::5800 ]<---->>train tensor(-1.6142) 490\n",
      "TEST :  -1068.123704154016\n",
      "[0>  31::     0] training =  0, steps =   1, max_step = 200, reward=-0.075387 ::[[-0.21658001]]: [ TARGET:-409.460427 replay::5800 ]<---->>train tensor(0.2112) 120\n",
      ">>train tensor(1.2854) 698\n",
      "[0>  31::     0] training =  0, steps =   4, max_step = 200, reward=-0.231845 ::[[-0.22154593]]: >>train tensor(1.0946) 257\n",
      "[0>  31::     0] training =  0, steps =  42, max_step = 200, reward=-129.589084 ::[[-0.20873403]]: >>train tensor(2.9771) 446\n",
      ">>train tensor(1.5646) 454\n",
      ">>train tensor(0.3676) 496\n",
      ">>train tensor(0.4254) 414\n",
      "[0>  31::     0] training =  0, steps =  43, max_step = 200, reward=-132.202081 ::[[-0.20765668]]: [ TARGET:-388.239694 replay::5800 ]<----tensor(-1.9353) 425\n",
      "[0>  31::     0] training =  0, steps =  44, max_step = 200, reward=-135.904531 ::[[-0.21628509]]: >>train tensor(-3.8045) 160\n",
      "[0>  31::     0] training =  0, steps =  82, max_step = 200, reward=-268.471362 ::[[-0.21790529]]: >>train tensor(-0.1930) 600\n",
      ">>train tensor(0.6197) 613\n",
      ">>train tensor(0.7137) 625\n",
      ">>train tensor(-0.5309) 435\n",
      ">>train tensor(0.3249) 461\n",
      ">>train tensor(-0.1652) 581\n",
      ">>train tensor(-0.7414) 366\n",
      ">>train tensor(-2.9337) 498\n",
      ">>train tensor(-1.8988) 619\n",
      "[0>  31::     0] training =  0, steps =  84, max_step = 200, reward=-287.546318 ::[[-0.2086228]]: >>train tensor(-0.9041) 558y::6000 ]<----\n",
      "[0>  31::     0] training =  0, steps = 122, max_step = 200, reward=-506.364222 ::[[-0.21683569]]: >>train tensor(-0.2374) 429\n",
      ">>train tensor(0.8765) 486\n",
      ">>train tensor(0.5592) 458\n",
      ">>train tensor(-1.2811) 509\n",
      ">>train tensor(-0.4992) 640\n",
      ">>train tensor(-0.1143) 243\n",
      "[0>  31::     0] training =  0, steps = 123, max_step = 200, reward=-507.118895 ::[[-0.2178769]]: [ TARGET:-386.882131 replay::6000 ]<---->>train tensor(-2.3105) 173\n",
      "[0>  31::     0] training =  0, steps = 125, max_step = 200, reward=-508.905398 ::[[-0.22817353]]: >>train tensor(4.4128) 40\n",
      "[0>  31::     0] training =  0, steps = 162, max_step = 200, reward=-738.573071 ::[[-0.17709122]]: >>train tensor(-3.1986) 522\n",
      ">>train tensor(-4.4077) 745\n",
      ">>train tensor(-3.7710) 242\n",
      ">>train tensor(-2.1125) 300\n",
      ">>train tensor(-0.7425) 422\n",
      ">>train tensor(1.1493) 248\n",
      "[0>  31::     0] training =  0, steps = 163, max_step = 200, reward=-746.252420 ::[[-0.17538338]]: [ TARGET:-389.105779 replay::6000 ]<---->>train tensor(0.9134) 534\n",
      "[0>  31::     0] training =  0, steps = 165, max_step = 200, reward=-756.317798 ::[[-0.19839812]]: >>train tensor(4.2722) 80\n",
      "[0>  31::     0] training =  0, steps = 200, max_step = 200, reward=-901.404974 ::[[-0.23094779]]: >>train tensor(-2.0808) 664\n",
      ">>train tensor(-2.1403) 765\n",
      ">>train tensor(-2.0486) 535\n",
      ">>train tensor(-2.2918) 626\n",
      ">>train tensor(-0.6851) 161\n",
      ">>train tensor(-1.4418) 192\n",
      "[0>  31::     0] training =  0, steps = 200, max_step = 200, reward=-901.404974 ::[[-0.23086611]]: [ TARGET:-387.953990 replay::6000 ]<---->>train tensor(0.7790) 192\n",
      "TEST :  -1638.274187346352\n",
      ">>train tensor(4.3954) 120\n",
      ">>train tensor(-1.1216) 222\n",
      ">>train tensor(-0.2493) 316\n",
      "[0>  32::     0] training =  0, steps =  42, max_step = 200, reward=-152.925737 ::[[-0.21382867]]: TARGET:-396.788211 replay::6000 ]<---->>train tensor(-1.4334) 212\n",
      ">>train tensor(-0.1841) 669\n",
      ">>train tensor(-1.1655) 235\n",
      ">>train tensor(1.00000e-02 *\n",
      "       8.6287) 230\n",
      "[0>  32::     0] training =  0, steps =  43, max_step = 200, reward=-163.739243 ::[[-0.20763073]]: [ TARGET:-402.460527 replay::6000 ]<---->>train tensor(0.5100) 445\n",
      "[0>  32::     0] training =  0, steps =  45, max_step = 200, reward=-191.928268 ::[[-0.15802701]]: >>train tensor(1.7178) 160\n",
      "[0>  32::     0] training =  0, steps =  82, max_step = 200, reward=-366.948563 ::[[-0.17911437]]: "
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    print(CFG)\n",
    "\n",
    "    env = gym.make(CFG['task'])\n",
    "    \n",
    "    encoder = encoderstack(env)\n",
    "    task_factory = taskfactory(env)\n",
    "    task = task_factory.new(GYM_CFG, 0, -1)\n",
    "    \n",
    "    def callback_task(agent, stop_q):\n",
    "        return callback(task, agent, stop_q)\n",
    "    \n",
    "    stop_q = Queue()\n",
    "    agent_launch(0, GYM_CFG, task_factory, encoder, ActorNN, CriticNN, stop_q, callback_task)\n",
    "\n",
    "if '__main__' == __name__:\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
