device = "cpu"#"cuda"#
tensor = "torch.DoubleTensor"#"torch.cuda.DoubleTensor"#
model_path = "models_dtb"
load = true
save = true
dbgout = true#false#
loss_debug = false

n_step = 1              # reduce variance, how many rewards we take into account at critic predictions, but increasing bias a little by little
n_critics = 1           # how many critics we want per task, must be < than n_step ( temporary limitation ~ lazyness )
batch_size = 32         # for one critic! therefore real full batch = batch_size * n_critics * n_simulations(this is given by task)
discount_rate = 0.9#95   # rewards discount factor
disjoint_critics = false#true#

task = "LunarLanderContinuous-v2"
n_simulations = 3
action_repeat = 1
max_reward_val = +1000
min_reward_val = -1000
replay_size = 200000#100#
max_n_eps = 1000
max_ep_draw_count = 30
mcts_rounds = 20
mcts_random_cap = 1000000
td_lambda_replay_count = 30

reinforce_clip = false
tau_clip = 0

lr_actor = 1e-4
lr_critic = 1e-3

tau_base = 5e-2 # update from full episodes
tau_final = 8e-3
tau_replay_counter = 10000
tau_adapt = 1e-3 # update from active experiences ~ we want to have it lower generaly
actor_update_delay = 13 # soft update of actor ~ off-policy, how much fits on explorer before
critic_update_delay = 10 # soft update of critic ~ off-policy, how much fits on explorer before
critic_learn_delta = 50 # here how much episodes before critic(and therefore also actor)-explorer update

# usually when we normalize advantages, we want to use sum in PG, and also attention is preferred
pg_mean = true#false#

good_reach = 20 # select from experience only related things if task is designed that way
advantages_enabled = true#false#
advantages_boost = true#false#
normalize_advantages = false#true#

attention_enabled = false#true#
attention_hidden = 64
attention_amplifier = 2.0

max_n_step = -10000#1#

# scheduling of replay buffer randomnes
replay_alpha = 1.0
replay_beta_iters = 6000 # here we should have enough experience to do serious prioritzation
replay_beta_base = 0.2
replay_beta_top = 0.8
replay_prio_eps = 1e-16

#####################################
# neural nets settings
#####################################

#lstm advancement
history_count = 3
history_features = 64

her_state_size = 0
her_state_features = 0
her_max_ratio = 0

dbgout_train = true#false#
fast_exp_epochs = 4
ppo_eps = 2e-1
loss_min = 1000

dbgout_ratio = true#false#
ddpg = false#true#

replay_cleaning = false#true#
prob_treshold = 3.0

gae = true#false#
gae_tau = 0.95

replay_reanalyze = true#false#
