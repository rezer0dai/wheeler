{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HER + multi agent ( PPO + DDPG ) cooperation + multitask ( 3D navigation )\n",
    "- Reacher 20 Arms ( UnityML ) ~ experimental Work In Progress ( well can takes a time to make this )\n",
    "- not working, concept of idea i am heading to check \n",
    "- however before i need properly benchmark reacher with my framework ( w/o HER or fancy stuffs )\n",
    "- here is just for demonstrating idea + implementation ~ so framework is OK to run it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import generics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os, time\n",
    "\n",
    "os.chdir(\"..\")\n",
    "\n",
    "import numpy as np\n",
    "import toml, gym\n",
    "\n",
    "import torch\n",
    "from torch.multiprocessing import Queue, Process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare statistic helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "def print_stats(scores, title):\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111)\n",
    "    plt.plot(np.arange(1, len(scores)+1), scores)\n",
    "    plt.ylabel('Rewards')\n",
    "    plt.xlabel(title)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load task configs ~ this should be adopted offline for particular task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "CFG = toml.loads(open('cfg.toml').read())\n",
    "DDPG_CFG = toml.loads(open('experimental/ddpg_cfg.toml').read())\n",
    "PPO_CFG = toml.loads(open('experimental/ppo_cfg.toml').read())\n",
    "\n",
    "torch.set_default_tensor_type(CFG['tensor'])\n",
    "print(CFG['task'])\n",
    "CFG['task'] = \"data/Reacher.x86_64\"\n",
    "CFG['total_simulations'] = 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import wheeler environment and particular utils we want to use ~ general ones ( shared across tasks )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from utils.task import Task\n",
    "from utils.taskinfo import *\n",
    "\n",
    "from utils.rbf import *\n",
    "from utils.normalizer import *\n",
    "\n",
    "from utils.taskmgr import *\n",
    "from utils.replay import *\n",
    "\n",
    "from utils.fastmem import Memory\n",
    "\n",
    "from utils.curiosity import *\n",
    "\n",
    "from utils.unity import unity_factory\n",
    "from utils.crossexp import *\n",
    "\n",
    "from agent.zer0bot import agent_launch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Task wrapper ~ when is goal met, how to step ( update rewards function, .. ), when / how to reset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "CLOSE_ENOUGH = 1.25\n",
    "\n",
    "def extract_goal(state):\n",
    "    return state[-4-3:-1-3] # this is nice, from perspective, that we are OK if our states are stacked ~ we work with latest one\n",
    "\n",
    "def goal_distance(goal_a, goal_b):\n",
    "    return np.linalg.norm(goal_a - goal_b.reshape(goal_a.shape), axis=-1)\n",
    "\n",
    "def fun_reward(s, n, goal, objective_id, cfg, her):\n",
    "    xid = objective_id % 4\n",
    "    if xid >= 3: # full task 3D naigation #True:#\n",
    "        return -1 * (2 * CLOSE_ENOUGH < np.abs(goal_distance(extract_goal(n), goal)))\n",
    "# X- Y - Z subtask navigation ~ multi task\n",
    "    a = np.abs(extract_goal(n)[xid] - goal[xid])\n",
    "    b = np.abs(extract_goal(s)[xid] - goal[xid])\n",
    "    if b < CLOSE_ENOUGH:\n",
    "        return 0.\n",
    "    return -1 + .9 * int(a < b) # little shape it maybe good idea ? ~ experimental\n",
    "\n",
    "def sample_goal(cfg, trajectory_goal, trajectory_state):\n",
    "    hs = cfg['her_state_size']\n",
    "    pos = extract_goal(trajectory_state.copy())\n",
    "    for i in range(3):# be carefull extremly expensive\n",
    "        radius = np.abs(np.random.rand() * CLOSE_ENOUGH)\n",
    "        angle = np.random.rand() * np.pi * 2\n",
    "        a = np.cos(angle) * radius\n",
    "        b = np.sin(angle) * radius\n",
    "        ids = np.random.choice(hs, 2, p=[1/hs]*hs, replace=False)\n",
    "\n",
    "        target = pos if random.randint(0, 5) else trajectory_goal\n",
    "        goal = target.copy()\n",
    "        goal[ids[0]] += a\n",
    "        goal[ids[1]] += b\n",
    "\n",
    "        if np.abs(goal_distance(goal, target)) < CLOSE_ENOUGH:\n",
    "            return goal, target\n",
    "    return (\n",
    "        pos if random.randint(0, 3) else trajectory_goal, \n",
    "        pos if random.randint(0, 3) else trajectory_goal\n",
    "    )\n",
    "\n",
    "def goal_select(total_after, n_step):\n",
    "    if total_after <= n_step + 1: # only last n_state remainds\n",
    "        return 0\n",
    "    if random.randint(0, 2):\n",
    "        return random.randint(1, n_step)\n",
    "    if 0 == random.randint(0, 3):\n",
    "        return 0\n",
    "    return random.randint(1, total_after - 1 - n_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class GymTask(Task):\n",
    "    def reset(self, seed = None, test = False):\n",
    "        cfg = {\"goal_size\":5., \"goal_speed\":0.}\n",
    "        state = super().reset(cfg, test)[0]\n",
    "\n",
    "        self.goal_ = state\n",
    "        \n",
    "        if test: return state # we will get array of states\n",
    "        \n",
    "        state = state.reshape(-1)\n",
    "        self.prev = state\n",
    "        return [ state ]\n",
    "    \n",
    "    def step_ex(self, action, test = False):\n",
    "        state, done, reward = self.env.step(self.bot_id, self.objective_id, action)\n",
    "        self.goal_ = state.copy()\n",
    "\n",
    "        if not test:\n",
    "            reward = fun_reward(\n",
    "                self.prev.reshape(state.shape), state,\n",
    "                self.goal(0).reshape(-1), self.objective_id, \n",
    "                self.cfg, False)\n",
    "        self.prev = state\n",
    "        return action, state, reward, done, True\n",
    "\n",
    "    def goal_met(self, states, rewards, n_steps):\n",
    "        return sum(abs(r) for r in rewards) > 30\n",
    "\n",
    "    def goal(self, ind = 0):\n",
    "#        print(extract_goal(self.goal_.reshape(-1)).reshape(1, -1) if self.goal_.shape[0] != 20 else extract_goal(self.goal_[ind].reshape(-1)).reshape(1, -1))\n",
    "        if self.goal_.shape[0] != 20: return extract_goal(self.goal_.reshape(-1)).reshape(1, -1)\n",
    "        return extract_goal(self.goal_[ind].reshape(-1)).reshape(1, -1)\n",
    "    \n",
    "    def update_goal(self, _, goals, states, n_goals, n_states, updates):\n",
    "        for i, (g, s, n_g, n, u) in enumerate(zip(goals, states, n_goals, n_states, updates)):\n",
    "            if u:\n",
    "                ind = i + goal_select(len(states) - i, self.cfg['n_step'])\n",
    "                g, n_g = sample_goal(self.cfg, goals[ind], states[ind])\n",
    "            \n",
    "            yield (\n",
    "                fun_reward(s, n, g, self.objective_id, self.cfg, True),\n",
    "                g, s,\n",
    "                n_g, n,\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generic proxy for creating our Task ( multiprocess environments purpose mainly ) \n",
    "- but can also add wrapping function approx values ( action value to tanh, sigmoid, .. ) - this not works well with PPO now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class GymInfo(TaskInfo):\n",
    "    def __init__(self, replaybuf, factory, Mgr, args):\n",
    "        super().__init__(\n",
    "                33, 4, -1, +1,\n",
    "                CFG,\n",
    "                replaybuf,\n",
    "                factory, Mgr, args)\n",
    "\n",
    "    def new(self, cfg, bot_id, objective_id):\n",
    "        # first task created must be our testing one ( not for learning, just for testing .. TODO : move inside framework )\n",
    "        task = GymTask(cfg,\n",
    "                self.env,\n",
    "                objective_id, bot_id,\n",
    "                self.action_low, self.action_high)\n",
    "        if -1 == objective_id:\n",
    "            task.reset()\n",
    "        return task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implement callback for testing policy ~ per X training rounds, we want to test it ~ enable visuals if you want"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def context_callback(count, print_every):\n",
    "    context_callback.z = 0\n",
    "    context_callback.scores = []\n",
    "    context_callback.total_scores = []\n",
    "    def callback(task, agent, scores):        \n",
    "        context_callback.z += 1\n",
    "        context_callback.scores += scores\n",
    "        if 0 == context_callback.z % print_every:\n",
    "            print(\"\\nTEST review : \", np.mean(context_callback.scores[-print_every:]), \"LAST stats :\", sum(scores), sum(map(lambda r: r != 0, scores)), len(scores))\n",
    "        # we can save scores to main queue, and avarage them, or we can ..\n",
    "        # run testing w/ visuals :\n",
    "        for _ in range(count):\n",
    "            done, states, rewards = task.test_policy(agent)\n",
    "            rewards = sum(rewards)\n",
    "            context_callback.total_scores.append(rewards)\n",
    "            if not done:\n",
    "                return None\n",
    "    \n",
    "        print_stats(context_callback.scores, \"all-out::testing_phase\")\n",
    "\n",
    "        print(\"\\n\")\n",
    "        print(\"=\"*80)\n",
    "        print(\"training over\", context_callback.z * DDPG_CFG['n_simulations'] * DDPG_CFG['mcts_rounds'])\n",
    "        print(\"=\"*80)\n",
    "\n",
    "        return context_callback.total_scores\n",
    "    return callback\n",
    "\n",
    "def ppo_callback(task, agent, scores): # agent cooperation #1 loading core of the actor model, and do sample efficient PPO on it\n",
    "    agent.bot.alpha_sync([\"neslayer_2_\"] + [\"_%i.\"%i for i in range(PPO_CFG[\"n_simulations\"], DDPG_CFG[\"n_simulations\"])])\n",
    "    # we avoid loading last layer of NoisyNetwork ~ to let degree of freedom and adaptation\n",
    "    # also explorer have less ( maybe should have more ? ) detached actors to work with, but we anyway sync it by actor so does not matter\n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare neural network which we will be using"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from models import ddpg_model, noisy_model, state_action_model\n",
    "\n",
    "def CriticNN(state_size, action_size, wrap_value, cfg):\n",
    "    return state_action_model.Critic(state_size, action_size, wrap_value, cfg, fcs1_units=256, fc2_units=128)\n",
    "\n",
    "def ActorNN(state_size, action_size, wrap_action, cfg):\n",
    "    return noisy_model.Actor(state_size, action_size, wrap_action, cfg, hiddens=[400, 300])\n",
    "    return noisy_model.Actor(state_size, action_size, wrap_action, cfg, hiddens=[128, 64])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select encoders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from utils.encoders import *\n",
    "from utils.rnn import *#GRUEncoder\n",
    "\n",
    "def encoderstack():\n",
    "    norm = GlobalNormalizer(DDPG_CFG, 33)\n",
    "    return norm # well GRU/LSTM testing comes if we go sucessfully at least here ...\n",
    "    DDPG_CFG['max_n_episode'] = 70 # we want gae + rnn; need re-eval eps ~ 1000 per shot is overkill\n",
    "    DDPG_CFG['batch_size'] = 16 # this is basically numero of sample eps, an from each ep we draw about <1, 40> steps\n",
    "    experience = FasterGRUEncoder(DDPG_CFG, norm.total_size())#GRU#LSTM\n",
    "    encoder_norm = StackedEncoder(DDPG_CFG, 33, norm, experience)\n",
    "    encoder_norm.share_memory()\n",
    "    return encoder_norm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cook Task : replay buffer ( fast / prio-gae-rnn ) + task manager ( local / remote / unity )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def taskfactory(): # agent cooperation #2 sharing experiences\n",
    "    return GymInfo(cross_exp_buffer(CFG), unity_factory(CFG, CFG['total_simulations']), RemoteTaskManager, (LocalTaskManager, 1 + CFG['total_simulations']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Glue it all together ~ select buffer, encoders, agents, ... and RUN!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "class GoalEncoder(IEncoder):\n",
    "    def __init__(self, cfg, size):\n",
    "        super().__init__(cfg)\n",
    "        self.size = size\n",
    "        self.net = torch.nn.Linear(size, cfg['her_state_features'], bias=False)\n",
    "    def out_size(self):\n",
    "        assert False, \"...\"\n",
    "    def forward(self, goals, _):\n",
    "        return self.net(goals)\n",
    "\n",
    "def main():\n",
    "    print(CFG)\n",
    "    \n",
    "    encoder = encoderstack()\n",
    "    task_factory = taskfactory()\n",
    "    task = task_factory.new(DDPG_CFG, 0, -1)\n",
    "    \n",
    "    callback = context_callback(10, 10)\n",
    "    def callback_task(agent, stop_q):\n",
    "        return callback(task, agent, stop_q)\n",
    "    \n",
    "    def ppo_callback_wrap(agent, stop_q):\n",
    "        return ppo_callback(task, agent, stop_q)\n",
    "\n",
    "    stop_q = Queue()\n",
    "    \n",
    "    #import threading\n",
    "    worker = Process(#threading.Thread(#\n",
    "        target=agent_launch,\n",
    "        args=(\n",
    "            1,\n",
    "            PPO_CFG, task_factory, \n",
    "            encoder, ActorNN, CriticNN,\n",
    "            stop_q, ppo_callback_wrap,\n",
    "            GoalEncoder(PPO_CFG, PPO_CFG['her_state_size']),))\n",
    "\n",
    "    worker.start() # agent for efficient explorering!\n",
    "    # now agent for complex learning ( efficiently handle pas experiences )\n",
    "    agent_launch(0, DDPG_CFG, task_factory, encoder, ActorNN, CriticNN, stop_q, callback_task, GoalEncoder(DDPG_CFG, DDPG_CFG['her_state_size']))\n",
    "    \n",
    "    scores = stop_q.get()\n",
    "    print(\"FINISHED!\")\n",
    "    print_stats(scores, \"learning algorithm\")\n",
    "    \n",
    "    worker.join()\n",
    "\n",
    "if '__main__' == __name__:\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Anaconda3]",
   "language": "python",
   "name": "Python [Anaconda3]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
